# 新版 book 大纲

## chyyuu的想法
1. 确定几个基本的app要做的事情
2. 看看OS应该如何一步一步地实现这些app的正确运行
3. 在支持这些app中，逐步引入 类似syscall/function的interface，和不同程度的实现

## 新的全局思路

保持应用一直不变，这就像是提交给编译器的源码一直不变一样。

比如一开始将应用直接视为一个裸机程序，此时的内核其实就是一个用户库。后来应用自身的复杂和执行环境的复杂导致内核也要变得足够复杂，并利用 CPU 提供的种种特殊机制。有如重新体验了一次操作系统发展史，实际上也算是 CPU 进化史，因为是应用侧的逐渐复杂导致 CPU 提供越来越多的机制，随后才是操作系统进行适配。

目前我的思路是先在第一章定死了进程、系统调用、任务切换这些概念，然后每一章通过探索 CPU 的能力来逐渐实现这些机制，确实会导致在逻辑上有些不通畅。

## 代码与文档风格

每章还是代表内核的某个状态，随着章节的逐渐推进，内核的功能也逐渐增多。

每一个小节给出对应的代码，所以到底要不要那种复制粘贴代码的风格呢？我们应该主要讲清楚设计而不是纠结于实现。所以，事实上，将接口的功能讲清楚（各种不同情况下的返回值），让希望自己完全实现一个内核的读者能够自己来提供接口的实现；如果对此没有兴趣的话只需点击链接去看 Github 上的实现就好了。

lab 的风格是要说明需要实现哪些函数，可能修改哪些部分、调用哪些接口等等。大概还需要一个完善的测试框架，...

如果按照 step by step 的方式来设计，实验题目真的非常难搞...因为模块都不是很全，那怎么办嘛。原先 ucore 的填空就挺让人头痛的了，建议不要再来填空。但是实际上在用户进程出来之前我们都没啥实验可以搞...

我们先不考虑练习题目了...

xv6 实际上是从用户程序入手，然后到 syscall，最后才是内核的各模块的。这种思路是否直接借鉴呢？其实好像还挺对的。但是我们的思路是通过不断挖掘 CPU 的能力来获取更高级的抽象能力。实际上直到进程这一章

## 知识点总览

### RISC-V 架构篇

* RISC-V 指令集，包括通用寄存器以及一些常见的功能寄存器（如 sp, ra）等，了解常见的 RISC-V 汇编指令，如算术、访存、跳转指令
* RISC-V 调用规范：明确调用规范存在的意义和为何需要一个栈，了解 caller/callee saved 寄存器各有哪些，如何传递参数和返回值，以及 stackframe 的分布
* RISC-V 特权级与 trap 处理：最基本的情况下只需了解

### Rust 编程语言篇

### 操作系统篇

## lab0: 最小化内核

代码内容一直到最小化内核，并支持格式化输出。最后可以在模拟器/k210 串口终端里面看到 Hello world。

涉及内容：移除 OS 支持启用 no_std，Rust 模块化编程结构，链接脚本（与编译相关，涉及到程序的内存布局），初始化 `entry.asm`（这里面就已经要强调栈的作用了），RustSBI 与调用 SBI 接口，借用 SBI 接口来实现格式化输出。

## 附录 A：RISC-V 架构相关内容

## Log

新的内存布局：用户和内核共处同一个地址空间。整块物理内存（包括内核的代码/数据）全部以 $\mathtt{0xFFFF\_FFFF\_0000\_0000}$ 的偏移量移动到高地址空间，所有 MMIO （都低于 $\mathtt{0x8000\_0000}$）全部恒等映射到低地址空间，这个是为了兼容 M 态驱动程序。随后，要求所有的用户程序都从 $\mathtt{0x8000\_0000}$ 作为起始地址。这样就能够保证虚拟地址空间不产生冲突了。且尽可能和原来的实现兼容。

移除线程 Thread 强调进程 Process。注意现在的进程 Process 里面表示一个（单线程）用户程序

之前实现的不好的 syscall，比如 fork, exit, exec。完全没有的 syscall，比如 sbrk（lazy allocation）以及 fork（COW）。但是目前 Tutorial 的实现是一个 Process 里面有多个 Thread，每个 Thread 里面有一个 `Range<VirtualAddress>` 表示其运行栈在进程地址空间中的位置区间。在有多个 Thread 的情况下，fork 有点不太好弄...或者定义为只能对只有一个 Thread 的 Process 调用 fork？但目前来说完全没有必要做多 Thread 的 Process。目前多 Thread 的 Process 仅用在内核线程里面。

主要是 xv6 里面的测试程序挺爽的...

1. 没有站在 syscall 角度考虑问题，比如 xv6 的第一个 lab，确定 syscall 语义
2. 补充 syscall，如 ipc 的 pipe
3. 展开文件系统
4. 展开 Mutex

## 下一步计划

1. 不在代码中体现线程的概念，只保留进程 Process 同时作为资源控制和 CPU 调度单位

   资源控制：地址空间（复用 MemorySet），打开的文件描述符表

   CPU 调度单位：需要保存进程的运行状态、进程的 PID、进程在内核被切换出去之后的上下文（可以被看成一个函数调用，中断上下文在栈上无需保存）、内核栈在进程地址空间中的位置

2. 进程地址空间分布：不对内核和用户地址空间进行隔离

   低于 0x80000000：设备 MMIO 的恒等映射

   从 0x80000000 开始：用户程序的代码、数据（从 ELF 中解析），接下来属于用户堆区域，可以通过 `sbrk` 系列系统调用来增长或缩减用户堆的大小

   从 0xFFFFFFFF80000000 向下：首先是一个带有 guard page 的进程专属的内核栈，然后是一个带有 guard page 的进程专属的用户栈（需要以某种方式传递 `int argc, char* argv[]` 参数）

   从 0xFFFFFFFF80000000 向上：整块物理内存偏移量为 0xFFFFFFFF00000000 的线性映射，包括 RustSBI 代码、数据段，内核代码、数据（内含启动栈和内核堆），还有剩余可用的物理内存

3. 取消内核线程，将其变为内核进程，在 Process 加一个域表示是否为内核进程，这样易于实现。实现的时候要注意内核进程就无须开一个用户栈了，在 trap 的时候也无需换栈。

4. 子进程与进程回收

   除了 Runnable/Running/Sleeping 之外，进程还有一个 Exit 状态，表示它调用 exit 系统调用退出或者是由于某种原因被内核杀死。但是一个进程不能在最后一次进入内核态的时候自己把自己的资源全部回收，主要是因为目前代码还是在它自己的内核栈上面跑的。因此先将其用户代码、数据段包括用户栈回收，然后将其标志为 Exit 状态，并在 PCB 里面保存退出状态。其父进程可以通过 wait 系统调用来获取一个 Exit 的子进程的退出状态，顺带回收它的内核栈和 PCB。

   当一个进程退出时，将它的所有子进程让渡给初始进程 initproc，这个进程的任务就是回收之前还没有回收的进程。同时还需要考虑它的父进程，如果它的父进程正处于 wait 子进程退出的状态，则唤醒父进程。因此应该有一个全局的 condvar 来保存所有处于此状态的进程。

   xv6 风格由于没有内核堆分配器，因此大多数数据结构的风格都是开一个定长数组，对于内存有一定程度上的浪费。目前 tutorial 无需做到像是 biscuit 那样对于堆内存有着相对容错且稳定的设计，如果堆内存不足，内核直接 panic 即可。

5. Unix 万物皆文件

   即 xv6 里面的 file，这里继承的是 Unix 的万物皆文件哲学，也即每种资源都可以用一个文件描述符来访问，支持 open/close/read/write 等操作。

   目前我们预计提供三种不同的资源：首先是文件系统，代表块设备；其次是串口输入输出，代表字符设备；最后是作为 IPC 的管道，作为某种内核资源。

6. 高优先级系统调用

   * `int fork()`：fork 出一个子进程，对于父进程返回子进程的 PID，对于子进程返回 0。如果物理页帧不足，则不 fork 子进程并返回 -1。

   * `int exit(int status)`：调用 exit 退出进程，该函数不应该返回。传入的 `status` 将作为进程的返回状态，可以被父进程捕获到。

   * `int wait(int* status)`：等待**一个**子进程退出并将其返回状态存储在 `status` 里面，其返回值为子进程的 PID。如果该进程没有任何子进程则立即返回 -1，此时应忽略 `status` 里面的值。

   * `int getpid()`：返回当前进程的 PID。

   * `int exec(char* file, char* argv[])`：用 `file` 处的用户程序替换掉进程地址空间中的用户代码、数据段，清空用户栈并在上面传入 `argv` 作为可以被捕获到的初始参数。可能会消耗更多物理页面，因此如果物理页面不足，应放弃替换并返回 -1；否则不应该返回而是从用户程序的入口点开始执行。

   * `int open(char* file, int flags)`：以 `flags` 标志打开位于 `file` 处的 Unix 资源。暂时只支持打开位于块设备文件系统上的文件或目录，而不支持打开 `/dev/tty` 等其他设备。如果成功，返回其 fd；否则返回 -1。

     `int close(int fd)`：释放当前进程的 Unix 资源 `fd`。如果成功返回 0，如果失败（如当前进程不存在资源 `fd`）则应该返回 -1。

   * `int write(int fd, char* buf, int n)`：尝试向 Unix 资源 `fd` 内写入 `buf` 开头的 `n` 字节 ，返回实际写入的字节数。

     `int read(int fd, char* buf, int n)`：尝试从 Unix 资源 `fd` 读 `n` 字节到 `buf` 开头的一块内存，返回实际读到的字节数。

     注意 `write/read` 的表现随着 `fd` 对应的资源类型（文件系统中的文件/目录 `T_INODE`、串口设备 `T_DEVICE`、管道 `T_PIPE`）不同会发生变化，这里暂且不考虑。

   * `int pipe(int p[])`：创建一个管道，返回的两个 fd `p[0], p[1]` 分别表示管道的读端和写端。返回 0。

7. 低优先级系统调用

   * `int kill(int pid)`：杀死一个 PID 为 `pid` 的进程。如果成功则返回 0，否则返回 -1。（注意应该不允许杀死自己）
   * `int sleep(int n)`：休眠一个进程，时长为 `n` 个 ticks（在调度模型中进程执行 1 个 tick 时候就会切换到其他进程），返回值为 0。
   * `char* sbrk(int n)`：增长用户堆内存 `n` 字节，`n` 有可能为负数。 返回这块内存的起始地址，如果物理页面不足则返回 -1 也就是 0xFFFFFFFFFFFFFFFF。（在 libc 内可以将 `sbrk` 包装成 `malloc` 和 `free`，但是在 Rust 里面还是倾向于事先在数据段分配一块用户堆内存用于动态分配 ）
   * 创建/删除文件/目录，获取元数据之类的系统调用。

8. 参考 xv6 或者 ucore 实现一个简单的文件系统

   最先只需考虑依次实现：磁盘驱动（qemu 平台的 virtio 以及 k210 平台的 sd 卡）的封装、磁盘布局的实现（超级块、inode 和数据块区域及其 bitmap）、将 inode 包装为 Unix 资源类型 file 并实现相应 syscall

   时间充裕的话可以考虑实现块缓存或者日志系统

9. 手动实现 `spin::Mutex` 并解释其原理

   只需利用 Rust core::sync::atomic 提供的原子类型 `AtomicBool` 或者参照 xv6 直接用 RISC-V 提供的原子 CAS 系列指令即可。需要说明的是原子指令不仅仅是一条指令进行多个操作，站在多核角度还需要考虑内存一致性模型，以及 Memory Ordering。比如 RISC-V 提供的 LR-SC 指令和普通的 Load/Store 指令一样只有读或者写一个操作，只是设置了 Memory Ordering 之后也能用来实现 spinlock。这部分可以稍微介绍一下，但目前我也不是很懂。

   内核中基本上所有的共享资源都需要受到 spinlock 的保护。

   之后除了已有的 `CondVar` 之外，还可以在代码或者练习中实现其他同步原语，并在内核中进行测试。

10. 搬运 xv6 中所有能用的测试程序和测试脚本，也可以考虑从 ucore 里面移植一些。

## 新版设计

现在要做的事情是，在第三版的基础上，重新回到传统的 Unix 进程，不考虑线程（或者可以说每个进程只有一个线程），且每个进程独占一个用户栈和内核栈。但其实我们还比较想能够支持内核线程，来完全在内核态搞一个同步互斥实验，就像 ucore 里面的哲学家问题一样。

比如说用户进程 `Process` 的话，是一个资源控制单位，同时也要保存一个传统的进程控制块。资源控制单位就是一个 `MemorySet`，表示一个地址空间，并负责控制所有相关的物理页帧（包括保存多级页表映射以及保存数据的物理页帧），`MemorySet` 的情况我们下面会再提到。此外还需要有一个打开的文件列表，这个到文件系统的地方再提。进程控制块里面需要保存进程的运行状态，目前大概有 `Runnable/Running/Sleeping/Exit`，进程运行状态的转移后面再说。然后就是进程的标识符 `pid`。最后是一个进程上下文 `Context`，表示一个在所属内核栈上执行的寄存器状态，与一个著名的函数 `switch(Context*, Context*)` 有关。它表示将当前的寄存器状态保存在前一个 `Context*` 里面，然后从第二个 `Context*` 中恢复状态。这个 `Context` 可以就地放在所属内核栈上面，也可以放在 PCB 里面，后者的话相对浪费一点内存。我们一般会将它封装成一个函数调用，达到让编译器帮助我们保存 caller-saved 寄存器的效果。因此我们需要保存 sp（由于要换栈）, ra 以及所有的 callee-saved 寄存器。

`trap` 的相应情况：

> 这里思考一下 rjgg 提出的内核态调用用户态（这个与异步其实是无关的，只不过执行的开头从用户态变成了内核态）是否能够成立。每一个用户进程（只考虑单线程，也就是用户线程）会被内核看成一个内核线程，里面做的事情是一个循环，维护一个中断帧（我们在创建进程的时候构造好的，可以调用 `__restore` 返回用户态开始执行），在循环的开头我们尝试调用 `__restore` 回到用户态，在用户态执行了一段时间之后就会 trap（这是 RISC-V 的特有说法，x86_64 中的 trap 不是一个含义）回到内核态，我们得想办法让它能够回到 `context.run` 下一条指令继续执行。用户态应该是回到 `stvec` 的位置，然后那里会根据来自用户还是内核态判断是否换栈。如果来自用户的话，需要把栈切换到内核栈，而内核栈的位置需要被保存在 `sscratch` 寄存器中。这个 `sscratch` 应该是在进入用户态的时候被设置为内核栈的位置，然后回到内核态的时候第一时间从里面找到当前进程内核栈的位置。
>
> 突然想到，回收资源很大程度上来源于编译器在闭包返回之后会自动回收掉对应的栈帧，所以本质上是白嫖编译器，就像是白嫖 caller-saved 寄存器的保存一样。不过确实可以考虑给线程实现 Fn 系列 trait，在 drop 的时候自动回收相应的内核栈，...但是这个对于语言的理解要求比较高，我们目前暂不做考虑。
>
> 太复杂了，还是先溜了溜了。

我们（没有们，但是现在好像又有了？）想了一下，可能还是要加入 `Thread` 类，不然某些东西（特指内核线程）实在没法搞！而且确实也能够稍微深化对于进程/线程之间区别的认识。`Thread` 里面，应该还是需要一个 `Arc<Process>` 从而可以在所有进程退出后回收掉进程相应的页面，此外，需要一个东西指明内核栈的位置，还有一个地方存储内核上下文。由于我们下定决心给每个线程一个独立的内核栈，因此这两个东西其实可以合并，只要给出保存那一时刻内核栈的栈顶即可。

如果是内核线程的话，它就一直在内核栈上面跑，然后遇到了时钟中断的话（假设这个时候我们已经打开了），首先要进入中断处理，保存中断上下文（直接分配一块 stackframe），然后通过 call 进到中断入口点（这里没为这个 call 保存 caller-saved 寄存器，因为之前已经全部保存了），注意此时还在内核栈上，时钟中断里面应该是将线程移动到调度队列队尾，找出一个新的就绪线程（我们知道它的内核上下文在什么地方），然后调用 switch，保存一系列寄存器到当前的内核栈并保留它的栈顶位置到当前 `Thread` 的 TCB 里面，这之后（根据 switch 的第二个参数）切换内核栈，从内核栈顶恢复内核上下文（这个过程应该也是恢复了栈），注意之后还会在编译器的帮助下恢复所有 caller-saved 寄存器，这也就是为何内核上下文里面只需要保存 `pc, ra` 还有 12 个 callee-saved 寄存器。

线程开始运行：只需要调用一次 switch 就可以，自然 sp 和 ra 要分别正确设置为内核栈顶和入口点，还可以通过寄存器传参数。但是如果想实现 argc 和 argv 就要稍微麻烦一些了。（这个内核线程里面就不用了，是纯粹 C 风格的...）那就假设是用户进程吧。这个都是细节了，而且 Rust 里面应该也没法搞...我们是不是还需要实现一套 libc 呀...这个是后话了。内核线程的话，我们需要将 ra 设置到内核线程的入口点；如果是用户进程，由于需要切换特权级，我们必须借助 `__restore`，因此首先要构造一个内核上下文将 ra 设置到 `__restore`，然后构造一个中断上下文，将 `sepc` 设置到用户进程入口点，同时将 `sstatus.spp` 设置到用户态，这样才能够回去。所以这两个是独立的，而不应该像第二版那样包装到一起。初始化的时候先在栈上压入中断上下文，再压入内核上下文。这里其实有点类似自引用结构，所以在 TCB 里面保存的内核上下文大概是一个 `*mut KernelContext`。

中断的开关情况：第二版里面是所有的初始化结束之后就打开中断了。如果是内核线程的话，那就是只有在运行出现中断的时候才会关中断（RISC-V 默认进入 trap 的时候关掉中断，但是仅限于 S 态，M 态的中断还是照样能触发，现在想想，在 PLIC 的作用下，特权级+中断还是比较容易理解的，当时总结的规则似乎也仍然能工作）。第三版甚至内核线程启动的时候也用到中断上下文来打开中断和设置参数。这样想的话可能确实需要设置一些参数？那么大概也可以考虑一下内核线程的启动也加上中断上下文。倒是无所谓的。之前 xv6 里面忽略的点是其实应该在 syscall 的时候打开中断，从而遇到时间非常长的 syscall 的时候不至于在里面卡死。因此中断上下文里面也不需要保存 scause 和 stval。一旦不关闭中断，我们就必须注意当前访问的变量是否会产生并发冲突...会导致严重的潜在问题，甚至有可能产生死锁。

我觉得根据 spinlock 的层数决定是否关闭中断可能也不一定能行。哦，其实是可以的，因为其他核即使尝试获取相同的锁被卡住也不会有问题，只要不要在相同的核上面获取两次锁，导致这个核永久死锁就行了。但是这样子确实挺复杂的，而且不见得效果很好（文件系统里面各种锁可有不少，而且可能都是比较耗时的操作）。其实 trap 处理可以看成另一个内核线程，在触发中断的一瞬间新建，入口点为中断处理入口，然后在 sret 之后回收。这也就是为什么单核也有可能产生并发问题。这个本质确实就是同一个地址空间内的两个线程（执行流）由于访问相同的物理内存导致了并发问题。

这样可以解释第三版为何搞了一个关中断自旋锁，等价于 xv6 里面的 push_off/pop_off。它只会用在内核线程里面，因为需要基于时钟中断来实现内核线程的切换。但是本质上内核线程和用户进程应该没有区别的...？这个可以先一刀切，然后后面逐渐变得更加精细。这里要是想不出现死锁甚至还需要一点形式化验证的知识？暂时先参考第二版。

进程/线程的资源回收。先来考虑简单的用户进程的情况。一般来说是通过 exit 系统调用，我们只是尽可能回收一些资源，然后将该进程标记为 EXIT 状态，这个就不能像是之前的第三版那样，等到所有的 `Arc<Thread>` 都被 drop 掉之后，里面的 `Arc<Process>` 也被 drop 掉，然后就立即回收资源。我们需要非常小心，不能回收掉当前所使用的内核栈。要不要对进程和线程状态做一下分离：线程状态就可能是：Runnable/Sleeping，甚至可以没有线程状态，而是根据它在调度队列中的就绪子队列还是休眠子队列来决定。仔细想一下的话，我们可能并不需要这个线程状态。但是要具体实现之后才能说清楚...

子进程/父进程：每个进程要维护指向子进程的指针数组以及指向父进程的指针，是 `Arc` 还是 `Weak` 再论。因为传统 Unix 我们只能通过 fork 来创建新的进程，这个时候父进程里面需要有一个 Arc 指向子进程，这样它一旦从子进程的 PCB 里面拿到退出状态，就可以立马 drop 掉指向子进程的 Arc，可以想象这个时候子进程的引用计数将变成 0，最主要的是里面的 MemorySet 将被回收，于是 FrameTracker 们可以开始干活回收物理页帧了。指向父进程的指针用 Arc 还是 Weak 其实都行，那就用 Weak 吧...现在这个回收资源是基于进程的，其实也会导致内核线程出现一些问题，在内核线程结束后不能及时回收。其实内核线程怎么算是结束都还是个问题，第三版里面是将内核线程内核上下文的 ra 设置为一个特殊的内核线程版 exit 来尝试回收掉对应的资源，应该回收的应该只有存储内核栈数据以及存储一部分页表映射的物理页帧。不过在具体实现的时候只是 drop 掉了 `Arc<Thread>`，进而减少了内核进程的引用计数，实际上对应的内核栈并没有回收，尽管在第三版里面只有一个中断栈，但是每个内核线程还是有一个内核态运行栈的。我忽然想起第二版里面内核栈是直接在堆里面分配的，如果以一个 Arc 的方式放在 TCB 里面，对于内核线程来说还比较合适，因为一旦内核线程的引用计数变为 0，内核栈也就会被回收了。内核线程的一种非常优美的方式是基于闭包，这样至少整个栈帧都在编程语言的控制范围之内，不过这个东西我还没怎么想清楚，虽然非常好，但是还是放到第四版里面去吧。所以啊，所以，到底应该怎么设计呢~其实就是一个内核线程的取舍问题。用户进程是已经定下来的。

TCB/PCB 等合理减小锁的粒度。

MemorySet 里面 Segment 的不同类型。这些就明天再想吧。有两个最基本的，Linear 和 Frame。在最新设定的内存布局里面，Linear 指的是物理内存（包括内核代码数据）的偏移量为 0xFFFFFFFF00000000 的到高地址的映射，Frame 用于创建用户进程，需要从文件中读取各个段加载到地址空间，这个时候虚拟地址区间是确定的，然而物理页的位置则是视当时的情况而定。Device 和 Linear 差不多，物理地址区间和虚拟地址区间都是确定的，只是偏移量不同。之前看 xv6 的时候学到了页表的一些高级用法，比如 sbrk 时候的 lazy allocation，比如 fork 时候的 copy on write，最后就是 paging from disk 这种东西。我们明确需要实现的是 paging from disk，因为 k210 板子上的内存实在太小，但是我倒感觉不一定需要作为 lab...而且这个可能涉及到全局页面替换算法，具体是个什么机制还不好说。即使有页面替换算法，用于存储页表映射的页面也是不会被换出的。事实上只有用于存储用户态数据（其实内核态也可以？）的页面有可能会被换出并储存在磁盘上。于是在创建用户进程的时候我们在交换区分配磁盘页并拷贝数据，至少这个时候是不消耗物理内存的。然后发生 page fault 的时候尝试分配一个物理页，将磁盘页里面的数据读到物理页里面，更新页表映射（原先页表项是一个全 0，物理页号和 V 标志位都是 0）；如果限额已满的话，我们则需要从页面交换器里面换出一个物理页面，当然，我们还需要知道它的虚拟地址，这样才能够调整页表。因此页面交换器里面存储的是虚拟地址和物理页面的二元组。接下来的事情很简单，invalidate 原先的虚拟页面映射和二元组，更新物理页面上的数据，然后更新页面映射和二元组即可。那目前暂且还是保留限额这种设计吧。某种程度上来讲限额可以开稍微大一点，不然频繁的页面换入换出速度会很慢。所以虚拟内存这里算是差不多了。

> 这是我的写作风格啊，就是一切从零开始，然后随着需求的增加逐渐更新设计，否则就是退化为代码注释，你只能对着代码说服自己：这样的设计确实是有道理的。但是事实上还不够？获取我有些过于钻牛角尖了？

这个可能也是设计成 on demand 比较好？就是加载的时候在 MemorySet 新建一个 Segment 里面保存 inode 和 offset，发生 page fault 的时候去对应的进程的 MemorySet 查 Segment，我们发现这是一个类似于 mmap 把磁盘上的文件映射到虚拟地址空间的家伙，然后根据错误所在的虚拟地址查到实际的 inode 区间，从 inode 里面读数据，加上具体的偏移量得到一个完整的 4096 字节数组，此时才会考虑分配一个物理页面。第三版中的限额相当于是一个局部页面替换算法，但是也有可能出现物理内存耗尽的情况，因为它并没有考虑到所有进程都符合限额要求，但是加在一起耗尽了物理内存的情况。我们暂且先不用讨论这种情况...

啊啊，有点想实现地址空间的隔离了...其实就是在中断的时候，比如说是用户 trap 到内核，我们加入页表切换并刷新 tlb 即可，当然麻烦的事情在于必须引入 trampoline page...这个似乎不算很难，在 linker.ld 里面将 trap 相关的代码对齐到一个 4K 页面，然后我们在内核里面知道它的物理地址，然后在每个进程里面把它映射到最高的 4K 页面，然后需要把中断入口点 `stvec` 进行一些调整：需要根据它实际的虚拟地址往高地址空间再调整一些。那么它的好处：好像只是可以保证用户代码可以从 0x0 开始，仅此而已。我们并不关心什么幽灵、熔断漏洞...从 paging from disk 的角度，那可能就是用户地址空间的所有页面都可以随便换出了。

> 设计 lab 有点像一个奇怪的单元测试，对于模块的覆盖率越高越好，但是似乎就需要对于 lab 有很深的理解了...

我们目前应该还是应该用 rcore-fs 让整个系统跑起来。但是要适当对 rcore-fs 里面的内容进行一些展开。尤其是 Unix 资源 file 类型，支持 open/close/read/write，其传入的参数只有内存里面的缓冲区地址及其长度。这里面就要区分到 file 的不同类型了，如果是 inode 的话其实是每个进程里面都维护一个当前的偏移量，相当于是一个指针，不然就没法读了；pipe 的话是内核负责管理的一个 RingBuffer，读端和写端做的事情不太一样；要么就是其他的设备，这里特指串口，我们需要调用串口驱动程序来实现 read/write。对于一个用户程序，我们需要为它默认打开 fd=0 的标准输入和 fd=1 的标准输出，在 fork 的时候他们会被继承。其实就是一个 `Arc<File>`。仔细想一下文件系统还是特别复杂的，应该还是需要参考一下 rcore-fs。

我们目前的任务感觉变成了在第二版的基础上，借鉴第三版优秀的内存回收机制，然后再把我的多核/k210 的支持加上去，也就是说核心机制还是参考第二版的。不过，我确实还是在第三版里面学到了很多东西吧...希望是这样的。

下午看一下文件系统的代码，看看有哪些是可以直接搬运的，以及怎么跟进程结合起来。再看一下大概需要哪些锁。

关于第一个用户进程，其实我们可以将其硬编码成用户终端，由它完成所有它 fork 出来的子进程的回收。

## 最简单的文件系统

首先我们必须有一个块设备驱动，能够以 512 字节为单位将数据写入磁盘或者将数据读入内存。我们可以认为整个磁盘都是一个文件。

然后，我们*暂时*不需要考虑块缓存或者 dirty 这些东西。

之后应该是我们能够以某种方式理解磁盘上的内容，将其转化为一个文件系统。比如说超级块、Inode 块、数据块还有若干 bitmap 表示块的可用情况。每个 Inode 里面保存文件的元数据，比如文件大小，数据块的索引（可能是直接、间接的），访问权限不做考虑。对于一个文件系统操作（比如创建、删除、修改文件，注意这个时候我们只能通过 Inode 在磁盘上的位置，也就是 Inode ID 来区分文件），我们要将其转化为多次对于块的读写。同样，我们暂时不考虑文件系统操作的事务性，暂且假设在这一系列块读写的过程中系统不会崩溃。

当然，如果只能用 Inode ID 来区分文件是一件很痛苦的事情，我们都已经非常熟悉来通过绝对路径/相对路径来区分文件了，最主要的是文件/目录可以有名字了。实际上这一层做的事情是完成了绝对路径到 Inode ID 的映射，这个映射是多步骤、间接的。

然后是内存中的 Inode，它里面需要维护 readable/writable/offset 表示读写权限和当前指针，由此你可以想象它是对应于一个文件系统中一个特定的文件一个固定的位置，且读写权限有一定限制。这个简单一点的话就直接做成每次直接把 Inode 读到内存里面再附加上 readable/writable/offset 这些信息吧。

然而 Inode 只是可读写的 Unix 资源中的一种。初次之外还可能有管道 Pipe、字符设备等等。所以我们需要通过 enum 的方式将它包装到一个 file 里面。每个进程都会维护一个目前可用的 file 列表。对于进程来讲，它则是可以通过每个 file 在 file 列表中的位置也就是 fd 作为 token 来访问对应的 Unix 资源。目前 open 就设置为只能打开 Inode 类 Unix 资源就好了。对了，好像还需要区分一下文件和目录。

感觉这个用 Rust 来写...emmmmm

## 锁

最简单也是最重要的就是自旋锁 SpinLock。之后还是改成 Spinlock 更好一点？

然后还有一个是 SpinNoIrqLock，似乎是获取锁之后关中断。可能的用途首先是在第三版的内核线程里面，名字叫 Lock，目前是用来包裹 Processor，这个是用来保存每个核上的运行状态。但是内核线程什么时候会访问 Processor 呢？我能想到的可能就是最后 exit 的时候会修改当前核的状态，如果获取了 Processor 锁的情况下进入时钟中断，它会再尝试调度一下再次尝试获取 Processor 锁，然后可能就炸了...那一般情况下应该是来做什么用的呢？正在并发的线程大概有这么几种：一直跑在内核态的内核线程、用户态通过异常（包括 ecall） Trap 到内核态（这个时候我们通常会重新打开中断，不然会显著提高响应时间）、要么就是从某个地方中断到 S 模式的中断处理内核线程。这个分析有点过于复杂了，且好像没有捕捉到本质...xv6 里面的做法是所有的锁都是 SpinNoIrqLock，于是可以保证内核线程、用户态异常 Trap 处理线程在手里持有任意一把锁的时候都不会开中断。这样相对来说粒度比较粗吧...虽然解释的不是非常清楚，但是这好像还算是一种非常好的说明方向。目前到底怎么设计才能不出现死锁不太好说，但是如果是异常 Trap 到内核又把中断打开的话，同步互斥就需要特别注意了，现在的一刀切确实还比较简单。

xv6 里面的 SleepLock 要跟一个 SpinLock 绑定应该是出于语言上的设计。像 tutorial 里面 Condvar 这样的设计还是比较好的，就沿用之前的设计就好了。

## 重要的小功能

由于我们自己实现 spinlock 和 sleeplock(condvar)，这样的话，我们可以尝试仿照 rCore 和 xv6 记录每个 spinlock 被尝试获取了多少次。这个可能有两个维度：记录总体的被获取次数和获取成功的次数来度量并发度，另一个记录连续获取失败的次数，在获取成功后清零，如果这个次数超过了一个阈值的话则可以判定为死锁直接 panic，这个就确实需要手写一个 Mutex 了，完成之后我们至少可以知道什么地方出现了死锁。这个目前已经搞定了。

为了方便 debug，可以考虑加入一个 backtrace 功能来处理死锁和 panic。逻辑是，在成功获取锁之后，将当前的 backtrace（目前设定为一个定长的地址序列）保存下来。然后一旦发生死锁，那么首先打印锁里面保存的 backtrace（这样我们就知道是通过什么路径获取锁的），然后打印当前的 backtrace，这样的话调试可能会稍微方便一些。

## 一些小功能（相对优先级最低）

1. 加入方便的多级 debug 功能，可以通过编译时加入命令行参数来设置输出哪些 log，比如说 info, warning, error 几种。
2. 把 xwh 的 sd 卡烧写小工具用起来，它的好处在于不用拔插 sd 卡，也不需要使用读卡器。但是速度会相对慢一点，毕竟是通过 spi 而不是 usb。

## lab 设计

预备 lab1：Rust 语言学习

预备 lab2：RISC-V 汇编编程，代码在模拟器上运行

### Chapter0：我们的内核要做些什么事情

这里借鉴一下 ostep/xv6 里面对于进程的初步介绍，这里我们站在用户的视角。说不定还可以参考一下 Unix 编程之类的东西？

### Chapter1：环境配置/最小化内核

代码目标：能够在 qemu/k210 通过 RustSBI 启动内核并在屏幕上格式化输出

阶段性目标：完成环境配置、跑通流程

练习题：应该不用设置练习题？

备注：putchar 姑且使用 sbi 提供的接口即可，直接访问物理内存

### Chapter2：Trap

代码目标：支持时钟中断/ebreak，每秒钟输出一行 1000ticks

相关知识：搞清楚栈的作用，RISC-V 调用规范，到底什么是需要保存的执行现场，RISC-V trap 相关硬件机制，trapframe 里面需要保存什么/保存在哪里，*也许可以初步引入执行流的概念*？

阶段性目标：整体在 S 态，跑一个启动线程，支持从 S 态 trap 到 S 态

练习1：实现 backtrace 函数，进行一串函数调用，在最底层调用 backtrace。这是为了更好了解调用规范。

练习2：原先是想设置成 sleep(n) 的。但是实在太简单了，也没有增进 trapframe 的理解。只有它自己的话真的很难设计啊。

备注：仍然直接访问物理内存。

那你对执行程序有啥用呢：有了时钟中断之后我们至少能够找到任务切换的时机了！

### Chapter3：内存资源管理（这一章感觉更类似于添头...）

代码目标：支持物理页帧分配/内核堆分配

相关知识：简单的内存布局，Rust 对于堆分配的支持，两种相关算法，在 Rust 语言方面需要讲解的是 drop

阶段性目标：内存资源的两个核心管理机制

练习：这一章不设置练习，或者说可以有练习但不是以 lab 的形式

备注：仍然直接访问物理地址。

### Chapter4：虚拟地址空间

代码目标：内核初始映射、支持虚拟地址空间的创建、进行内核重映射

相关知识：RISC-V 页表机制、对于多级页表、TLB 的一些讨论。

阶段性目标：支持虚拟地址空间的创建和回收。

那你对执行程序有啥用呢：有了页表机制之后我们终于可以认为每个进程独占一个巨大的虚拟地址空间了。

### Chapter5：用户进程

创建一个用户进程，只需构造一个中断帧然后 `__restore` 就能够回到用户进程开始执行。系统调用也是支持的，最开始我们只需要支持向终端写入字符和退出两个 syscall 即可。

阶段性目标：用户进程创建、传参

### Chapter6：内核线程、线程切换与线程调度、多进程、重要系统调用

代码目标：支持内核线程的创建和内核线程的切换。只需要从启动线程切换到另一个线程，再切换回来就行了。

相关知识：

阶段性目标：

那你对执行程序有啥用呢：

### Chapter7：同步互斥

代码目标：手动实现 spinlock 和 condvar（说明轮询/阻塞之间的区别）

### Chapter8：文件系统

代码目标：从驱动程序一直到上面实现一遍...要包含多种 Unix 资源。令人害怕......

### Chapter9：多核支持





### lab1：Linux 内核编程（阅读 chapter0）

描述：编写一些用户程序熟悉 kernel 所提供的 syscall 的使用方法。有 Linux 内核编程的感觉了。

目的：熟悉 Linux 的进程、文件等概念，熟悉常见的 fork/exec/wait/read/write 等 syscall。

### lab2：Trap（阅读 chapter1-5）

目的：了解函数调用和系统调用的区别，...

练习1：在内核里面实现 panic backtrace。

练习2：类比 xv6，实现一个循环调用，具体内容略过。

备注：这里要连续阅读 5 章，似乎学习曲线有些陡峭。不过第五周已经讲完进程和线程了，应该还行。

### lab3：虚拟存储（无需阅读新章节）

目的：在实验里面算法其实反而是最不关键的。

### lab4：Copy on Write（阅读 chapter6）

目的：对于 fork 的深入理解。

### lab5：

### lab6：

### lab7：同步互斥

在内核里面开一些线程进行同步互斥，目前的经典问题肯定是哲学家就餐问题或者是 MPSC 这种东西。需要首先保证不出现死锁，然后就是通过一个等待线程计时，要么就是统计所有锁的被阻塞次数来度量并发性能。

### lab8：文件系统

增大通过引入二级索引块增大支持的文件容量，但是这个只涉及到磁盘布局这一层，覆盖面不是很广。

新增一个文件系统 syscall，大概是 link/unlink 或者是软链接？总之我们希望文件系统的各层都能够得到修改。

mmap 也算是一个可选项？

## lab 设计：2020-10-28 新

### 可能的章节与代码风格

* 每个章节给出完整可运行且带有完整注释（可以通过 rustdoc 工具生成 html 版）的代码。
* 文档中给出重要的代码片段（照顾到纸质版的读者，事实上在网页版给出代码的链接即可）而并不需要完整的代码，但是需要有完整的执行流程叙述，对于边界条件有足够的讨论。在文档中插入的代码不带有注释，而是将解释放到文档的文字部分。
* 类似xv6，每一章的小节描述一项小功能是如何实现的，不同小节之间可能有一定的先后关系，也有可能是并列的。
* 尽可能讲清楚设计背后的思想与优缺点。
* 2020-10-28：前几章 Chapter1-4 需要等具体实现出来之后再规划章节。

### Chapter0 最小化内核

内核应完成功能：在 qemu/k210 平台上基于 RustSBI 跳转到内核，打印调试信息，支持内核堆内存分配。

章节分布：基本上和第二版/第三版一致。

### Chapter1 系统调用

主要动机：分离用户和内核特权级，用户需要请求内核提供的服务

用户程序：放置在一个固定的物理地址上，完成的功能为：完成一些计算任务，打印几行字符，然后死循环。

内核应完成功能：设置好内核和用户运行的栈，内核初始化完成后通过 sret 跳转到用户程序进行执行，然后在用户程序系统调用的时候完成特权级切换、上下文保存/恢复及栈的切换

实现备注：将编译之后的用户镜像和内核打包到一起放到内存上

新增系统调用：sys_write

### Chapter2 CPU复用Part1, 非抢占式调度

主要动机：多任务，因此需要实现任务切换

用户程序：相比 Chapter1，变成有多个用户程序(2~3 个)放在不同但固定的物理地址上，功能类似

内核应完成功能：实现通过 sys_yield 交出当前任务的 CPU 所有权，通过 sys_exit 表明任务结束。需要为每个任务分配一个用户栈和内核栈，且需要实现类似 switch 用来任务切换的函数。

新增系统调用：sys_yield/sys_exit

### Chapter3 内存隔离安全性：地址空间

主要动机：增加用户程序的怀疑，一个任务不能访问内核或其他任务的代码/数据

用户程序：相比 Chapter2，不同的用户程序之间可以使用存在交集的虚拟地址。

内核应完成功能：实现物理页帧分配器和页表机制。将用户程序打包放在内核的数据段中。

> 说明：目前还是不区分用户和内核空间，因为成本实在太高。但是必须要通过某种手段减少用于页表映射的物理页帧开销。

### Chapter4 CPU复用Part2，抢占式调度

主要动机：增加用户程序的怀疑，如果它死循环或者一直不 yield 怎么办？

用户程序：相比 Chapter3 不变。

内核应完成功能：基于时钟中断划分时间片，并以此来实现一些FIFO或优先级调度算法。

新增系统调用：sys_set_priority

### Chapter5 进程及重要系统调用

主要动机：引入重要的进程概念，正式将任务抽象为一个进程，实现一系列相关机制及 syscall

内核应完成功能：实现完整的子进程机制，初始化第一个用户进程 initproc。

新增系统调用：sys_fork/sys_exec/sys_wait/sys_kill/sys_getpid 以及之前 sys_yield/sys_exit 的更新

章节分布：

#### 背景：进程退出状态、子进程机制与相关 syscall 介绍

#### 初始用户进程 initproc

#### 实现 sys_fork

#### 实现 sys_exec

应用：用户终端 user_shell

#### 实现 sys_wait（轮询版本）

#### 实现 sys_kill

### Chapter6 同步互斥讨论

主要动机：解释内核中已有的同步互斥问题，并实现阻塞机制。

内核应完成功能：实现死锁检测机制，并基于阻塞机制实现 sys_sleep 和 sys_wait 以及 sys_kill

新增系统调用：sys_sleep 以及 sys_wait/sys_kill 的更新

章节分布：

#### 基于原子指令实现自旋锁

* 讨论并发冲突的来源（单核/多核）
* 关中断/自旋/自旋关中断锁各自什么情况下能起作用，在课上还讲到一种获取锁失败直接 yield 的锁
* 原子指令与内存一致性模型简介
* 具体实现
* 需要说明的是，课上的锁是针对于同一时刻只能有一个进程处于临界区之内。但是 Rust 风格的锁，也就是 Mutex 更加类似于一个管程（尽管 Rust 语言并没有这个概念），它用来保护一个数据结构，保证同一时间只有一个进程对于这个数据结构进行操作，自然保证了一致性。而 xv6 里面的锁只能保护临界区，相对而言对于数据结构一致性的保护就需要更加复杂的讨论。

#### 死锁检测

#### 阻塞的同步原语：条件变量

* 简单讨论一下其他的同步原语。

  课上提到的信号量和互斥量（后者是前者的特例）保护的都是某一个临界区

#### 基于条件变量实现 sys_sleep

#### 基于条件变量重新实现 sys_wait

#### 更新 sys_kill 使得支持 kill 掉正在阻塞的进程

### Chapter7 设备驱动

主要动机：将 I/O 设备用起来。

内核应完成功能：实现块设备驱动和串口驱动，理解同步/异步两种驱动实现方式

#### 背景知识：设备驱动、设备寄存器、轮询、中断

#### 设备树（可选）

#### 实现 virtio_disk 块设备的块读写（同步+轮询风格）

#### 实现 virtio_disk 块设备的块读写（异步+中断风格）

#### 实现串口设备的异步输入和同步输出

* 参考 xv6，可以在内核里面维护一个 FIFO，这样即使串口本身没有 FIFO 也可以

### Chapter8 Unix 资源：文件

主要动机：Unix 万物皆文件，将文件作为进程可以访问的内核资源单位

内核应完成功能：支持三种不同的 Unix 资源：字符设备（串口）、块设备（文件系统）、管道

新增系统调用：sys_open/sys_close

#### 背景知识：Unix 万物皆文件/进程对于文件的访问方式

#### file 抽象接口

* 支持 read/write 两种操作，表示 file 到地址空间中一块缓冲区的读写操作

#### 字符设备路线

* 直接将串口设备驱动封装一下即可。

#### 文件系统路线

* 分成多个子章节，等实现出来之后才知道怎么写

#### 管道路线

* 一个非常经典的读者/写者问题。

### Chapter9 多核（可选）

#### 多核启动与 IPI

#### 多核调度

### Appendix A Rust 语言快速入门与练习题

### Appendix B 常见构建工具的使用方法

比如 Makefile\ld 等。

### Appendix C RustSBI 与 Kendryte K210 兼容性设计

### 其他附录...





