先来读一下ch8的文档然后回顾一下QAQ

# 读文档

## 引言

暂时没什么问题。

## 用户态线程管理

这里讲了一下green thread，但不算特别清楚，我们也许可以自己写一个更简单的版本。

另一个问题是：green thread有没有必要放在这里也是个值得商榷的问题。

## 内核态线程管理

讲的貌似比较全面，但总感觉少一些东西。

比如`thread_create`没有提到可以传参区分不同线程的问题。

## 锁机制

为什么需要锁：背景是多线程共享数据协作，由于内核调度/中断导致出现数据竞争/race condition，或者叫什么失去同步？

锁的基本思路：忙等和睡眠锁好像不太好说成是两种不同的锁机制？这里提到要关注锁的不同属性：互斥、公平和性能

> 经典四条规则：空闲则入、忙则等待（互斥）、有限等待（公平，至少避免饥饿）、让权等待（此处权指的是CPU使用权，阻塞）
>
> 这里另有一个问题：就是到底阻塞是指什么？

1. 用户态软件级实现：直接while一个单变量是不行的，于是直接过渡到双进程的Peterson算法，好像跨度有点大。后面可以拓展到多进程的Dekkers和Eisenberg算法（这个测例里面有）。这些算法在现代处理器架构下还成立吗？显然需要考虑到访存模型。
2. 机器指令硬件级
   * 关中断：保证临界区内不会被调度。缺点：过度放权给应用。而且多核情况下不适用。
   * 基于原子指令：这里介绍了不同架构提供的原子指令，如CAS/TAS以及RV上面的AMO和LR/SC指令，利用它们实现自旋锁。然而问题在于如果在用户态仅使用这个的话可能会由于忙等而浪费CPU时间。内核由于拥有更多的信息可以通过阻塞来提高总体效率。比如`futex`系统调用。（这里没有提到内存一致性模型和内存顺序，有必要吗？）
3. 内核态操作系统级
   * 为了解决上面提到到浪费CPU时间忙等的问题，可以使用`_yield`甚至`sleep`，但会导致执行延后甚至饥饿
   * 然后这一小节提到了一个`MutexBlocking`，还没介绍最关键的`block_current_and_run_next`

本章所有同步原语自身操作能够保证原子性的原因：单核，于是同时只有一个线程在系统调用

单内存位置：原子变量；多内存位置：以原子变量为标志，附加内存一致性模型

## 信号量机制

关于0的一些判定条件似乎有些问题。而且很多说法不严谨，需要重置。

信号量两段伪代码有什么区别？

基于信号量实现同步。

## 条件变量机制

开头的例子好像说明还不是很充分。

## 并发中的问题

这里面列举了三种问题：

* 互斥缺陷：指对于共享变量没有加锁
* 同步缺陷（违反顺序缺陷）：需要使用信号量等同步原语限制线程的执行依赖关系
* 死锁缺陷：死锁产生的必要条件/死锁避免/银行家算法

# 读 OSTEP 的并发部分

## 26 introduction

每个程序可以有多个执行点（point of execution）

线程间的上下文切换不必切换地址空间，然而仍然需要保存和恢复一些内容

每个线程都有自己独立的栈，引入TLS概念（然而，堆仍然是共享的，而且栈之间可能会冲突，尽管不太可能）

为什么使用线程：首先是可能充分利用多核获得更好的并行度；其次是不必整个程序完全放弃CPU使用权等待I/O，在I/O没有准备好的时候可以做一些其他的事情。这些使用多进程架构也可以达成，然而使用线程协作会更加自然，共享数据的性能更高（相比进程间通信）。

> 锁：我觉得更偏向于逻辑意义上。
>
> 锁的粒度：决定最大可能的并行度，也就决定总体效率。
>
> 万物起源于RAM，至少在当下的冯诺依曼架构之下。
>
> 对于数据竞争，需要有一个仲裁者。这可以是CPU和RAM之间的总线，也可以是某层软件。
>
> 先假定单核、简单访存模型（如何简单？），然后再尝试拓展到多核+多级缓存复杂微体系结构，这二者之间需要区分，但是本章不一定需要过多涉及多核。
>
> 就并发这一节来说，可以介绍在我们简单系统中的并发以及真实多核环境下的并发（比如常用的Rust高性能并发库`crossbeam`）。
>
> 为了实现原子化：可以依赖于硬件提供的特殊指令、操作系统内核支持、某些纯软件算法或者它们的组合。当然，每种做法涉及到不同的取舍...严格来说分析起来也许并不容易。
>
> 内核为什么是并发程序？事实上在第三章任务切换的时候就已经能够看到了。为什么引入`UPSafeCell`?
>
> 内核态并发和用户态并发的不同点：用户态并发可以有内核的支持！
>
> 无论是单核还是多核，**交错**是比访存模型更应该强调的内容，即使前者假定访存模型为最为严格的顺序一致性仍有一些问题需要注意。
>
> 提高并发度意味着放宽执行限制条件，这样正确性更难以保证，但是有可能达到更高的性能。
>
> 一个我经常喜欢说的：串行化
>
> 问题是：线程并发时的互斥需求，也就是不允许两个线程同时处于某个共享数据结构的临界区之内（这里还是需要将线程看成静态的状态机，每执行一条指令视为一次状态转移），如何满足这样的互斥需求？就是需要想办法让所有的临界区原子化，也可以说是在逻辑上将它们串行化（也许不一定正确的形象描述：两队并作一队）。
>
> 这样一种很自然的做法是：进入临界区的时候打上一个标记，表明已经有线程在临界区之内了，然后离开临界区的时候将标记清除。注意这个标记本身也属于共享数据。对于单标记的情形，使用普通的访存指令就不行了，我们需要转而使用原子指令。然而我们还可以使用多标记，可以不用借助原子指令也能够保证正确，然而其正确性不容易看出，总体也更为复杂。
>
> 所以，从总体顺序上看来，我们应该先讲解纯软件做法，然后过渡到基于原子指令比较简单的做法。
>
> 但是，只要引入原子指令，就不可避免的涉及到内存顺序。所以这里面我们再回顾一下吧。
>
> 顺序就是指：能够读到最近一次写入的值。顺序一致性模型（这家伙严格定义比较绕，我也不知道是否等价于下面的说法）显然能够满足这样的要求，它的本质是总线将所有的访存操作串行化。如果考虑到缓存的存在，那么即使单个内存位置也不容易了。于是这个时候我们可能会使用诸如MESI这样的缓存一致性协议（cache coherence protocol）。回顾一下“读到最近一次写入的值”，就会有一个问题，假如一个核写入了一个值，在一段极短时间之后另一个核读这个值，那么另一个核是读不到这个值的，因为这段时间可能不足以完成共享缓存上的一次通信。
>
> 每个核会看到一个顺序，但是不同的核看到的顺序不一定一致。
>
> 普通的访存指令相对来说比较随波逐流，就是在当前时间点看到什么就是什么。原子指令的话，除了一条指令浓缩多阶段操作的话，如果使用Relaxed顺序的话，就基本也是随波逐流。
>
> 考虑一下TSO允许违反W->R（这里的约束是指单线程上的吗？因为多线程之间本来是没有顺序的）。需要注意到在多级缓存、多核且存在缓存一致性协议的情况下，每次读写都可能很复杂。对于后面的acquire-release语义的话，...
>
> 一条重要支线：从程序员编写代码到最终执行之间，在顺序上带来的差距：首先编译器会进行优化，可能调整指令的先后顺序；处理器也可能多发射+乱序执行，虽然最后还是顺序提交，但是访存请求可能是乱序提交的（这一点可能有点口胡）。如果是编写应用的程序员的话，可能还需要考虑到操作系统的调度。这些因素都可能使得最后的执行结果与我们的预期不相符，因此我们可能要手动加上一些限制来精确描述我们期望的行为。
>
> [C++这里](https://en.cppreference.com/w/cpp/atomic/memory_order)讲的比较清楚（以后看这个就行了！），可能在编程语言这边相比ISA确实要容易一些。我们不太需要考虑标准库和指令如何配合实现编程语言规范，也许在某些ISA上原生支持，其他ISA上就没那么容易。但总体来讲我觉得规范应该还是尽可能贴近大多数主流ISA的。注意比如acquire和release或者更加严格的语义，约束是分成当前线程和线程之间两部分的。
>
> 在这样的基础上，重新考虑Rust的无畏并发：`Send`和`Sync`。

`thread_create`和`thread_join`的例子说明其中一种不确定性来源：内核调度器（包括线程创建和线程调度）。

让线程变得更加糟糕、难以使用的另一个原因：共享数据。

经典例子：两个线程多次修改一个全局计数器。

结果不确定|反汇编|这里只是考虑到时钟中断以及对应的调度，也强调即使多个线程都使用相同的寄存器也不会引起冲突

注意这个时候我们其实仍然假定这是一个单核、简单访存模型，于是看起来能够影响我们的只有时钟中断和调度了。

借由这个例子引入了数据竞争、竞态条件、临界区还有互斥的概念。

然后是引入了一个原子操作的插曲，简单来说就是"all or nothing"，它在计算机体系结构、并发代码、文件系统、数据库系统、分布式系统这些系统软件中都有十分广泛的应用。特别是在数据库的语义中，将一组操作原子化之后称为一个事务。在这本教材关于并发的内容中，仅会使用同步原语将短小的指令序列转化为原子块。但实践中原子性的内涵远不止于此。比如write-ahead log来避免文件系统操作过程中的失败。

那么如何得到我们期望的原子性呢？使用原子指令可以对同内存位置的简单访存命令的组合原子化。但是对于多内存位置的复杂数据结构的操作在多线程环境下也需要原子化。硬件不可能为每一种可能的操作都提供一条对应的指令。于是，实践中硬件提供一组比较通用的指令，我们基于它们构建常用的同步原语，进而为我们的数据结构实现定制化的原子化。注意这里可能还涉及到操作系统的介入。

多线程在访问共享数据的时候需要做到互斥，然而还有另一种常见的需求：即约束线程之间的执行顺序。比如在某个时刻线程A必须等待线程B达成了某条件之后才能继续执行下去。这样的话我们可以将线程并发执行涉及到的同步和互斥两个需求统称为同步互斥，事实上它们也经常会存在交叉。

概念：

* 临界区是指访问线程间共享资源的代码片段。
* 数据竞争、竞态条件是指多个线程同时（？）进入临界区，它们都尝试与共享资源交互，导致不正确的结果。
* 为了避免这些问题，需要引入互斥原语。

为什么要在OS课程中讨论并发？首先是历史原因，**内核是最早的并发程序**，为了使内核正确运行需要用到很多并发技巧。后面多线程程序也需要用到同样的技巧（不同的一点是这一次我们可以有操作系统的支持了！）有一个比喻也许很好：内核就像一个庞大无比的数据库，每一次中断相当于一次增删改查。

## 27 thread API

这里面主要提到了：

* `pthread_create`

* `pthread_join`

* `pthread_mutex_lock`

* `pthread_mutex_unlock`

* `pthread_mutex_init`

* `pthread_mutex_trylock`当锁已被持有时返回错误

* `pthread_mutex_timedlock`尝试获取锁，存在超时设置

* `pthread_cond_wait`注意参数中带有一个互斥锁的指针

* `pthread_cond_signal`

## 28 locks

尝试使用不同的方式构建互斥锁。

首先给出了锁的定义：这是一个变量，记录当前是否有线程在临界区中。然后有两个操作：`lock`和`unlock`。

可以直接使用pthread提供的互斥锁，然而在保护复杂的数据结构的时候需要考虑到锁的粒度。

锁的构建依赖于：硬件和OS支持。

评估锁：互斥、公平性和性能。

尝试1：关中断。

尝试2：普通访存，单flag，自旋。

尝试3：原子访存（分别基于test-and-set/CAS/LL/SC原语），单flag，自旋。

顺带一提：纯软件算法（如Peterson算法和Dekker's算法）不如提供一点硬件支持简单，而且**无法用在现代体系结构的松散一致性模型上**（这个我倒未曾考虑到，或者说应该可以用，只是效率比不上直接用硬件原语）。

三个维度评估尝试3。

尝试4：原子访存（基于fetch-and-add原语）构建ticket-lock，...保证公平性。

如何减少自旋消耗的CPU时间？（顺带一提：不使用自旋的另一个原因是为了避免优先级反转，解决方案可以使用优先级继承）

方案1：获取失败之后直接yield，上下文切换开销可能过大，而且并未解决饥饿的问题。

方案2：阻塞（也许不一定需要OS支持？不过书里提到需要OS支持，先这样吧）

最后提到了Linux中的`futex`系统调用。

## 后面的等写到后面的时候再看，重点显然是locks这一节

# 第八章代码变更

* `config.rs`里面新增了一个`MEMORY_END`
* `lang_item.rs`里面引用了新增的`current_kstack_top`，新增了panic时候的`backtrace`
* `mm/memory_set.rs`中`from_elf`的时候不再对用户栈进行映射了
* 新增`sync/condvar.rs`(我的建议是将里面的`wait`接口改成`wait_with_mutex`之类的，好像也不一定？)
* `sync/mod.rs`相应修改
* `sync/mutex.rs`新增两种互斥锁实现`MutexBlocking`和`MutexSpin`
* `sync/semaphore.rs`新增信号量实现
* `syscall/fs.rs`从`current_task`变成`current_process`
* `syscall/mod.rs`将sleep变成一个系统调用，然后新增了第八章的系统调用
* `syscall/process.rs`相关系统调用：`getpid`,`fork`,`exec`,`waitpid`,`kill`，大多数都是比较普通的修改
* `syscall/sync.rs`新增同步互斥相关系统调用的实现
* `syscall/thread.rs`新增线程相关系统调用的实现
* `task/id.rs`可以认为是每个线程的资源
* `task/manager.rs`有一些可能比较微妙的修改，还不好说
* `task/mod.rs`最重要的是新增了`block_current_and_run_next`，修改了`exit_current_and_run_next`的逻辑，还有一个`remove_inactive_task`是及时在退出一个进程的时候销毁所有阻塞的线程，这个实现还不完善，可能也还没有在后面章节使用
* `task/pid.rs`被`task/id.rs`代替了
* 对于进程而言，`task/process.rs`代替了原先的`task/task.rs`（这个现在归到线程那里了），以前进程的那些经典接口还是要保留的
* `task/processor.rs`里面应该也是进行一些相关改动
* `timer.rs`使用条件变量优化sleep系统调用
* `trap/mod.rs`中，在`trap_return`中，每个线程在所属进程地址空间中有自己独立的Trap上下文了
* 第八章线程提前退出独有的测例`early_exit`还有`early_exit2`
* 探索互斥锁实现相关测例：`race_adder`,`race_adder_arg`（这个传入的参数其实有问题），`race_adder_atomic`,`race_adder_loop`,
* 软件方法实现互斥锁测例：`peterson`和`eisenberg`
* 互斥锁测例：哲学家问题`phil_din_mutex`，`race_adder_mutex_blocking`,`race_adder_mutex_spin`
* 信号量相关测例：mpsc问题`mpsc_sem`，
* 有栈、无栈协程相关测例

# 内核级线程管理

## 代码变更

``task/id.rs``: 涉及两个分配器，PID 分配器和内核栈分配器，内核会为每个线程分配一个内核栈。内核栈有一个ID，其在内核地址空间中的地址根据此ID来计算。分配之后RAII的资源有`PidHandle`（这个在PCB中），`KernelStack`（这个在TCB中），还有一个`TaskUserRes`，其中包括一个`tid`，一个应该是从`tid`算出来的`ustack_base`，一个所属进程的弱引用。看了当时的实现发现这个并不是创建之后就直接分配内存的。分配的资源就是在进程地址空间里面映射Trap上下文和用户栈。每个进程还有一个TID分配器，不过名字叫做`task_res_allocator`了。

`task/id.rs`是代替原先的`task/pid.rs`。原先的话是直接弄了一个`PidAllocator`，`PidHandle`也是之前就有的，`KernelStack`也是之前就有的。

这个好像应该是最主要的变化了。

主要数据结构的变更：

`TaskControlBlock`从进程控制块变成线程控制块，另一种说法是调度的单位——任务从进程变成了线程。

新增了进程控制块`ProcessControlBlock`。

任务管理器`TaskManager`调整了一些相关的接口，然后就是新增了early exit相关的内容。

处理器管理结构`Processor`好像没什么变化。

这样的话我们可以尝试从几个不同的角度依次说明：

* 数据结构设计、内存布局
* 进程接口变更
* 新增的线程接口

需要注意，我们规定进程和线程接口是不相容的，不然会导致UB。

# 锁机制

线程共享数据（全局变量进行协作）

引入互斥锁，上锁-临界区-解锁

互斥锁的多种不同实现
