## 21/06/22

先开个头，过会完善。

思路就是先分出一个核S负责执行所有的系统调用。其他核一旦通过系统调用陷入到内核需要将系统调用请求提交给核S，等到核S处理完毕之后再唤醒来源核上对应的task。这就是所谓第一阶段的计划。

将目前的实现过程记录在这里。

首先，要完整运行的话，只需在zCore根目录下`make rootfs && make libc-test && make other-test && make image`，随后在zCore/zCore目录下`make run LINUX=1 SMP=2 LOG=warn MODE=release ACCEL=1`即可。

根据此前的分析，我们希望将`loader/linux.rs`中`handle_user_trap`函数中的`syscall.syscall.await`执行过程移动到hart 1上面。然后其余的地方保持不变。应当如何实现呢？首先应该新创建一种Future（姑且称为`RemoteSyscallFuture`），这种Future会将`syscall.syscall`包裹起来，当poll该Future的时候，将以某种方式将syscall请求提交到远程syscall模块，随后返回`Poll::Pending`，这样才能在hart 0的运行时上面暂停该任务。那么如何将其唤醒呢？远程syscall模块自己的运行时将以poll的方式执行syscall，在执行完毕之后应该以某种方式唤醒hart 0上的任务。

那么，我们可能需要研究一下现有的Waker机制。看了一下已有的叶子Future的实现，其中一个比较典型的例子是`YieldFuture`，是在我们调用`yield_now`的时候用到的，而`yield_now`用于在裸机模式下实现基于时间片的抢占式调度。`YieldFuture`的实现是内部保存一个flag，首次poll的时候flag为false，此时将flag设置为true，调用`cx.waker().wake_by_ref()`，然后返回`Poll::Pending`；第二次poll的时候发现内部flag为true则直接返回`Poll::Ready`。如果想知道这里面的Waker做了什么事情，就需要找到顶层Future管理器以及对应生成的Waker本身。我能想到的一个比较高层的Future是`run_user`生成的，不过这个大概还不是顶层Future。这个Future还会被`ThreadSwitchFuture`包裹起来（详见zircon_object里面的thread.rs）并提交给kernel_hal。于是这个`ThreadSwitchFuture`又是什么呢？它只是会poll里层Future，但是在poll之前先切换到对应进程的地址空间。那么我们发现在这里是找不到对应的Waker的。结论：`ThreadSwitchFuture`应该就是顶层Future。

那么看起来Waker的实现还是要在executor库如PreemptiveScheduler中寻找了。看了一下实现比较复杂，但是总体上来说，在每个task被spawned到运行时中的时候我们会为其生成一个对应的Waker用来唤醒它，内部应该是使用到了某种索引。同时，事件循环的逻辑是，每次会取出（也即take）一个task，然后尝试poll这个task。如果结果是Ready的话则销毁对应的Waker，结果是Pending的话则什么都不做，因为task已经被移除了。应该是当我们调用`cx.waker().wake_by_ref()`的时候task才会被重新加入到运行时中。重新思考一下`YieldFuture`，第一次poll的时候，注意在poll之前task已经被移除了，所以这个时候要重新将其加入到运行时中，相当于从队头移动到队尾，是经典的Round Robin操作。这样就有了第二次被poll的机会，这次的话直接返回Ready即可让task继续向下运行。

总结一下，如果使用的是rcore-os/executor或者PreemptiveScheduler这些运行时的话，需要满足的要求是在顶层Future被poll之前它们会被移除。同时需要在适当的时机调用Waker将顶层Future重新放置到就绪队列中。不过，目前我们希望自己重新实现一套运行时，就不必遵循这里的规则了。但是这可以作为参考。这里可以提炼出一条重要的规则：Waker应该是由运行时负责生成和管理的，这也跟之前async_modules中的经验一致了。

还有一些问题。

第一个问题：我们是否要为其实现自己的调度器，还是沿用已有的PreemptiveScheduler？

> 目前来看，为这种模块实现一个模块管理器并在里面自己实现一个event loop，也就是可以参考之前的async_modules看上去是比较好的。这样就不用去计较PreemptiveScheduler的种种细节了，同时也更加灵活。

第二个问题：SubmissionQueue和CompletionQueue是否还需要？

> 如果照搬之前的async_modules的话，自然还是需要的。

然后就是考虑系统调用本身的实现是同步还是异步的。同步的自然比较简单。对于异步的，假设其与I/O有关，且我们不打开hart 1的中断使能，那么也就有这么一种复杂的情况：在hart 0接收到外设中断之后，需要以某种方式通知hart 1该外设上的数据已经准备好了，从而唤醒hart 1运行时上的syscall任务。如果实现不好的话，这一步可能涉及到大量的数据拷贝。另外，实际情况是否真的有这么复杂，还需要一些代码阅读（比如，可能是一些软件上的异步事件）。

现在总体上考虑一下第一阶段的设计吧。

`RemoteSyscallFuture`是对`syscall.syscall`的封装，确切来说，是`syscall.syscall(num as u32, args)`这个Future的一个wrapper。注意，这个Future的返回值是一个isize。这个Future会比较奇怪，因为它是hart 0和hart 1上两套运行时之间沟通的桥梁。首先，它是hart 0运行时体系中的一个叶子Future。当我们在poll这个Future的时候，`cx.waker().wake_by_ref()`可以用来唤醒hart 0中的顶层任务。唤醒的时候自然是hart 1上的一个系统调用已经执行完毕了，hart1应该用怎样的方式通知hart 0？所以一种不错的思路可能是将`cx.waker().wake_by_ref()`包在一个闭包里面由hart 1直接去调用。当然`RemoteSyscallFuture`内部的记录也需要在某个时候更新，因为它还需要正确的返回一个isize。

因此，当第一次poll `RemoteSyscallFuture`的时候，我们需要将`syscall.syscall`这个Future、唤醒当前task的`cx.waker().wake_by_ref()`的闭包（以直接拷贝或引用的形式，比如说不是Future本体而是所有用来生成该Future所需的信息）、还有一个指向`RemoteSyscallFuture`内部的`*mut isize`（这个是用来保存syscall返回值）提交到hart 1，然后就可以返回`Poll::Pending`。这将使当前task在远程syscall返回之前不会被放回到hart 0运行时中。当hart 1完成了远程syscall之后，将返回值写回到`RemoteSyscallFuture`，然后调用传进来的闭包唤醒hart 0上的task。过了一段时间之后，hart 0上的`RemoteSyscallFuture`将会第二次被poll到，此时我们直接返回`Poll::Ready`，带上`RemoteSyscallFuture`内部的syscall返回值即可。

这样来看的话，我们的AsyncModule固然需要Submission Queue提供的filter/reordering等能力，Completion Queue可能是真的不需要了。因为如果一个模块要调用其他模块的话，它可以向其他模块在提交任务的时候带上一个闭包来允许对方唤醒自己的某个任务，这样的实现可能会比较方便？另一方面，从局部性上来说也不一定会劣于Completion Queue的实现。

如果只限定于`RemoteSyscallFuture`，AsyncModule的实现？由于没有调用到其他的模块，至少是不需要考虑Waker的。然后显然需要有一个无锁的Submission Queue方便hart 0来提交任务。自身的事件循环的话，就是从Submission Queue取出hart 0的请求来填充自身的任务队列，然后poll已有（在hart 0上生成的）或是在hart 1上原地生成的Future。这里最大的问题是Reactor的实际应用，也即哪些事件能让中间暂停的syscall继续执行，这就需要调研已有的异步syscall之所以为异步的原因。某些事件若想完成跨核通信的话，也许并不容易。

如何使用宏来降低实现复杂性？

同时将一些可能的困难记录一下。

第一点：`Syscall`里面有一个`CurrentThread`的引用，回顾一下知道`CurrentThread`只不过是`Arc<Thread>`的一层封装。也就是说这可能会为我们将其移动到另外一个核上去执行带来一些本不必要的困难。我们可以考虑直接将`Arc<Thread>`移动到另一个核上，然后再顺势生成`Syscall`实例来有效利用已有的API。那么zCore里面又为何是这样设计的呢？`CurrentThread`自身是被传入`run_user`闭包中的，然后`run_user`里面就一直以引用的形式访问它。不对它进行clone，是出于性能考虑，还是必须保持引用计数不变？目前还不清楚。不过，我觉得如果将`CurrentThread`复制一份传到另一个核上应当没有什么问题。

第二点：zCore并没有采用KPTI的设计。因此，在hart 1上，每次在执行一个syscall之前都需要先切换到线程所在进程的地址空间。在hart 0上这是由顶层Future `ThreadSwitchFuture`保证的，在hart 1上就没有了。所以我们后续需要将zCore改成KPTI的设计吗？从性能和实现复杂性两方面考虑一下？

第三点：网卡驱动和相关的测例（还有其他测例）需要调研一下。目前如果只考虑Linux裸机上的测例的话，注意`zCore/tests`目录，大概可以分成一大堆libc测试（进而分成functional/math/musl/regression几类，其中regression类别中包括pthread/printf相关测例，这些测例的源码可以在`zCore/libc-test`目录下找到）、与busybox有关的测试，还有就是`tests/src`目录下的socket（包括tcp/udp）和串口相关的测试。但是`tests/src`这里的测试还没有跑起来，具体的环境配置方式还需要再了解一下。最终生成的rootfs可以在`zCore/rootfs/x86_64`目录下找到，`x86_64/bin`里面除了busybox还有一些test开头的测例，比如`testpipe1`等等。这些测例的源码可以在`linux-syscall/test`目录下找到。于是测例的位置我们大致上找到了。

接下来是研究一下网卡驱动是否真的是异步的。zCore这里对于设备的抽象相当复杂，可以说是跟rCore一脉相承了。想当初最早接触rCore的时候也是这里最让我头大。我们从系统调用入手吧，可以看到sys_socket是同步的，而sys_connect和sys_accept都是异步的。例如sys_connect，最终是调用了`Socket` Trait的`connect().await`。这个`Socket` Trait可以在linux-object中找到，看起来目前应该是为`TcpSocketState`和`UdpSocketState`实现了这个Trait。比如`TcpSocketState`...等等，这里看起来好像是个悲伤的故事，`TcpSocketState`的connect/accept被声明为async fn，但实际上都是基于轮询的同步实现。那么有非轮询式访问的设备吗？据说只有串口和键盘了？

那么，无论网卡驱动是否支持中断处理，至少zCore里面是完全轮询的，这个答案已经找到了。

第四点：为何某些系统调用是异步的需要调研一下。在进行这一步之前首先需要当心的是：某些async fn的实现却可能是同步的！首先找出所有可能异步的syscall，在linux下有这些：read/connect/accept/recvfrom/recvmsg/wait4/futex/nanosleep/clock_nanosleep/semop/poll/select/vfork。看起来颇为怪异的一点是与read对应的write为什么不是异步的。

再稍微分一下类吧：

第一类：read。抛去socket不看，另一类是`FileLike` trait的`.read().await`。这部分有点复杂，先不想看了。

第二类：与网络相关的connect/accept/recvfrom/recvmsg，从已知的信息来看实际上是同步实现

第三类：与任务相关：wait4/vfork。比如wait4，可以追溯到linux-object的Process的wait_child和wait_child_any，最终调用`child.wait_signal(Signal::PROCESS_TERMINATED).await`，这个`wait_signal`则是来自于zircon-object，是一个比较典型的软件事件机制：

```rust
/// Asynchronous wait for one of `signal`.
pub fn wait_signal(self: &Arc<Self>, signal: Signal) -> impl Future<Output = Signal> {
    #[must_use = "wait_signal does nothing unless polled/`await`-ed"]
    struct SignalFuture {
        object: Arc<dyn KernelObject>,
        signal: Signal,
        first: bool,
    }

    impl Future for SignalFuture {
        type Output = Signal;

        fn poll(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {
            let current_signal = self.object.signal();
            if !(current_signal & self.signal).is_empty() {
                return Poll::Ready(current_signal);
            }
            if self.first {
                self.object.add_signal_callback(Box::new({
                    let signal = self.signal;
                    let waker = cx.waker().clone();
                    move |s| {
                        if (s & signal).is_empty() {
                            return false;
                        }
                        waker.wake_by_ref();
                        true
                    }
                }));
                self.first = false;
            }
            Poll::Pending
        }
    }

    SignalFuture {
        object: self.clone(),
        signal,
        first: true,
    }
}
```

vfork的await同样来源于wait_signal。

第四类：与时间相关：nanosleep，这个最终可以追溯到kernel-hal中的SleepFuture。

第五类：与同步相关：futex/semop，实现比较复杂，但同样是一些软件事件。

第六类：与I/O复用相关：poll/select，这两个原理差不多，应该都是调用FileLike的`async_poll`接口来检查该文件描述符的状态是否变更。这里又会有一些叶子Future，硬件/软件事件均有可能。

看完这一轮代码的话可以说对zCore的了解又上了一个层次。（雾

## 28/06/22

初步实现了一下，觉得可能最大的困难是：现在core 0和core 1是两套runtime了，那么各种异步信号机制还能正常工作吗？当core 1上的一个syscall被挂起之后，应该在何时、以怎样的方式来唤醒它。把syscall的执行移动到core 1上面会在这种意义上带来影响吗？

想了一下，应该不会有什么问题。但前提是core 1的runtime需要有一套自己的waker，对应到那些顶层syscall Future，在poll这些syscall Future的时候将这些waker传递下去，并在叶子Future返回pending的时候将这些waker注册到对应的reactor的位置上。这样，当一个事件触发之后，原来是唤醒core 0上的一个task，现在则是唤醒core 1上的一个syscall的执行。当core 1上的一个syscall执行完毕之后，进而才会去唤醒core 0上对应的task。

如果一个runtime需要支持从其他地方唤醒它里面的Future，就需要自己实现一套waker了。这个实现还是挺复杂的，可以参考preemptive scheduler。如果想先跑起来的话，我们可以尝试先支持实现为同步的那些syscall，也就是暂时填充一个空的waker，然后断言单次poll的结果为ready。这一套waker系统可以说每个async模块都要用到，也许可以将其独立出来。

目前简单实现了一个不支持Waker的版本，也就是能够在另一个核上跑立即返回的syscall了。

## 01/07/22

现在我们开始研究一下PreemptiveScheduler里面waker的相关实现。或者可以试试通读一下这个库的代码。

发现zCore主仓库已经多出了60多次commit，那么先更新一下吧。

关于kvm：原版zCore的话是否启用kvm看起来都能正常运行，启用kvm的话运行速度确实会比较快一点。我的修改版的话启用kvm之后连hart1上的输出都看不到就直接卡死了。大概情况是hart0进入到`wait_for_exit`函数，打印完`executor run!`之后就卡死了。也看不到hart1上的任何输出。这个行为十分令人费解。不启用kvm的话能看到正常的panic信息，因为目前还不支持poll实际上异步的系统调用。那我们姑且先在关闭kvm的情况下跑通试试。顺带，尝试了一下更新之前的代码（用的是`spin::Mutex`而非更新之后的`kernel-sync`库），启用kvm之后也是会卡死的，证明不是这次更新的问题。然后应该也跟hart1的切换页表行为无关，在那之前就已经卡死了。log一下，发现hart0已经将请求加入到队列中了，主要问题是hart1自始至终没动静。目前搞不清楚什么情况，明天继续调试吧。

## 04/07/22

现在来通读一下PreemptiveScheduler这个库的代码。在主分支上的版本是e8cd353，好的。

### ExecutorRuntime

暴露到最外层的应该是ExecutorRuntime，注意目前它默认只会在单核上访问（也就是每个核有一个，但不考虑同步问题）。这样的话似乎不满足我们的需求？因为我们需要通过在hart1上调用`waker.wake_by_ref`来唤醒hart0上的task。我们且看一看后面会不会有什么影响。每个ExecutorRuntime包含一个当前的cpu_id，一个TaskCollection（没有使用互斥锁保护），一个strong_executor和一组weak_executors（类型型如`Arc<Pin<Box<Executor>>>`，其中Executor为我们比较熟知的执行器），一个current_executor，还有一个Context。这里的Context包括了ra，sp，被调用者保存寄存器还有页表基址。

`ExecutorRuntime::new`，初始化了任务管理器TaskCollection，同时一开始有一个strong_executor而没有任何weak_executors，将任务管理器的指针拷贝一份交给strong。`downgrade_strong_executor`就是将当前的strong_executor内部状态ExecutorState标记为WEAK，将其移动到weak_executors中，最后新建一个strong_executor。`add/remove_task`均是调用TaskCollection的方法。默认为ExecutorRuntime实现了Send和Sync两个trait。然而，GLOBAL_RUNTIME里面还是给每个ExecutorRuntime加上了`spin::Mutex`作为同步保护。

然后是对外暴露出的几个函数：`handle_timeout`，`run_until_idle`，`sched_yield`以及`spawn`，我们依次来看一下。目前`handle_timeout`实现并不完整，只是在关中断的前提下调用`sched_yield`。而`sched_yield`的话，首先根据cpu_id获取当前核上的ExecutorRuntime（也就是`get_current_runtime`），然后如果它的current_executor不是None的话，就从这个executor的Context切换到当前runtime的Context。这个应该和传统意义上的内核线程切换是一个意思。也就是说，当某个Executor正在运行的时候，通过调用`sched_yield`可以将CPU控制权交还给它所在的Runtime。然而我们知道每个Executor应该负责一组任务（一个TaskCollection），这意味着组内的其他任务应该暂时也没有机会被执行了。然后是Runtime的主循环`run_until_idle`，每次迭代我们先运行strong_executor。控制权交还给Runtime之后说明strong_executor主动yield或者其中的一个task超时了。对于主动yield的情况，调用`downgrade_strong_executor`将其移动到weak_executors并新建一个strong_executor。注意，目前看起来所有的Executor共享Runtime里面的TaskCollection，这样合理吗？接着去除掉一些被killed掉的weak_executors之后，按照顺序运行这些weak_executors，最后进入下一次迭代。`spawn`的话实际调用的是`spawn_task`函数，`spawn_task`可能支持不同的优先级和不同的核，不过目前`spawn`不考虑优先级，也只将任务创建在当前的核上。这里我觉得倒是没什么好说的。

### TaskCollection && WakerPage

接下来我们看一下我们比较关心的TaskCollection和WakerPage的实现吧。重点关注Waker是如何唤醒一个特定的任务的。

TaskCollection的单位自然是Task，其中包含一个id，一个`Mutex<Pin<Box<>>>`的Future本体，一个`Mutex<TaskInner>`，还有一个标记任务是否结束的AtomicBool。TaskInner则包括优先级、状态`TaskState`（可能为RUNNABLE和RUNNING两种，BLOCKED被注释掉了）、还有一个intr_enable。Task的id看起来是通过`alloc_id`函数进行全局分配的，任意两个Task的id应该都不同。通过`Task::new`进行初始化，可以看到状态为RUNNABLE，finish和intr_enable均为false。这个Task也实现了poll方法，也就是poll该Task里面的Future。不过有一些细节：在poll之前需要获取Future的锁并在poll期间持有；根据intr_enable还有开关中断等操作。其实intr_enable有些意义不明，这里我们先放着不管。

接着是一个叫做FutureCollection的结构，包括一个`Arc<Task>`的slab allocator，也就是`PinSlab<Arc<Task>>`；一个`pages: Vec<Arc<WakerPage>>`，也附带一个优先级。然后大概是使用Key（一种usize的封装）来进行某种索引，每个Key对应到一个Future，由insert方法返回给调用者（实际上这个Key是PinSlab库提供的，这里我们只是对其进行一些索引操作）。每个Key是由subpage_idx（6位），page_idx和priority组装而成。其中page_idx应该是一级索引，可以索引到一个`Arc<WakerPage>`；然后在这个WakerPage里面再使用二级索引subpage_idx找到一个Future。也就是说每个WakerPage应该保存了64个Future的Waker信息。这样我们可以insert或者remove一个Future了。

然后就是TaskCollection了。它的结构：

```rust
pub struct TaskCollection {
    cpu_id: u8, // Just for debug, not used
    future_collections: Vec<Mutex<FutureCollection>>,
    pub task_num: AtomicUsize,
    generator: Option<Mutex<Pin<Box<dyn Generator<Yield = Option<Key>, Return = ()>>>>>,
}
```

注意future_collections是一组FutureCollection，对于每个不同的priority都有一个，目前有MAX_PRIORITY=32种不同的优先级。

使用`new`进行初始化，主要是对于每种priority插入一个FutureCollection，值得注意的是generator的初始化。可以看到generator是每个yield一个Key出来。这个generator整体上锁，实现可以在`TaskCollection::generator`找到。这个函数比较长，稍微读一下：变量`priority`表示当前的优先级，根据它可以得到对应的FutureCollection，然后遍历其中所有的WakerPage...

这里插入一下WakerPage的内部实现，可以看到有三个64位的位图：notified，dropped还有borrowed，一开始它们均被初始化为0。它们都是用内存顺序为顺序一致性的原子变量实现的，也就是说多核上应该也没什么问题。这里notified这个位图应该说比较关键，它某个位为1的话表示对应的Future准备好再次被polled了。可以`initialize`某个idx，其效果是notified对应位置1，dropped和borrowed对应位置0。`mark_dropped`和`notify`方法分别将dropped和notified对应位置1，而`mark_borrowed`可以将borrowed对应位置为指定值。`take_notified`的作用是返回notified的值并将notified清零，这里返回值需要经过dropped和borrowed的筛选，如果dropped或者borrowed对应位为1的话，返回值的对应位为0。`take_dropped`是返回dropped的当前值并将dropped清零。`clear`则是将三个位图的某一位均清零。最后是`make_waker`，这是对于给定的一个Future的idx和`dropped: &Arc<AtomicBool>`生成一个WakerRef。

WakerRef的结构如下：

```rust
pub struct WakerRef {
    page: Arc<WakerPage>,
    idx: usize,
    dropped: Arc<AtomicBool>,
}
```

至于方法，`mark_borrowed`是调用`WakerPage::mark_borrowed`对于自身的idx对应的位进行修改。`wake_by_ref`和`drop_by_ref`则是分别调用`WakerPage::notify/mark_dropped`也是对WakerPage进行更新。不过事先需要检查WakerRef里面的dropped，均需要dropped为false，`drop_by_ref`之后会将这个dropped改为true。这样的话，一个WakerPage维护的64个Future应该每个都有自己的WakerRef，可以通过它对WakerPage进行更新。后面我们为WakerRef实现了Woke trait，Woke trait来自于一个同名的crate，感觉还是比较好用的：

```rust
pub trait Woke: Send + Sync {
    fn wake(self: Arc<Self>) {
        Self::wake_by_ref(&self)
    }

    fn wake_by_ref(arc_self: &Arc<Self>);
}
```

我们只需要实现`wake_by_ref`这个方法。然后，所有实现了Woke trait的类型会得到额外的好处：该crate会帮助我们生成对应的RawWakerVTable，并进而帮助我们生成Waker和WakerRef，其中WakerRef也是这个crate定义的：

```rust
pub fn waker<W>(wake: Arc<W>) -> Waker
where
    W: Woke;

pub fn waker_ref<W>(wake: &Arc<W>) -> WakerRef<'_>
where
    W: Woke;
```

这样就知道唤醒的大概流程了：WakerRef生成的Waker可以更新WakerPage，而后WakerPage会在`TaskCollection::generator`中轮询（现在我们跳回到原来轮询WakerPage的地方），每一轮最多只会返回一个待唤醒的Future的Key。generator中如果没有要唤醒的Future，则会开启drop流程，不过这并非我们关注的重点。现在的问题是：这个generator是被谁调用的？我们看到`TaskCollection::take_task`调用generator来取出一个任务（这里回想一下，notified其实就是一个就绪队列），而这个`take_task`则应该是被Executor调用的。

这样的话，TaskCollection和WakerPage的实现就看完了。一开始我认为WakerPage是对应到一个物理页面，但实际上并非如此。

### Executor

首先是Executor的结构：

```rust
pub struct Executor {
    id: usize,
    task_collection: Arc<TaskCollection>,
    stack_base: usize,
    pub context: ExecuterContext,
    #[cfg(any(target_arch = "riscv64", target_arch = "aarch64"))]
    context_data: ContextData,
    task_id: usize,
    state: ExecutorState,
}
```

其中id是和Task的id以差不多的方式分配的。在`new`的时候会给每个Executor分配一个栈，并调用`init_stack_and_context`进行栈的初始化。Executor的主循环是`run`方法。每次迭代从TaskCollection中调用`take_task`方法取出一个任务，目前的实现是如果当前Runtime没有的话则从其他核的Runtime上通过`steal_task_from_other_cpu`方法偷任务（这样的话岂不是Runtime已经跨核了，从目前的实现来看似乎并没有问题）。拿到任务之后也能拿到对应的WakerRef（通过Arc放到堆上），进而调用`woke::waker_ref`生成woke定义的WakerRef，然后拿到Rust标准的Context。在poll取出的Future的时候用的就是这个Context，具体来说，到了叶子Future被阻塞的时候注册到Reactor的也就是这些Context。注意一下borrowed的相关细节，在poll之前需要将borrowed标记为true，之后则将borrowed标记为false，当我们想自己实现一个靠谱的Runtime的时候这可能是不可忽略的细节。如果poll的结果是Ready的话则调用`WakerRef::drop_by_ref`在TaskCollection中回收掉对应的Future；如果是Pending的话则什么都不做，因为该Future在被唤醒之前不会再被polled到了。

然后ExecutorState有这么几种：STRONG/WEAK/KILLED/UNUSED。注释提到WEAK状态只允许poll一个Future，然后就需要转化为KILLED并被dropped掉。在`run`中也能找到对应的逻辑。如果当前Executor没有任务的话，则大多数情况调用`sched_yield`将控制权交还给Runtime。

### 我们自己的Runtime设计

感觉有点复杂...先去抽空了解下zCore目前的网络/存储栈的情况，过一会就要用到了。

