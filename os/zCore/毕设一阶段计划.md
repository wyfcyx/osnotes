## 21/06/22

先开个头，过会完善。

思路就是先分出一个核S负责执行所有的系统调用。其他核一旦通过系统调用陷入到内核需要将系统调用请求提交给核S，等到核S处理完毕之后再唤醒来源核上对应的task。这就是所谓第一阶段的计划。

将目前的实现过程记录在这里。

首先，要完整运行的话，只需在zCore根目录下`make rootfs && make libc-test && make other-test && make image`，随后在zCore/zCore目录下`make run LINUX=1 SMP=2 LOG=warn MODE=release ACCEL=1`即可。

根据此前的分析，我们希望将`loader/linux.rs`中`handle_user_trap`函数中的`syscall.syscall.await`执行过程移动到hart 1上面。然后其余的地方保持不变。应当如何实现呢？首先应该新创建一种Future（姑且称为`RemoteSyscallFuture`），这种Future会将`syscall.syscall`包裹起来，当poll该Future的时候，将以某种方式将syscall请求提交到远程syscall模块，随后返回`Poll::Pending`，这样才能在hart 0的运行时上面暂停该任务。那么如何将其唤醒呢？远程syscall模块自己的运行时将以poll的方式执行syscall，在执行完毕之后应该以某种方式唤醒hart 0上的任务。

那么，我们可能需要研究一下现有的Waker机制。看了一下已有的叶子Future的实现，其中一个比较典型的例子是`YieldFuture`，是在我们调用`yield_now`的时候用到的，而`yield_now`用于在裸机模式下实现基于时间片的抢占式调度。`YieldFuture`的实现是内部保存一个flag，首次poll的时候flag为false，此时将flag设置为true，调用`cx.waker().wake_by_ref()`，然后返回`Poll::Pending`；第二次poll的时候发现内部flag为true则直接返回`Poll::Ready`。如果想知道这里面的Waker做了什么事情，就需要找到顶层Future管理器以及对应生成的Waker本身。我能想到的一个比较高层的Future是`run_user`生成的，不过这个大概还不是顶层Future。这个Future还会被`ThreadSwitchFuture`包裹起来（详见zircon_object里面的thread.rs）并提交给kernel_hal。于是这个`ThreadSwitchFuture`又是什么呢？它只是会poll里层Future，但是在poll之前先切换到对应进程的地址空间。那么我们发现在这里是找不到对应的Waker的。结论：`ThreadSwitchFuture`应该就是顶层Future。

那么看起来Waker的实现还是要在executor库如PreemptiveScheduler中寻找了。看了一下实现比较复杂，但是总体上来说，在每个task被spawned到运行时中的时候我们会为其生成一个对应的Waker用来唤醒它，内部应该是使用到了某种索引。同时，事件循环的逻辑是，每次会取出（也即take）一个task，然后尝试poll这个task。如果结果是Ready的话则销毁对应的Waker，结果是Pending的话则什么都不做，因为task已经被移除了。应该是当我们调用`cx.waker().wake_by_ref()`的时候task才会被重新加入到运行时中。重新思考一下`YieldFuture`，第一次poll的时候，注意在poll之前task已经被移除了，所以这个时候要重新将其加入到运行时中，相当于从队头移动到队尾，是经典的Round Robin操作。这样就有了第二次被poll的机会，这次的话直接返回Ready即可让task继续向下运行。

总结一下，如果使用的是rcore-os/executor或者PreemptiveScheduler这些运行时的话，需要满足的要求是在顶层Future被poll之前它们会被移除。同时需要在适当的时机调用Waker将顶层Future重新放置到就绪队列中。不过，目前我们希望自己重新实现一套运行时，就不必遵循这里的规则了。但是这可以作为参考。这里可以提炼出一条重要的规则：Waker应该是由运行时负责生成和管理的，这也跟之前async_modules中的经验一致了。

还有一些问题。

第一个问题：我们是否要为其实现自己的调度器，还是沿用已有的PreemptiveScheduler？

> 目前来看，为这种模块实现一个模块管理器并在里面自己实现一个event loop，也就是可以参考之前的async_modules看上去是比较好的。这样就不用去计较PreemptiveScheduler的种种细节了，同时也更加灵活。

第二个问题：SubmissionQueue和CompletionQueue是否还需要？

> 如果照搬之前的async_modules的话，自然还是需要的。

然后就是考虑系统调用本身的实现是同步还是异步的。同步的自然比较简单。对于异步的，假设其与I/O有关，且我们不打开hart 1的中断使能，那么也就有这么一种复杂的情况：在hart 0接收到外设中断之后，需要以某种方式通知hart 1该外设上的数据已经准备好了，从而唤醒hart 1运行时上的syscall任务。如果实现不好的话，这一步可能涉及到大量的数据拷贝。另外，实际情况是否真的有这么复杂，还需要一些代码阅读（比如，可能是一些软件上的异步事件）。

现在总体上考虑一下第一阶段的设计吧。

`RemoteSyscallFuture`是对`syscall.syscall`的封装，确切来说，是`syscall.syscall(num as u32, args)`这个Future的一个wrapper。注意，这个Future的返回值是一个isize。这个Future会比较奇怪，因为它是hart 0和hart 1上两套运行时之间沟通的桥梁。首先，它是hart 0运行时体系中的一个叶子Future。当我们在poll这个Future的时候，`cx.waker().wake_by_ref()`可以用来唤醒hart 0中的顶层任务。唤醒的时候自然是hart 1上的一个系统调用已经执行完毕了，hart1应该用怎样的方式通知hart 0？所以一种不错的思路可能是将`cx.waker().wake_by_ref()`包在一个闭包里面由hart 1直接去调用。当然`RemoteSyscallFuture`内部的记录也需要在某个时候更新，因为它还需要正确的返回一个isize。

因此，当第一次poll `RemoteSyscallFuture`的时候，我们需要将`syscall.syscall`这个Future、唤醒当前task的`cx.waker().wake_by_ref()`的闭包（以直接拷贝或引用的形式，比如说不是Future本体而是所有用来生成该Future所需的信息）、还有一个指向`RemoteSyscallFuture`内部的`*mut isize`（这个是用来保存syscall返回值）提交到hart 1，然后就可以返回`Poll::Pending`。这将使当前task在远程syscall返回之前不会被放回到hart 0运行时中。当hart 1完成了远程syscall之后，将返回值写回到`RemoteSyscallFuture`，然后调用传进来的闭包唤醒hart 0上的task。过了一段时间之后，hart 0上的`RemoteSyscallFuture`将会第二次被poll到，此时我们直接返回`Poll::Ready`，带上`RemoteSyscallFuture`内部的syscall返回值即可。

这样来看的话，我们的AsyncModule固然需要Submission Queue提供的filter/reordering等能力，Completion Queue可能是真的不需要了。因为如果一个模块要调用其他模块的话，它可以向其他模块在提交任务的时候带上一个闭包来允许对方唤醒自己的某个任务，这样的实现可能会比较方便？另一方面，从局部性上来说也不一定会劣于Completion Queue的实现。

如果只限定于`RemoteSyscallFuture`，AsyncModule的实现？由于没有调用到其他的模块，至少是不需要考虑Waker的。然后显然需要有一个无锁的Submission Queue方便hart 0来提交任务。自身的事件循环的话，就是从Submission Queue取出hart 0的请求来填充自身的任务队列，然后poll已有（在hart 0上生成的）或是在hart 1上原地生成的Future。这里最大的问题是Reactor的实际应用，也即哪些事件能让中间暂停的syscall继续执行，这就需要调研已有的异步syscall之所以为异步的原因。某些事件若想完成跨核通信的话，也许并不容易。

如何使用宏来降低实现复杂性？

同时将一些可能的困难记录一下。

第一点：`Syscall`里面有一个`CurrentThread`的引用，回顾一下知道`CurrentThread`只不过是`Arc<Thread>`的一层封装。也就是说这可能会为我们将其移动到另外一个核上去执行带来一些本不必要的困难。我们可以考虑直接将`Arc<Thread>`移动到另一个核上，然后再顺势生成`Syscall`实例来有效利用已有的API。那么zCore里面又为何是这样设计的呢？`CurrentThread`自身是被传入`run_user`闭包中的，然后`run_user`里面就一直以引用的形式访问它。不对它进行clone，是出于性能考虑，还是必须保持引用计数不变？目前还不清楚。不过，我觉得如果将`CurrentThread`复制一份传到另一个核上应当没有什么问题。

第二点：zCore并没有采用KPTI的设计。因此，在hart 1上，每次在执行一个syscall之前都需要先切换到线程所在进程的地址空间。在hart 0上这是由顶层Future `ThreadSwitchFuture`保证的，在hart 1上就没有了。所以我们后续需要将zCore改成KPTI的设计吗？从性能和实现复杂性两方面考虑一下？

第三点：网卡驱动和相关的测例（还有其他测例）需要调研一下。目前如果只考虑Linux裸机上的测例的话，注意`zCore/tests`目录，大概可以分成一大堆libc测试（进而分成functional/math/musl/regression几类，其中regression类别中包括pthread/printf相关测例，这些测例的源码可以在`zCore/libc-test`目录下找到）、与busybox有关的测试，还有就是`tests/src`目录下的socket（包括tcp/udp）和串口相关的测试。但是`tests/src`这里的测试还没有跑起来，具体的环境配置方式还需要再了解一下。最终生成的rootfs可以在`zCore/rootfs/x86_64`目录下找到，`x86_64/bin`里面除了busybox还有一些test开头的测例，比如`testpipe1`等等。这些测例的源码可以在`linux-syscall/test`目录下找到。于是测例的位置我们大致上找到了。

接下来是研究一下网卡驱动是否真的是异步的。zCore这里对于设备的抽象相当复杂，可以说是跟rCore一脉相承了。想当初最早接触rCore的时候也是这里最让我头大。我们从系统调用入手吧，可以看到sys_socket是同步的，而sys_connect和sys_accept都是异步的。例如sys_connect，最终是调用了`Socket` Trait的`connect().await`。这个`Socket` Trait可以在linux-object中找到，看起来目前应该是为`TcpSocketState`和`UdpSocketState`实现了这个Trait。比如`TcpSocketState`...等等，这里看起来好像是个悲伤的故事，`TcpSocketState`的connect/accept被声明为async fn，但实际上都是基于轮询的同步实现。那么有非轮询式访问的设备吗？据说只有串口和键盘了？

那么，无论网卡驱动是否支持中断处理，至少zCore里面是完全轮询的，这个答案已经找到了。

第四点：为何某些系统调用是异步的需要调研一下。在进行这一步之前首先需要当心的是：某些async fn的实现却可能是同步的！首先找出所有可能异步的syscall，在linux下有这些：read/connect/accept/recvfrom/recvmsg/wait4/futex/nanosleep/clock_nanosleep/semop/poll/select/vfork。看起来颇为怪异的一点是与read对应的write为什么不是异步的。

再稍微分一下类吧：

第一类：read。抛去socket不看，另一类是`FileLike` trait的`.read().await`。这部分有点复杂，先不想看了。

第二类：与网络相关的connect/accept/recvfrom/recvmsg，从已知的信息来看实际上是同步实现

第三类：与任务相关：wait4/vfork。比如wait4，可以追溯到linux-object的Process的wait_child和wait_child_any，最终调用`child.wait_signal(Signal::PROCESS_TERMINATED).await`，这个`wait_signal`则是来自于zircon-object，是一个比较典型的软件事件机制：

```rust
/// Asynchronous wait for one of `signal`.
pub fn wait_signal(self: &Arc<Self>, signal: Signal) -> impl Future<Output = Signal> {
    #[must_use = "wait_signal does nothing unless polled/`await`-ed"]
    struct SignalFuture {
        object: Arc<dyn KernelObject>,
        signal: Signal,
        first: bool,
    }

    impl Future for SignalFuture {
        type Output = Signal;

        fn poll(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {
            let current_signal = self.object.signal();
            if !(current_signal & self.signal).is_empty() {
                return Poll::Ready(current_signal);
            }
            if self.first {
                self.object.add_signal_callback(Box::new({
                    let signal = self.signal;
                    let waker = cx.waker().clone();
                    move |s| {
                        if (s & signal).is_empty() {
                            return false;
                        }
                        waker.wake_by_ref();
                        true
                    }
                }));
                self.first = false;
            }
            Poll::Pending
        }
    }

    SignalFuture {
        object: self.clone(),
        signal,
        first: true,
    }
}
```

vfork的await同样来源于wait_signal。

第四类：与时间相关：nanosleep，这个最终可以追溯到kernel-hal中的SleepFuture。

第五类：与同步相关：futex/semop，实现比较复杂，但同样是一些软件事件。

第六类：与I/O复用相关：poll/select，这两个原理差不多，应该都是调用FileLike的`async_poll`接口来检查该文件描述符的状态是否变更。这里又会有一些叶子Future，硬件/软件事件均有可能。

看完这一轮代码的话可以说对zCore的了解又上了一个层次。（雾

## 28/06/22

初步实现了一下，觉得可能最大的困难是：现在core 0和core 1是两套runtime了，那么各种异步信号机制还能正常工作吗？当core 1上的一个syscall被挂起之后，应该在何时、以怎样的方式来唤醒它。把syscall的执行移动到core 1上面会在这种意义上带来影响吗？

想了一下，应该不会有什么问题。但前提是core 1的runtime需要有一套自己的waker，对应到那些顶层syscall Future，在poll这些syscall Future的时候将这些waker传递下去，并在叶子Future返回pending的时候将这些waker注册到对应的reactor的位置上。这样，当一个事件触发之后，原来是唤醒core 0上的一个task，现在则是唤醒core 1上的一个syscall的执行。当core 1上的一个syscall执行完毕之后，进而才会去唤醒core 0上对应的task。

如果一个runtime需要支持从其他地方唤醒它里面的Future，就需要自己实现一套waker了。这个实现还是挺复杂的，可以参考preemptive scheduler。如果想先跑起来的话，我们可以尝试先支持实现为同步的那些syscall，也就是暂时填充一个空的waker，然后断言单次poll的结果为ready。这一套waker系统可以说每个async模块都要用到，也许可以将其独立出来。

目前简单实现了一个不支持Waker的版本，也就是能够在另一个核上跑立即返回的syscall了。

## 01/07/22

现在我们开始研究一下PreemptiveScheduler里面waker的相关实现。或者可以试试通读一下这个库的代码。

发现zCore主仓库已经多出了60多次commit，那么先更新一下吧。

关于kvm：原版zCore的话是否启用kvm看起来都能正常运行，启用kvm的话运行速度确实会比较快一点。我的修改版的话启用kvm之后连hart1上的输出都看不到就直接卡死了。大概情况是hart0进入到`wait_for_exit`函数，打印完`executor run!`之后就卡死了。也看不到hart1上的任何输出。这个行为十分令人费解。不启用kvm的话能看到正常的panic信息，因为目前还不支持poll实际上异步的系统调用。那我们姑且先在关闭kvm的情况下跑通试试。顺带，尝试了一下更新之前的代码（用的是`spin::Mutex`而非更新之后的`kernel-sync`库），启用kvm之后也是会卡死的，证明不是这次更新的问题。然后应该也跟hart1的切换页表行为无关，在那之前就已经卡死了。log一下，发现hart0已经将请求加入到队列中了，主要问题是hart1自始至终没动静。目前搞不清楚什么情况，明天继续调试吧。

## 04/07/22

现在来通读一下PreemptiveScheduler这个库的代码。在主分支上的版本是e8cd353，好的。

### ExecutorRuntime

暴露到最外层的应该是ExecutorRuntime，注意目前它默认只会在单核上访问（也就是每个核有一个，但不考虑同步问题）。这样的话似乎不满足我们的需求？因为我们需要通过在hart1上调用`waker.wake_by_ref`来唤醒hart0上的task。我们且看一看后面会不会有什么影响。每个ExecutorRuntime包含一个当前的cpu_id，一个TaskCollection（没有使用互斥锁保护），一个strong_executor和一组weak_executors（类型型如`Arc<Pin<Box<Executor>>>`，其中Executor为我们比较熟知的执行器），一个current_executor，还有一个Context。这里的Context包括了ra，sp，被调用者保存寄存器还有页表基址。

`ExecutorRuntime::new`，初始化了任务管理器TaskCollection，同时一开始有一个strong_executor而没有任何weak_executors，将任务管理器的指针拷贝一份交给strong。`downgrade_strong_executor`就是将当前的strong_executor内部状态ExecutorState标记为WEAK，将其移动到weak_executors中，最后新建一个strong_executor。`add/remove_task`均是调用TaskCollection的方法。默认为ExecutorRuntime实现了Send和Sync两个trait。然而，GLOBAL_RUNTIME里面还是给每个ExecutorRuntime加上了`spin::Mutex`作为同步保护。

然后是对外暴露出的几个函数：`handle_timeout`，`run_until_idle`，`sched_yield`以及`spawn`，我们依次来看一下。目前`handle_timeout`实现并不完整，只是在关中断的前提下调用`sched_yield`。而`sched_yield`的话，首先根据cpu_id获取当前核上的ExecutorRuntime（也就是`get_current_runtime`），然后如果它的current_executor不是None的话，就从这个executor的Context切换到当前runtime的Context。这个应该和传统意义上的内核线程切换是一个意思。也就是说，当某个Executor正在运行的时候，通过调用`sched_yield`可以将CPU控制权交还给它所在的Runtime。然而我们知道每个Executor应该负责一组任务（一个TaskCollection），这意味着组内的其他任务应该暂时也没有机会被执行了。然后是Runtime的主循环`run_until_idle`，每次迭代我们先运行strong_executor。控制权交还给Runtime之后说明strong_executor主动yield或者其中的一个task超时了。对于主动yield的情况，调用`downgrade_strong_executor`将其移动到weak_executors并新建一个strong_executor。注意，目前看起来所有的Executor共享Runtime里面的TaskCollection，这样合理吗？接着去除掉一些被killed掉的weak_executors之后，按照顺序运行这些weak_executors，最后进入下一次迭代。`spawn`的话实际调用的是`spawn_task`函数，`spawn_task`可能支持不同的优先级和不同的核，不过目前`spawn`不考虑优先级，也只将任务创建在当前的核上。这里我觉得倒是没什么好说的。

### TaskCollection && WakerPage

接下来我们看一下我们比较关心的TaskCollection和WakerPage的实现吧。重点关注Waker是如何唤醒一个特定的任务的。

TaskCollection的单位自然是Task，其中包含一个id，一个`Mutex<Pin<Box<>>>`的Future本体，一个`Mutex<TaskInner>`，还有一个标记任务是否结束的AtomicBool。TaskInner则包括优先级、状态`TaskState`（可能为RUNNABLE和RUNNING两种，BLOCKED被注释掉了）、还有一个intr_enable。Task的id看起来是通过`alloc_id`函数进行全局分配的，任意两个Task的id应该都不同。通过`Task::new`进行初始化，可以看到状态为RUNNABLE，finish和intr_enable均为false。这个Task也实现了poll方法，也就是poll该Task里面的Future。不过有一些细节：在poll之前需要获取Future的锁并在poll期间持有；根据intr_enable还有开关中断等操作。其实intr_enable有些意义不明，这里我们先放着不管。

接着是一个叫做FutureCollection的结构，包括一个`Arc<Task>`的slab allocator，也就是`PinSlab<Arc<Task>>`；一个`pages: Vec<Arc<WakerPage>>`，也附带一个优先级。然后大概是使用Key（一种usize的封装）来进行某种索引，每个Key对应到一个Future，由insert方法返回给调用者（实际上这个Key是PinSlab库提供的，这里我们只是对其进行一些索引操作）。每个Key是由subpage_idx（6位），page_idx和priority组装而成。其中page_idx应该是一级索引，可以索引到一个`Arc<WakerPage>`；然后在这个WakerPage里面再使用二级索引subpage_idx找到一个Future。也就是说每个WakerPage应该保存了64个Future的Waker信息。这样我们可以insert或者remove一个Future了。

然后就是TaskCollection了。它的结构：

```rust
pub struct TaskCollection {
    cpu_id: u8, // Just for debug, not used
    future_collections: Vec<Mutex<FutureCollection>>,
    pub task_num: AtomicUsize,
    generator: Option<Mutex<Pin<Box<dyn Generator<Yield = Option<Key>, Return = ()>>>>>,
}
```

注意future_collections是一组FutureCollection，对于每个不同的priority都有一个，目前有MAX_PRIORITY=32种不同的优先级。

使用`new`进行初始化，主要是对于每种priority插入一个FutureCollection，值得注意的是generator的初始化。可以看到generator是每个yield一个Key出来。这个generator整体上锁，实现可以在`TaskCollection::generator`找到。这个函数比较长，稍微读一下：变量`priority`表示当前的优先级，根据它可以得到对应的FutureCollection，然后遍历其中所有的WakerPage...

这里插入一下WakerPage的内部实现，可以看到有三个64位的位图：notified，dropped还有borrowed，一开始它们均被初始化为0。它们都是用内存顺序为顺序一致性的原子变量实现的，也就是说多核上应该也没什么问题。这里notified这个位图应该说比较关键，它某个位为1的话表示对应的Future准备好再次被polled了。可以`initialize`某个idx，其效果是notified对应位置1，dropped和borrowed对应位置0。`mark_dropped`和`notify`方法分别将dropped和notified对应位置1，而`mark_borrowed`可以将borrowed对应位置为指定值。`take_notified`的作用是返回notified的值并将notified清零，这里返回值需要经过dropped和borrowed的筛选，如果dropped或者borrowed对应位为1的话，返回值的对应位为0。`take_dropped`是返回dropped的当前值并将dropped清零。`clear`则是将三个位图的某一位均清零。最后是`make_waker`，这是对于给定的一个Future的idx和`dropped: &Arc<AtomicBool>`生成一个WakerRef。

WakerRef的结构如下：

```rust
pub struct WakerRef {
    page: Arc<WakerPage>,
    idx: usize,
    dropped: Arc<AtomicBool>,
}
```

至于方法，`mark_borrowed`是调用`WakerPage::mark_borrowed`对于自身的idx对应的位进行修改。`wake_by_ref`和`drop_by_ref`则是分别调用`WakerPage::notify/mark_dropped`也是对WakerPage进行更新。不过事先需要检查WakerRef里面的dropped，均需要dropped为false，`drop_by_ref`之后会将这个dropped改为true。这样的话，一个WakerPage维护的64个Future应该每个都有自己的WakerRef，可以通过它对WakerPage进行更新。后面我们为WakerRef实现了Woke trait，Woke trait来自于一个同名的crate，感觉还是比较好用的：

```rust
pub trait Woke: Send + Sync {
    fn wake(self: Arc<Self>) {
        Self::wake_by_ref(&self)
    }

    fn wake_by_ref(arc_self: &Arc<Self>);
}
```

我们只需要实现`wake_by_ref`这个方法。然后，所有实现了Woke trait的类型会得到额外的好处：该crate会帮助我们生成对应的RawWakerVTable，并进而帮助我们生成Waker和WakerRef，其中WakerRef也是这个crate定义的：

```rust
pub fn waker<W>(wake: Arc<W>) -> Waker
where
    W: Woke;

pub fn waker_ref<W>(wake: &Arc<W>) -> WakerRef<'_>
where
    W: Woke;
```

这样就知道唤醒的大概流程了：WakerRef生成的Waker可以更新WakerPage，而后WakerPage会在`TaskCollection::generator`中轮询（现在我们跳回到原来轮询WakerPage的地方），每一轮最多只会返回一个待唤醒的Future的Key。generator中如果没有要唤醒的Future，则会开启drop流程，不过这并非我们关注的重点。现在的问题是：这个generator是被谁调用的？我们看到`TaskCollection::take_task`调用generator来取出一个任务（这里回想一下，notified其实就是一个就绪队列），而这个`take_task`则应该是被Executor调用的。

这样的话，TaskCollection和WakerPage的实现就看完了。一开始我认为WakerPage是对应到一个物理页面，但实际上并非如此。

### Executor

首先是Executor的结构：

```rust
pub struct Executor {
    id: usize,
    task_collection: Arc<TaskCollection>,
    stack_base: usize,
    pub context: ExecuterContext,
    #[cfg(any(target_arch = "riscv64", target_arch = "aarch64"))]
    context_data: ContextData,
    task_id: usize,
    state: ExecutorState,
}
```

其中id是和Task的id以差不多的方式分配的。在`new`的时候会给每个Executor分配一个栈，并调用`init_stack_and_context`进行栈的初始化。Executor的主循环是`run`方法。每次迭代从TaskCollection中调用`take_task`方法取出一个任务，目前的实现是如果当前Runtime没有的话则从其他核的Runtime上通过`steal_task_from_other_cpu`方法偷任务（这样的话岂不是Runtime已经跨核了，从目前的实现来看似乎并没有问题）。拿到任务之后也能拿到对应的WakerRef（通过Arc放到堆上），进而调用`woke::waker_ref`生成woke定义的WakerRef，然后拿到Rust标准的Context。在poll取出的Future的时候用的就是这个Context，具体来说，到了叶子Future被阻塞的时候注册到Reactor的也就是这些Context。注意一下borrowed的相关细节，在poll之前需要将borrowed标记为true，之后则将borrowed标记为false，当我们想自己实现一个靠谱的Runtime的时候这可能是不可忽略的细节。如果poll的结果是Ready的话则调用`WakerRef::drop_by_ref`在TaskCollection中回收掉对应的Future；如果是Pending的话则什么都不做，因为该Future在被唤醒之前不会再被polled到了。

然后ExecutorState有这么几种：STRONG/WEAK/KILLED/UNUSED。注释提到WEAK状态只允许poll一个Future，然后就需要转化为KILLED并被dropped掉。在`run`中也能找到对应的逻辑。如果当前Executor没有任务的话，则大多数情况调用`sched_yield`将控制权交还给Runtime。

### 我们自己简化版的Runtime设计

感觉有点复杂...先去抽空了解下zCore目前的网络/存储栈的情况，过一会就要用到了。

我觉得比较简单的设计的话，就类似于一个之前的Completion Queue的实现就好。这样确实没有那么高效但胜在简单方便QAQ 

我们的异步模块应该包括Ready/Pending两个Future的队列。然后针对一个取出来的task，在poll之前需要生成一个包含它独有信息的waker，即task的id以及指向该模块调度数据结构的指针。

实现出来简单跑了一下，不开kvm的话看起来是能正常运行的，但是比正常版本要慢很多，感觉是过多的数据拷贝或者是同步开销？开kvm的话貌似是卡在更早的地方。

## 05/07/22

导师的意见是在kvm上跑起来比较关键，于是就先弄这个bug。在qemu中打开[multithread tcg](https://wiki.qemu.org/Features/tcg-multithread)可能是kvm之前一个不错的过渡，可以先尝试一下。具体的做法是将QEMU的加速配置改成`-accel tcg,thread=multi`。改了之后似乎我们的版本能正常运行了，但是依然很慢。特别是尝试`ls libc-test/src/math`这种大目录的时候。大概可以打开`LOG=info`看一下什么操作最为耗时。感觉上非常耗时的大概就是`fstatat`这种syscall。

> 这里稍微提一下，能否把page fault的处理在核间迁移？

在x86_64平台上，QEMU模拟的CPU可以配置的feature很多（无论是否打开kvm），而且我们找不到这些配置的相关说明，所以说局面有点陷入僵局。

后面尝试用GDB调试了一下，tcg的行为看起来很正常，kvm则非常奇怪。这里有一个关闭ASLR的[相关回答](https://askubuntu.com/questions/964540/gdb-qemu-cant-put-break-point-on-kernel-function-kernel-4-10-0-35)，先mark一下。

从目前的情况来看，使用GDB调试的帮助似乎不大。

顺带一提，加上配置`DISK=on`应该就可以把块设备从membuf换成virtio blk，不过可能还需要改代码。

## 06/07/22

今天和向老师交流了一下，总结一下纪要：

1. 使用远程的SiFive u740而非x86_64 QEMU+kvm，优势是我对于RV还是更加熟悉，而且u740上有天然的大小核
2. 达成了项目总体设计的共识：即将内核划分成多个互相之前使用RPC进行通信的异步模块
3. 参考io_uring的接口设计，特别是参数传递方式考虑模块之间的通用接口
4. 借鉴io_uring，采用completion queue的设计而非提供一个callback函数供被调用模块使用
5. 原先的设计是每个模块是一个线程，希望将其改成每个模块是一个协程

目前已经提交了u740的使用申请，但暂未得到回复。

## 12/07/22

今天确认了两个事实：首先是zCore上的x86_64平台的存储设备还并不支持virtio-blk，目前都是使用ramdisk作为替代。使用参数`DISK=on`会报错，原因是7.0.0版本的qemu似乎已经移除了ahci总线。其次是主线zCore的多核启动在启用kvm的情况下也无法顺利运行，这就与我们的代码改动无关了。

根据导师的建议，目前是尝试调整qemu的配置参数（包括cpu model还有各种flags），直到多核可以顺利跑起来且性能不出现明显瓶颈为止。一个可能比较大的性能瓶颈是vm exit指令的频率。我们可以尝试使用perf来监控qemu的运行来测量这些硬件计数。

根据导师的建议，目前简单的将参数换成了`-cpu qemu64`，就把多核跑起来了，目测速度和tcg一样慢的可以（`qemu64`应该等价与tcg）。同样的参数，原版zCore跑的就非常快，我们的实现要至少慢上一个数量级。我们需要明确：这样的速度差异，到底是我们的实现问题，还是qemu模拟过程中过多的抽象层导致的？更详细一点，我们的最终目标是要比较我们的实现和原版zCore在bare metal上的性能。那么目前的问题就有两种情况：

1. 即使在bare metal上我们的实现也比原版zCore慢一个数量级，这就说明是我们的设计或实现有问题；
2. 仅在qemu目前的参数配置下我们很慢，在bare metal上我们的实现性能不会这么差，则说明是qemu参数配置的问题，目前的抽象栈与我们期望的bare metal有较多差异。

那么如何知道目前的问题是哪一种呢？

我们先考虑再尝试一下各种参数。

> 顺便：其他参数的尝试以及详细数据，以`ls libc-test/src/math`操作为例
>
> * 不使用任何加速，采用Makefile中默认的cpu model（不启用tcg multithread），耗时6.99s->47.17s，即40.18s
>
> * `-cpu kvm64`能正常跑，耗时6.87s->46.97s，即40.1s
>
> * tcg multithread能正常跑，耗时6.55s->46.56s，即40.01s
>
> 相比而言，原版zCore，默认cpu model，耗时7.88s->8.92s，即1.04s

为什么我们这么慢？

大概总结一下我们在跑自己实现的时候看到的一些一致的现象，比如输出是：

```
/ # ls libc-test/src/math
[  6.553935 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1159b8(WRITE | USER), pid=1028
[  6.557232 WARN  1 0:0 linux_syscall] setpgid: unimplemented
[  6.577655 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x95d9c(WRITE | USER), pid=1028
[  6.578147 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e4318(WRITE | USER), pid=1028
[  6.578557 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x92a80(WRITE | USER), pid=1028
[  6.601704 WARN  1 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  6.601924 WARN  1 0:0 linux_syscall] setpgid: unimplemented
[  6.626130 WARN  1 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  6.626168 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x94d84(WRITE | USER), pid=1028
[  6.649589 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f5288(WRITE | USER), pid=1028
[  6.649873 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f7ff0(WRITE | USER), pid=1028
[  6.650147 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e5024(WRITE | USER), pid=1028
[  6.650480 WARN  1 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[  6.673560 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f6390(WRITE | USER), pid=1028
[  6.673985 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[  6.745771 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[  6.747110 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x93948(WRITE | USER), pid=1028
/ # [  6.914618 WARN  1 0:0 linux_syscall::vm] mprotect: unimplemented
[  6.938176 WARN  1 0:0 linux_syscall::vm] mprotect: unimplemented
[  6.962777 WARN  1 0:0 linux_syscall] getuid: unimplemented
[  7.513620 WARN  1 0:0 linux_syscall] brk: unimplemented
[  8.065632 WARN  1 0:0 linux_syscall] brk: unimplemented
[  8.641566 WARN  1 0:0 linux_syscall] brk: unimplemented
[  9.721508 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 10.825578 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 12.937472 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 15.097418 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 19.297288 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 23.497174 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 31.872965 WARN  1 0:0 linux_syscall] brk: unimplemented
{实际输出}
[ 46.560628 WARN  1 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[ 46.608559 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[ 46.704576 WARN  1 0:0 linux_syscall] geteuid: unimplemented
/ # QEMU: Terminated
```

而下面是原版zCore的输出：

```
/ # ls libc-test/src/math
[  7.881749 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1159b8(WRITE | USER), pid=1028
[  7.886516 WARN  0 0:0 linux_syscall] setpgid: unimplemented
[  7.886814 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x95d9c(WRITE | USER), pid=1028
[  7.887260 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e4318(WRITE | USER), pid=1028
[  7.887798 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x92a80(WRITE | USER), pid=1028
[  7.888933 WARN  0 0:0 linux_syscall] setpgid: unimplemented
[  7.889212 WARN  0 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  7.905922 WARN  0 0:0 linux_syscall::vm] mprotect: unimplemented
[  7.906600 WARN  0 0:0 linux_syscall::vm] mprotect: unimplemented
[  7.908023 WARN  0 0:0 linux_syscall] getuid: unimplemented
[  7.913904 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.916394 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.918769 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.925064 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.934318 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.955441 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.987248 WARN  0 0:0 linux_syscall] brk: unimplemented
[  8.075086 WARN  0 0:0 linux_syscall] brk: unimplemented
[  8.205886 WARN  0 0:0 linux_syscall] brk: unimplemented
[  8.581359 WARN  0 0:0 linux_syscall] brk: unimplemented
{实际输出}
[  8.927216 WARN  0 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  8.927666 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x94d84(WRITE | USER), pid=1028
[  8.928118 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f5288(WRITE | USER), pid=1028
[  8.929099 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f7ff0(WRITE | USER), pid=1028
[  8.929390 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e5024(WRITE | USER), pid=1028
[  8.929689 WARN  0 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[  8.929917 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f6390(WRITE | USER), pid=1028
[  8.930245 WARN  0 0:0 linux_syscall] geteuid: unimplemented
[  8.930544 WARN  0 0:0 linux_syscall] geteuid: unimplemented
[  8.930786 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x93948(WRITE | USER), pid=1028
/ # QEMU: Terminated
```

可以看到，拉开差距的关键在两个部分：

第一是中间一长串未实现的brk调用。而且在我们的实现中，每两次调用的间隔越来越久。我们后续可以通过`LOG=info`来看中间究竟发生了什么事情。总体而言，原版zCore总共用了0.7秒左右，而我们用了24秒左右。

第二是最终输出的打印部分。可以看到原版zCore只用了不到半秒钟，而我们的实现用了15秒左右。

其余部分则可以说性能相近，至少没有成为瓶颈。

## 13/07/22

一个非常诡异的现象是是否启用tcg multithread跑的时间差不多。

从输出部分可以看到一个有趣的现象：

```
[ 37.456578 INFO  1 0:0 linux_syscall::file::file] writev: fd=FileDesc(1), iov=0x115c80, count=2
Makefile                fmaf.c                  modfl.c
[ 37.457103 INFO  1 0:0 linux_syscall] <= Ok(86)
[ 37.457317 INFO  1 0:0 linux_syscall::async_syscall] task 1433 dropped
[ 37.480386 INFO  1 0:0 linux_syscall::async_syscall] select task id: 1434
[ 37.480588 INFO  1 0:0 linux_syscall::file::file] writev: fd=FileDesc(1), iov=0x115ca0, count=2
README                  fmaf.err                modfl.err
[ 37.481106 INFO  1 0:0 linux_syscall] <= Ok(88)
[ 37.481332 INFO  1 0:0 linux_syscall::async_syscall] task 1434 dropped
[ 37.504323 INFO  1 0:0 linux_syscall::async_syscall] select task id: 1435
[ 37.504508 INFO  1 0:0 linux_syscall::file::file] writev: fd=FileDesc(1), iov=0x115ca0, count=2
REPORT                  fmaf.exe                modfl.exe
[ 37.504989 INFO  1 0:0 linux_syscall] <= Ok(88)
[ 37.505184 INFO  1 0:0 linux_syscall::async_syscall] task 1435 dropped
[ 37.529004 INFO  1 0:0 linux_syscall::async_syscall] select task id: 1436
[ 37.529398 INFO  1 0:0 linux_syscall::file::file] writev: fd=FileDesc(1), iov=0x115ca0, count=2
acos.c                  fmaf.exe.ld.err         modfl.exe.ld.err
[ 37.529943 INFO  1 0:0 linux_syscall] <= Ok(95)
```

可以看到每隔5行有一个实际的输出。值得注意的是重大的延迟差异在于task dropped和select task之间有0.025s左右的延迟，一共大约有400行的实际输出，于是总延迟在10s左右，这构成了输出部分的绝大部分延迟。那么为什么会有这么高的延迟呢？通过进一步打log，可以看到大部分延迟集中在事件循环开头从共享队列中取出client提交的任务并加到自身队列的过程。进一步定位，发现原因在于大多数时候共享队列中没有任务，但是仍然需要尝试获取共享队列的锁（目前是用的`spin::Mutex`来同步互斥而非`crossbeam`，因为`crossbeam`居然不支持`no_std`），而该锁的同步开销或者自身的基础操作开销居然成为了瓶颈。

所以这意味着我们异步模块的基本架构可能需要做出变更：在接受外部请求方面，需要从轮询转化为一种另类的wakeup，至少尽量减少轮询的频率。另一种思路则是从互斥锁换成无锁队列。找了一下，发现crossbeam的`SegQueue`是支持alloc的，所以先换上试试。crossbeam声称它支持alloc，但实际上并不...看起来是GG了。

于是，另一种思路是使用一个`AtomicBool`来维护目前共享队列中是否有任务。仅当它为`true`的时候，再拿锁并加任务。轮询原子变量应该比加锁要快的多。

但是这样真的有用吗？会不会根本原因在于client响应时间过长？

注意到可能有一个关键性的差异。在原版zCore中，对于一个实际上立即返回的syscall，在`syscall.syscall` poll返回之后立刻推进原来Future的进度；而对于我们的修改版，首先在发出syscall请求之后，会返回一个Pending，然后原本的Future会被丢回到client core上的runtime中。在我们server core调用wakeup_fn之后，我们还需要等待：原先的Future被调度回来，重新走一遍自顶向下的poll路径，接着才能继续推进原先Future的进度。那么这0.025s左右的延迟可能来自于这两部分？

## 19/07/22

继续几天之前的工作...如何知道这0.025s的延迟来自什么地方？

根据经验来看对Future重新poll一遍应该不会产生什么显著开销，那么就应该是调度器某些实现的问题？让我们重新去看一下client调度器的代码吧。

通过`LOG=debug`可以看到重新poll一次Future的延迟非常低。主要问题是我们在server端wakeup一个client之后，经过了大概0.02s之后才看到client的输出，而且是一个比较奇怪的`handle kernel timeout`，看起来与时钟中断有关。

具体来说，中间发生的事情如下：

```
[ 11.446432 INFO  1 0:0 linux_syscall::async_syscall] wakeup client!
[ 11.466677 DEBUG 0 0:0 executor::runtime] handle kernel timeout
[ 11.466895 DEBUG 0 0:0 executor::runtime] switch 1 -> idle
[ 11.467044 DEBUG 0 0:0 executor::runtime] switch idle -> 1
[ 11.467200 DEBUG 0 0:0 executor::runtime] run strong executor
[ 11.467400 DEBUG 0 0:0 executor::executor] running future 1:2
[ 11.467576 DEBUG 0 0:0 zcore_loader::linux] go to user: tid = 1059 pc = 45567
[ 11.467871 DEBUG 0 0:0 zcore_loader::linux] back from user: tid = 1059 pc = 45567 trap reason = Syscall
[ 11.468182 DEBUG 0 0:0 executor::executor] back from future 1:2
[ 11.468184 INFO  1 0:0 linux_syscall::async_syscall] queue op completed, before adding new req
```

首先明确一下时钟中断的调用链，也就是这个`handle kernel timeout`是如何被打印出来的。我们看到kernel-hal中的`trap_handler`函数在时钟中断的时候尝试调用`executor::handle_timeout`，而从这个函数的实现来看，它是通过调用`sched_yield`函数从当前的executor切换回管理这个executor的runtime。

所以问题的关键可能是，为什么一定要等到时钟中断的时候client的调度才会产生变化？

这时候，我忽然看到了这样一行输出：

```
[ 11.468389 DEBUG 0 0:0 executor::executor] no other tasks, wait for interrupt
```

这大概可以看出，zCore目前真的是没有任何一个后台任务。在当前应用thread Future向server提交请求进入阻塞状态之后，当前executor上就再也没有任何活跃任务了，于是直接调用`wait_for_interrupt`等待下一个时钟中断，在此期间CPU直接摆烂...即使server已经完成了系统调用请求，还是要等到client上触发了时钟中断才能接着往下跑...

那么解决方案有两种，要么去掉这个万恶的`wait_for_interrupt`，要么加上一个固定的后台任务。看起来第一种方法比较简单...但是实际上是不行的，因为目前串口也是通过中断驱动的。如果这样做就相当于内核态全程关中断了，无法进行输入。于是我们考虑另一种简单的方案，就是将时钟中断频率从100Hz提高为500Hz。这个只需要改kernel-hal就行了。效果如下：

```
/ # ls libc-test/src/math
[  4.976590 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1159b8(WRITE | USER), pid=1028
[  4.979743 WARN  1 0:0 linux_syscall] setpgid: unimplemented
[  4.980984 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x95d9c(WRITE | USER), pid=1028
[  4.981336 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e4318(WRITE | USER), pid=1028
[  4.981720 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x92a80(WRITE | USER), pid=1028
[  4.986152 WARN  1 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  4.986823 WARN  1 0:0 linux_syscall] setpgid: unimplemented
[  4.990865 WARN  1 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  4.990872 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x94d84(WRITE | USER), pid=1028
[  4.995567 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f5288(WRITE | USER), pid=1028
[  4.996432 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f7ff0(WRITE | USER), pid=1028
[  4.997047 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e5024(WRITE | USER), pid=1028
[  4.997470 WARN  1 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[  5.000148 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f6390(WRITE | USER), pid=1028
[  5.000587 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[  5.019448 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[  5.020370 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x93948(WRITE | USER), pid=1028
/ # [  5.049136 WARN  1 0:0 linux_syscall::vm] mprotect: unimplemented
[  5.053262 WARN  1 0:0 linux_syscall::vm] mprotect: unimplemented
[  5.059380 WARN  1 0:0 linux_syscall] getuid: unimplemented
[  5.168183 WARN  1 0:0 linux_syscall] brk: unimplemented
[  5.278573 WARN  1 0:0 linux_syscall] brk: unimplemented
[  5.393731 WARN  1 0:0 linux_syscall] brk: unimplemented
[  5.609772 WARN  1 0:0 linux_syscall] brk: unimplemented
[  5.830575 WARN  1 0:0 linux_syscall] brk: unimplemented
[  6.252955 WARN  1 0:0 linux_syscall] brk: unimplemented
[  6.684915 WARN  1 0:0 linux_syscall] brk: unimplemented
[  7.524890 WARN  1 0:0 linux_syscall] brk: unimplemented
[  8.364865 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 10.040016 WARN  1 0:0 linux_syscall] brk: unimplemented
{output}
[ 13.116722 WARN  1 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[ 13.126304 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[ 13.145473 WARN  1 0:0 linux_syscall] geteuid: unimplemented
/ #
```

确实比原来快了很多，如果我们将其调整为1000Hz的话，用时总计4秒左右。而原版zCore的总用时大概在1秒多，所以就是说要调整为4000Hz?可以试试，这回总延迟降低到1.81秒（原版zCore 1.05秒）。当然我们并不想让时钟中断的频率这么高，所以还是加上一个固定的后台任务会更好。后台任务的做法似乎并不行，因为这样就变成全程关中断了，会造成无法输入。

---

另一件事是搞清楚qemu的模拟相比裸机性能是否有很大差异（特别是虚拟化相关开销）。

首先通过perf看看tcg multithread是否在vmexit这里会有些开销。

---

另外一件事情是在x86_64平台上启用virtio-blk并使用异步操作。

---

今天跟导师讨论之后的纪要：

1. 尽量快些尝试`qemu-system-x86_64`启用kvm情况下的多核启动，这将决定平台问题，如果周四之前还是搞不定的话就去拿sifive unleashed物理板子，当然我估计驱动也并不完备...我印象中搞过的人不多。远程的u740板子优先级也排在kvm之后。

   导师提到kvm有一个额外的好处就是对于各种performance counter有原生支持，在RV板子上虽说也有差不多的东西但是要自己写一些额外的代码...这些性能监控指标将很大程度上决定我们的工作是否有效，因为我们毕竟是专注于cache利用率、同步开销这些指标而不单单是表面上的运行时长。

2. 搞清楚zCore里面，在x86_64平台上，中断从始至终何时打开、何时关闭

3. 继续替换存储相关的软件栈，使用virtio

不过考虑到我对x86的了解，这些都有些困难就是了...

## 21/07/22

今天搞定了启用kvm情况下的多核启动，然后我之前的代码也能跑通了。虽然过程比较曲折，但结果却出人意料的简单。同时，今天Hifive Unmatched板子到手了，虽然在kvm跑通的情况下可能这个不太会被用到了。然后发现kvm跑的速度居然要比tcg慢，这个感觉可以先放着不管。

跑通之后，下一步就考虑替换存储软件栈了。

稍微找了一下，`qemu-system-x86_64`似乎是不支持以MMIO的形式接入virtio设备的，可能需要从pci总线接入。

我们调整了一下`DISK=on`模式下的QEMU块设备相关设置如下：

```Makefile
qemu_opts += -device virtio-blk-pci,drive=userdisk
```

然后zCore的pci相关驱动已经比较完善了，因此可以看到扫描到了这个块设备，但是没有进一步进行处理：

```
[  0.752253 INFO  0 0:0 zcore_drivers::bus::pci] pci: 0:3:0 1af4:1001 (1 0) irq: 11:Some(INTA)
[  0.753208 WARN  0 0:0 zcore_drivers::bus::pci] NoResources, failed to initialize PCI device: 1af4:1001
```

顺带一提，每个pci设备有一个vendor ID和一个device ID。vendor ID为0x1af4表示这是一个virtio设备，然后后面的device ID区分它是块设备还是网卡等等。从virtio spec可以看出，0x1001确实是一个块设备。那么，怎么在尽量复用已有代码的前提下，把这个块设备驱动起来呢？

（随便找了一圈似乎直接MMIO的virtio-blk在x86这里是不可用的，因此还是上pci，第一步是先搞定轮询式访问，再考虑中断式访问）

## 22/07/22

找到[pci总线介绍](https://wiki.osdev.org/PCI)(osdev yyds)，先稍微看一眼...

有一个Configuration Space，其中每个外设有256字节（寄存器以32位作为单位，共64个）的寄存器空间用来进行配置

然后有一个`CONFIG_ADDRESS`（应该是端口号0x03F8的一个32位寄存器）用来表明访问哪个寄存器，其中包含(bus,device,function)三元组确定一个pci外设，然后还有一个8位的offset（低2位必须为0）表示访问这个pci外设的哪个configuration register。写入了`CONFIG_ADDRESS`之后，就可以通过`CONFIG_DATA`（端口号0x0CFC的一个32位寄存器）访问这个寄存器。

对于一个一般pci外设而言，这256字节的布局见[1](https://wiki.osdev.org/PCI#Header_Type_0x0)，即Header Type=0x0的情况。其中比较有用的应该是6个BAR(Base Address Register)，还有用于中断处理的Interrupt PIN和Interrupt Line。BAR每个32位，类似于一个指针，指向实际与设备打交道的区域。有两种BAR，memory类型指向一块物理内存区域，而I/O类型则不限于物理内存区域。二者布局见[2](https://wiki.osdev.org/PCI#Base_Address_Registers)。先考虑memory类型，当Type=0x0的时候表明一个32bit的base register；当Type=0x1的时候表明一个64bit的base register，注意这会消耗两个BAR。

关于中断处理，据说pci设备支持三种不同类型的中断处理：pic最简单，但可能已经不支持了；ioapic最复杂；另外一种是通过MSI(Message Signaled Interrupts)，而virtio看起来使用的是MSI-X。

一个问题：如何从Configuration Space中找到virtio spec中提到的Capability？看起来应该是Configuration Space，当Header Type=0x0的时候可以找到一个Capabilities Pointer，指向一个Capabilities构成的链表的表头，格式是在Configuration Space中的偏移量。(关于Configuration Space的解析可以参考[3](https://github.com/elliott10/pci-rs/blob/master/src/lib.rs#L544))。然后链表中的每一项开头格式都是固定的：

```c
struct virtio_pci_cap {
    u8 cap_vndr; //*
    u8 cap_next; //*
    u8 cap_len; //*
    u8 cfg_type;
    u8 bar;
    u8 padding[3];
    le32 offset;
    le32 length;
};
```

即开头的这三个字节。已有的pci驱动能够打印出这个链表，结果如：

```
[  0.784881 INFO  0 0:0 zcore_drivers::bus::pci] Capability { cap_ptr: 152, data: MSIX }
[  0.785729 INFO  0 0:0 zcore_drivers::bus::pci] Capability { cap_ptr: 132, data: Unknown(9) }
[  0.786651 INFO  0 0:0 zcore_drivers::bus::pci] Capability { cap_ptr: 112, data: Unknown(9) }
[  0.787750 INFO  0 0:0 zcore_drivers::bus::pci] Capability { cap_ptr: 96, data: Unknown(9) }
[  0.788604 INFO  0 0:0 zcore_drivers::bus::pci] Capability { cap_ptr: 80, data: Unknown(9) }
[  0.789479 INFO  0 0:0 zcore_drivers::bus::pci] Capability { cap_ptr: 64, data: Unknown(9) }
```

可以看出应该是倒序打印（这里我有点想直接改pci-rs，不知道会不会有什么影响）。这里的9指的是第一个字节的内容为0x09，能够对上virtio文档的内容，那么这几项应该分别指的是virtio spec中提到的：

```
/* Common configuration */
#define VIRTIO_PCI_CAP_COMMON_CFG 1
/* Notifications */
#define VIRTIO_PCI_CAP_NOTIFY_CFG 2
/* ISR Status */
#define VIRTIO_PCI_CAP_ISR_CFG 3
/* Device specific configuration */
#define VIRTIO_PCI_CAP_DEVICE_CFG 4
/* PCI configuration access */
#define VIRTIO_PCI_CAP_PCI_CFG 5
```

链表的最后一项第一个字节为0x11，是一个MSI-X相关的配置。这样看起来的话终于初步弄懂PCI总体如何使用了，但是怎么写代码还不清楚...QAQ无论如何，virtio特定的处理不应该放在pci-rs里面，而相对的pci-rs需要对外提供更加好用的抽象，目前还是不够的。

## 23/07/22

此时我们可以简单回顾一下virtio的基础知识（基础？）参考virtio-1.1版本文档：

一个virtio设备，无论采用何种总线接入计算机，均包含如下的内容：

* device status

  当驱动程序对设备进行初始化的时候，驱动一般会遵循文档3.1节所提到的一系列步骤，而device status可以指出当前进行到了哪一步。按照通常情况下被操作的顺序有如下的二进制位：

  ACK(1)：表示guest OS发现并识别了该virtio设备；

  DRIVER(2)：表示guest OS了解如何驱动该设备；

  FAILED(128)：表示guest OS段出错导致其决定放弃使用该设备；

  FEATURES_OK(8)：表示驱动程序已经接收到了所有它支持的feature，同时与设备之间对于feature的协商已经完成；

  DRIVER_OK(4)：表示驱动程序初始化完毕并可以驱动该设备；

  DEVICE_NEEDS_RESET(64)：表示设备出现了不可恢复错误，从而需要被重置。

  驱动程序必须根据驱动初始化进程来更新device status。

* feature bits

  设备会提供一组特性。在驱动初始化的时候，驱动读取设备提供的所有特性，并从中选取一个它支持的子集回报给设备。这个过程就是virtio设备和驱动关于特性的协商。只有重置设备之后，才能重新协商。这种做法有利于设备和驱动之间的兼容性。

* notifications

  virtio设备和驱动之间一共有三种不同的通知类型：

  配置变更通知：设备->驱动。

  available buffer通知：驱动->设备。用于向设备提交请求。

  used buffer通知：设备->驱动。用于告知驱动请求完成。

  具体情况似乎与总线的选取有关。

* device configuration space

  用来放置几乎不变或是初始化期间的那些参数。

  驱动不能假定对于config space超过32位宽的单field读取或者任意位宽的多fields读取是原子的。当驱动读取config space的时候应当连续读取直到结果不发生变化为止。当访问可选的config space域之前，驱动必须确认相关的feature是否存在。

* virtqueues(one or more)

virtqueue就单拎出来说。每个virtio设备可以有零个或多个virtqueue。驱动提交请求时，可以向virtqueue中加入一个available buffer，并可选地向设备发送一个available buffer通知；当设备处理请求完毕时，可以向virtqueue加入一个used buffer（告知驱动这个buffer已经使用完毕了），并可选地向驱动发送一个used buffer通知。

设备需要对于它使用的每一个buffer上报它向内存中写入的字节数，这被称为used length。设备并不需要保证按照buffer被设置为available的顺序来使用这些buffer。不过设备可以这样做，相关特性：VIRTIO_F_IN_ORDER。

virtqueue由三个部分组成：Descriptor区域/Driver区域/Device区域，它们曾经被称为Descriptor Table/Available Ring/Used Ring。Descriptor用来保存buffer的描述符，Driver区域保存驱动发给设备的额外数据，而Device区域则相反。spec提到virtqueue目前有两种格式：split/packed virtqueue。我们目前常用的应该还是split virtqueue。virtqueue的细节就不展开了。

---

如果使用PCI总线的话，virtio设备的configuration space主要有5个capabilities。

第一个是common configuration:

```c
struct virtio_pci_common_cfg {
    /* about the whole device */
    le32 device_feature_sel;		// RW
    le32 device_feature;			// RO
    le32 driver_feature_sel;		// RW
    le32 driver_feature;			// RW
    le16 msix_config;				// RW, configuration vector for MSI-X
    le16 num_queues;				// RO
    u8 device_status;				// RW
    u8 config_generation;			// RO，当配置发生重大变更时，由设备修改
    /* about a specific virtqueue */
    le16 queue_sel;					// RW，选择哪个virtqueue，影响下面的所有内容
    le16 queue_size;				// RW
    le16 queue_msix_vector;			// RW, queue vector for MSI-X
    le16 queue_enable;				// RW
    le16 queue_notify_off;			// RO，当前virtqueue在notification结构中的偏移，单位非字节
    le64 queue_desc;				// RW，descriptor table的物理地址
    le64 queue_driver;				// RW，driver area的物理地址
    le64 queue_device;				// RW，device area的物理地址
}
```

对于驱动的要求：驱动不能写入RO项；如果启用（即设备和驱动均通过）VIRTIO_F_RING_PACKED，queue_size不能被设置为0；如果没有启用，queue_size必须被设置为2的幂次；驱动必须在打开queue_enable之前设置完其他virtqueue相关域；将0写入device_status之后，驱动必须等待直到读到device_status为0之后才能重新初始化设备；驱动不能写入0到queue_enable（这就是说不能禁用一个queue？）。

第二个是notification，感觉没什么用，暂时跳过。

第三个是ISR status，指向至少一个字节，包含一个8bit的ISR status域用于INT#x的中断处理。ISR status的offset没有对齐要求。这个ISR用于区分设备特定配置变更的中断和普通的virtqueue中断。spec提到如果MSIX capability启用的话，驱动检测到一个queue中断的时候不应该访问这个ISR status。所以这个也没什么用？

第四个是device-specific configuration，没有提到驱动这方面的事情。

最后一个是PCI configuration。用于提供一种备用的访问其他四个capabilities的方式。所以感觉也没什么用。

我们可以与基于MMIO的virtio header比较一下看看有什么不同（下面是基于MMIO的一些相关的字段）：

* device_features: 0x010, le32, r
* device_features_sel: 0x014, le32, w
* driver_features: 0x020, le32, w
* driver_features_sel, 0x024, le32, w
* queue_sel, 0x030, w
* queue_num_max, 0x034, r（应该就是queue_size）
* queue_ready, 0x044, rw（写入一个0x1之后通知设备可以开始处理这个virtqueue上的请求了，暂时找不到对应）
* queue_notify, 0x050, w（写入这个寄存器之后通知设备virtqueue上有新请求，写入方法见available buffer notification，我印象中这个比较常用，pci上应该也差不多，只不过queue notify address的计算方法应该不同）
* interrupt_status, 0x060, r（当中断的时候，读取这个寄存器可以知道设备触发的是virtqueue中断还是设备配置中断）
* interrupt_ack, 0x064, w（当处理完中断之后，向interrupt_ack写入一个与interrupt_status读到的相同的值表明该中断处理完成）
* status, 0x070, rw（应该同pci上的device_status）
* 后面是等价的virtqueue包含的三个区域的64位物理地址
* config_generation, 0x0fc, r（应该等价与pci上的同名字段）

所以总体来说，看起来是差不多的。

---

那么基于PCI的virtio设备如何进行初始化和其他操作呢？

先跳过一些device layout相关的遗留信息，感觉并不会用到。

关于MSI-X vector设置：当MSI-X capability存在且被设备使能，config_msix_vector和queue_msix_vector被用来将配置变更中断和队列相关中断映射到MSI-X vectors中。这种情况下ISR status被弃置。

写入一个合法的MSI-X表项编号（0~0x7FF）到config_msix_vector/queue_msix_vector可以将各自的中断映射到表项编号对应的MSI-X vector。若要关闭某种中断的话，则只需写入NO_VECTOR（即0xFFFF）。注意映射的时候可能需要设备进行内部资源分配，因而是可能失败的。

关于virtqueue配置：驱动需要对于设备拥有的每个virtqueue按顺序进行如下操作：

1. 将virtqueue的索引（以0开头）写入queue_sel；
2. 从queue_size读取当前virtqueue的容量。如果结果为0，表明这个virtqueue并不存在；
3. （可选地）选择一个更小的virtqueue size写入queue_size；
4. 分配并零初始化virtqueue所要用到的buffer描述符表、Driver区域和Device区域；
5. （可选地）如果MSI-X特性存在且设备支持，选取一个向量用来请求virtqueue触发的中断。如前所述，写入queue_msix_vector，再读取一下，如果设置成功的话它不变，否则会变为0xFFFF。

关于available buffer通知的设置：

* 如果特性VIRTIO_F_NOTIFICATION_DATA未通过，驱动通知设备时，只需写入16位的virtqueue索引到queue notify address（计算方法详见原文）。
* 如果特性通过，驱动写入一个32位（详见原文）的值到queue notify address。

关于used buffer通知的设置（设备配置变更中断同理）：

这里只考虑基于MSI-X的情形。应该是从MSI-X总线上根据中断号为queue_msix_vector获取对应的中断信息。

驱动的中断处理：基于MSI-X，遍历所有映射到这个MSI-X vector上的virtqueue，检查这个virtqueue上是否有请求完成了。如果这个MSI-X vector与config_msix_vector相同，则检查设备配置是否发生变更。

---

既然要基于MSI-X做中断处理，我们再来研读一下相关文档[1](https://wiki.osdev.org/PCI#Message_Signaled_Interrupts)

感觉稍微有点复杂，等把轮询处理完毕再弄中断吧QAQ

## 24/07/22

然后就正式开始写驱动了。

第一件事应该是成功找到并识别重要的virtio pci capabilities。这是原先pci-rs中Capability的定义：

```rust
#[derive(Debug, Copy, Clone, PartialEq, Eq)]
pub struct Capability {
    cap_ptr: u16,
    data: CapabilityData,
}
```

可以看到其中略去cap_vendor为0x09即vendor-specific的情况。为了找到0x09的通用格式，保证它不限于virtio，需要看一下pci文档[1](https://docplayer.net/17396169-Pci-local-bus-specification-revision-3-0-august-12-2002.html)。很不巧的是，附录H中提到0x09的格式（特别是后半部分）是vendor-specific的。所以我们在pci-rs里面应该做的更加通用一些？

目前我们已经成功打印出每个Capability的基本信息。

记录一下Port相关抽象的设计。在pci-rs可以看到有一个名为PortOps的trait，抽象了不同架构上端口相关的行为。x86_64架构上的实现PortOpsImpl可以在zCore/drivers中找到。然后有这么几种方法：如果是16位端口号，可以读写8/16位的值；如果是32位端口号，只能读写32位的值。

打印了一下cfg_type=0x1的cap的bar信息：

```
Memory(34359738368, 16382, Yes, Bits64)
```

看起来这个bar的type=0x2，也就是基地址寄存器为64位，可以映射到64位地址空间的任意位置，而且需要消耗两个BAR。

打印出来一个综合版的信息：

```
[  0.767727 INFO  0 0:0 zcore_drivers::bus::pci] cfg_type=5,bar_idx=0,offset=0,length=0
[  0.768824 INFO  0 0:0 zcore_drivers::bus::pci] BAR::IO
[  0.769721 INFO  0 0:0 zcore_drivers::bus::pci] cfg_type=2,bar_idx=4,offset=12288,length=4096
[  0.770923 INFO  0 0:0 zcore_drivers::bus::pci] BAR::MEM(addr=0x800000000,len=0x3ffe,Yes,Bits64)
[  0.772205 INFO  0 0:0 zcore_drivers::bus::pci] cfg_type=4,bar_idx=4,offset=8192,length=4096
[  0.773414 INFO  0 0:0 zcore_drivers::bus::pci] BAR::MEM(addr=0x800000000,len=0x3ffe,Yes,Bits64)
[  0.774718 INFO  0 0:0 zcore_drivers::bus::pci] cfg_type=3,bar_idx=4,offset=4096,length=4096
[  0.775915 INFO  0 0:0 zcore_drivers::bus::pci] BAR::MEM(addr=0x800000000,len=0x3ffe,Yes,Bits64)
[  0.777173 INFO  0 0:0 zcore_drivers::bus::pci] cfg_type=1,bar_idx=4,offset=0,length=4096
[  0.778364 INFO  0 0:0 zcore_drivers::bus::pci] BAR::MEM(addr=0x800000000,len=0x3ffe,Yes,Bits64)
```

从中大概可以看出pci版的virtio header基地址是0x800000000，长度0x1000。这样的话，感觉可以把这个信息传给virtio-drivers了。当然，在此之前要整理一下zCore/drivers的代码。目前已经成功从(vendor, device)索引到virtio-pci设备并找到最重要的capability了。

感觉现在需要了解一下queue_notify_off的细节了。参考之前的VirtIOHeader的时候，发现有一个queue_align和queue_pfn对不上。找了一下这两个来自MMIO Legacy Interface。然后这两个大概也是和virtqueue的Legacy接口相关的，所以感觉我们可以先不管？

## 25/07/22

如果使用legacy virtqueue接口的话，则相当于三个区域的对齐要求更高（至少占用两个页面，大小自定），位置必须固定，且需要额外设置queue_align（即自定义的页面大小）和queue_pfn（应该是第一个区域的起始地址除以queue_align），这两个唯一确定三个区域的位置。相对的，MMIO header里面三个区域的64位基址是并没有用到的。如果PCI header仍使用legacy接口的话，queue_align会被固定在4096，而写入也会变成三个区域的物理页号而非具体的物理地址。同时，使用legacy接口的话，queue size不可调整。

如果不使用的话，三个区域则分别是连续内存即可，对齐要求较低，它们也不必连在一起。可能对于内存分配器有一定要求，不过我们可以强制性把对齐要求拉到4096，这样只要用物理页帧分配器就行了。这样做的话，在初始化的时候写入三个64位地址就行了。

---

现在终于得仔细研究一下`self.header.notify(0);`怎么弄了。这一步相当于正式向设备提交请求。

当没有VIRTIO_F_NOTIFICATION_DATA这项特性的时候，只需将virtqueue number（16-bit virtqueue index）写入设备。若基于PCI总线，我们需要找到另一个称之为VIRTIO_PCI_CAP_NOTIFY_CFG的capability。它的格式是：

```c
struct virtio_pci_notify_cap {
    struct virtio_pci_cap cap;
    le32 notify_off_multiplier;
}
```

于是queue notify address的计算方法如下：`cap.offset+queue_notify_off*notify_off_multiplier`，其中`queue_nofity_off`来自之前提到过的common_cfg capability。计算出来的值应该是`cap.bar`中的偏移量...说起来还蛮复杂的。

现在的情况是实现完了，但是`read_block`的时候直接卡在spin loop里面了。这就说明前面的某些设置还是不对...

坑1：在PCI上queue_size应该设置为256（根据common_cfg）而不是16。

坑2：发现漏掉一个queue_enable没有设置（默认居然是0！），但是设置完了还是一样。

坑3：感觉问题可能出在我某些地方使用了legacy interface，但是又没有完全用...比如4.1.4.8节。问题应该就出在这里，不去访问位于BAR0的特殊header的话，设备不知道你要用legacy接口的。我感觉要不完全放弃legacy接口吧。这体现在写三个区域的物理地址的时候直接写而不是写页号。然而分配策略还是沿用之前，这会造成一定的内存浪费，不过目前并不重要...暂时先尝试不调低queue_size，也就是还保持256。

坑4：感觉虚实映射没太搞明白...x86上PHYS_MEMORY_BASE居然是0...?页表映射看起来是在rboot里面，物理内存是以config.physical_memory_offset来映射的，从生成出来的rboot.conf来看应该是0xFFFF800000000000。重新思考一下我之前干了什么吧。实际上Capability的解析我是调用的接口，然后实际的header是要从BAR里面找到的，而这个时候我竟然直接写物理地址，就炸了。这回在拿到一个BAR开头的物理地址的时候，先将其转换为虚拟地址，然后再用来获取&'static mut header。这样搞完之后发现地址反而炸了，感觉BAR这种鬼东西（物理地址0x8_0000_0000）应该已经不是RAM了，盲猜不知道什么时候进行了恒等映射。所以之前的流程才能跑通。我强行加上一个offset反而炸了。感觉BAR0（一个BAR::IO）应该也是这样的，起始地址0x6000，应该也是恒等映射的。

突发奇想关一下kvm试试。一样GG。尝试调整queue_enable的位置，然并卵。

## 26/07/22

跟Lluis学到一手`-trace`（顺带一提，重复多个`-trace`就可以跟踪一组事件），综合来看感觉还是notify出了问题，因为notify之后没有看到任何virtio相关的trace输出。翻了一下QEMU源码，`hw/virtio/virtio-pci.c`应该比较相关。在初始化块设备的时候有这样的log:

```
virtio_set_status vdev 0x558b0ecb19a0 val 1
virtio_set_status vdev 0x558b0ecb19a0 val 2
[  1.513451 INFO  0 0:0 virtio_drivers::pci::blk] device features: SEG_MAX | GEOMETRY | BLK_SIZE | FLUSH | TOPOLOGY | CONFIG_WCE | DISCARD | WRITE_ZEROES | RING_INDIRECT_DESC | RING_EVENT_IDX | VERSION_1
virtio_set_status vdev 0x558b0ecb19a0 val 8
[  1.514275 INFO  0 0:0 virtio_drivers::pci::blk] config: BlkConfig { capacity: Volatile(204800), size_max: Volatile(0), seg_max: Volatile(254), cylinders: Volatile(203), heads: Volatile(16), sectors: Volatile(63), blk_size: Volatile(512), physical_block_exp: Volatile(0), alignment_offset: Volatile(0), min_io_size: Volatile(0), opt_io_size: Volatile(0) }
[  1.515343 INFO  0 0:0 virtio_drivers::pci::blk] found a block device of size 102400KB
[  1.515717 INFO  0 0:0 virtio_drivers::queue] max_queue_size=256
[  1.515939 INFO  0 0:0 virtio_drivers::queue] desc_paddr=0x263000,avail_paddr=0x263100,used_paddr=0x264000
[  1.516399 INFO  0 0:0 virtio_drivers::pci::header] queue_enable=0
[  1.516644 INFO  0 0:0 virtio_drivers::pci::header] queue_enable=1
virtio_set_status vdev 0x558b0ecb19a0 val 4
virtio_blk_data_plane_start dataplane 0x558b0ecc2ca0
virtio_queue_notify vdev 0x558b0ecb19a0 n 0 vq 0x7fbd3ae69010
```

在notify地址不变的情况下，将u16写入改成u32，还是不行。

现在让我比较怀疑的是BAR0的冲突问题。因为notify_cap.cap.bar里面就是0。然后最后一个capability（pci_cfg）的bar也是0。spec中提到设置pci_cfg的cap.bar/length/offset就可以在pci_cfg.pci_cfg_data里面读到其他BAR的数据。简单列一个表，对于BAR0来说：

从pci_cfg的视角来看，0x0->cap.cap_vndr, 0x1->cap.cap_next, 0x2->cap.cap_len, 0x3->cap.cfg_type, 0x4->cap.bar后面略，也就是只会用到0x4以及后面的内容

从notify的视角来看，我们应该是只会用到0x0-0x3（即使notify的时候写入一个u32），所以这两个应该是不存在冲突的？？？

仔细看了一下文档，感觉notify address应该还是在BAR4里面的，之前的理解有误了。我感觉正确的地址应该就是之前我认为的pci_cfg所在的那个区域的开头。

那么再改回16位试试，因为发现qemu打了两遍log

```
virtio_queue_notify vdev 0x56509a55b9a0 n 0 vq 0x7f9fea88c010
virtio_queue_notify vdev 0x56509a55b9a0 n 0 vq 0x7f9fea88c010
qemu-system-x86_64: virtio: bogus descriptor or out of resources
```

这回log只打一行了，但错误还是一样...Google一下吧。

先把内存从1G->2G试试。看起来应该是跟内存没有关系。这个bug暂时有点束手无策...

观察了一下qemu的trace，这个块设备似乎在之前的启动阶段被访问过，应该是检查它里面是不是个bootloader。但是之前的访问都是顺利的啊...?

## 27/07/22

用GDB调试QEMU又是断点停不下来...只能看QEMU源码在里面打log了...

首先想办法看一下bogus desc的调用链。

在include/exec/下面能找到一些内存管理相关的header。从hwaddr.h中可以看到hwaddr其实就是u64。

尝试看看如下这个trace是如何发生的：

```
virtio_queue_notify vdev 0x559b3f296450 n 0 vq 0x7f9ea0088010
```

感觉gdb和print都很不好用...还是不知道具体怎么回事。

```
qemu-system-x86_64: info: into virtqueue_map_desc, addr=0x7ffd3f8f5088 pa=7f0001352600 sz=200
qemu-system-x86_64: info: into dma_memory_map, addr=7f0001352600, len=200
qemu-system-x86_64: virtio: bogus descriptor or out of resources
```

看输出，感觉是在映射512字节的缓冲区的时候出的问题。但是缓冲区已经转换为物理地址了啊...?

```
[  2.514073 INFO  0 0:0 virtio_drivers::queue] set_buf va=0xffffff01001ff650, pa=0x7f01001ff650 len=0x10
[  2.515153 WARN  1 0:0 linux_syscall::async_syscall] Hello world!
[  2.515342 INFO  0 0:0 virtio_drivers::queue] set_buf va=0xffffff0001352600, pa=0x7f0001352600 len=0x200
[  2.515811 INFO  0 0:0 virtio_drivers::queue] set_buf va=0xffffff01001ff598, pa=0x7f01001ff598 len=0x1
```

打印了一下，确实有虚实转换，但是最终得到的物理地址似乎超乎寻常的高。要知道，目前一共只分配了2G的RAM。改成1G之后，还是同样的效果。要知道，在common_cfg里面设置三个区域的物理地址的时候，结果似乎就非常合理：

```
[  1.531455 INFO  0 0:0 virtio_drivers::queue] desc_paddr=0x263000,avail_paddr=0x263100,used_paddr=0x264000
```

从DMA的实现来看，这里应该是直接分配了若干物理页帧，那么物理地址已经有了。然后需要的是物理转虚拟地址，因为在内核里面我们需要通过映射构造虚拟地址来访问一个物理地址。这三个区域的虚拟和物理地址都很正常：

```
[  1.539881 INFO  0 0:0 virtio_drivers::queue] desc_vaddr=0xffff800000263000,avail_vaddr=0xffff800000263100,used_vaddr=0xffff800000264000
[  1.540284 INFO  0 0:0 virtio_drivers::queue] desc_paddr=0x263000,avail_paddr=0x263100,used_paddr=0x264000
```

然后，zCore的物理内存分配器在初始化的时候打印出了所有可用的区域：

```
[  1.516617 INFO  0 0:0 zcore::memory] Frame allocator: add range 0x1000..0xa0000
[  1.517039 INFO  0 0:0 zcore::memory] Frame allocator: add range 0x100000..0x800000
[  1.517241 INFO  0 0:0 zcore::memory] Frame allocator: add range 0x808000..0x80b000
[  1.517402 INFO  0 0:0 zcore::memory] Frame allocator: add range 0x80c000..0x810000
[  1.517763 INFO  0 0:0 zcore::memory] Frame allocator: add range 0x1500000..0x37dbb000
[  1.517959 INFO  0 0:0 zcore::memory] Frame allocator: add range 0x3fe00000..0x3fed3000
```

上面那些非常高的物理地址是明显不在其中的。

## 28/07/22

先来看一下0xFFFFFF01XXXXXXXX，这个应该是在core0的内核栈上。而那个缓冲区的话，应该是在堆上面。内核堆的实现，就是在内核的`.data`或是`.bss`段上提前分配一个static mut的大数组。

rboot里面在映射栈的时候，是每次从物理页帧分配器中选择一个来映射；而在映射ELF的时候，看起来是预留下一部分连续物理内存进行线性映射。

而在kernel-hal/mem.rs中，可以看到virt_to_phys的实现方法也是线性映射，那么偏移量是否正确呢？

首先还是要看调用的是不是这个函数。在virtio-drivers中，页表映射是调用的来自外部的C接口：

```rust
extern "C" {
    fn virtio_dma_alloc(pages: usize) -> PhysAddr;
    fn virtio_dma_dealloc(paddr: PhysAddr, pages: usize) -> i32;
    fn virtio_phys_to_virt(paddr: PhysAddr) -> VirtAddr;
    fn virtio_virt_to_phys(vaddr: VirtAddr) -> PhysAddr;
}
```

然后在kernel-hal/drivers.rs中，确实看到了对应的实现，这和kernel-hal/mem.rs中是一致的：

```rust
#[no_mangle]
extern "C" fn virtio_phys_to_virt(paddr: PhysAddr) -> VirtAddr {
    paddr + KCONFIG.phys_to_virt_offset
}
#[no_mangle]
extern "C" fn virtio_virt_to_phys(vaddr: VirtAddr) -> PhysAddr {
    vaddr - KCONFIG.phys_to_virt_offset
}
```

如果这样的话，我们尝试看看错误的映射结果偏移量是多少：

```
>>> hex(0xffffff01001ff650-0x7f01001ff650)
'0xffff800000000000'
>>> hex(0xffffff0001352600-0x7f0001352600)
'0xffff800000000000'
```

这个是物理内存的偏移量。

然而，实际上在映射zCore ELF的时候，最后的.bss段：

```
[ INFO]: src/page_table.rs@057: mapping segment: Ph64(
[ INFO]: src/page_table.rs@057:     ProgramHeader64 {
[ INFO]: src/page_table.rs@057:         type_: Ok(
[ INFO]: src/page_table.rs@057:             Load,
[ INFO]: src/page_table.rs@057:         ),
[ INFO]: src/page_table.rs@057:         flags: Flags(
[ INFO]: src/page_table.rs@057:             0x6,
[ INFO]: src/page_table.rs@057:         ),
[ INFO]: src/page_table.rs@057:         offset: 0x13a000,
[ INFO]: src/page_table.rs@057:         virtual_addr: 0xffffff0000139000,
[ INFO]: src/page_table.rs@057:         physical_addr: 0xffffff0000139000,
[ INFO]: src/page_table.rs@057:         file_size: 0x0,
[ INFO]: src/page_table.rs@057:         mem_size: 0x1222a70,
[ INFO]: src/page_table.rs@057:         align: 0x1000,
[ INFO]: src/page_table.rs@057:     },
[ INFO]: src/page_table.rs@057: )
[ INFO]: src/page_table.rs@063: start_pa=0x3cf4e000,start_va=0xffffff0000139000
```

可以看到偏移量其实是：

```
>>> hex(0xffffff0000139000-0x3cf4e000)
'0xfffffeffc31eb000'
```

如果我们对于缓冲区的地址转换使用这个偏移量进行计算：

```
>>> hex(0xffffff0001352600-0xfffffeffc31eb000)
'0x3e167600'
>>> hex(0x3cf4e000+0x1222a70)
'0x3e170a70'
```

显然是正确的物理地址且在.bss段上。对于栈的话则不能使用这种方法，因为并不是一个线性映射。到这里我们终于知道了错误的原因：即`virtio_virt_to_phys`应该改成手动查页表（这样才能同时适用堆和栈）！错误的根本原因：传入的虚拟地址并不在0xFFFFFF8000000000开头的区间上，因此，直接减去这个偏移量明显是错误的。具体来说，如果是内核栈，开头地址是0xFFFFFF0100000000；如果是堆的话，会使用内核的起始虚拟地址即0xFFFFFF0000000000。

修改方法的话，初步考虑是在kernel-hal里面修改...尝试在`hal_fn::mem`里面新增一个方法。

由于zCore里面的基础设施已经比较完善终于是实现好了，现在我们读到了第一个block！然而并不是一个合法的SFS，看了一眼发现是对于x86_64平台上来说，disk.cow2根本是随便生成的...我们还需要进行转换才行。

这就需要我们追溯一下文件系统镜像生成的过程。x86_64上原本是怎么个玩法呢？在Makefile里面能够找到一个`user_img`，实际生成位置应该是在`EFI/zCore/x86_64.img`中。它的生成者应该是zCore而非zCore/zCore。于是我们直接将这个x86_64.img以raw的方式接入到zCore中。

然后看起来文件系统识别正常了...又出了一个新的错误：

```
panic cpu=1


panicked at 'called `Option::unwrap()` on a `None` value', zCore/src/handler.rs:22:60
[ 13.866566 ERROR 1 0:0 zcore::lang]
```

core1 panic了！我感觉应该是目前是在应用的地址空间，尽管内核的各数据段映射了，但是PCI所需的BAR0没有映射到。看log如下：

```
#之前一次成功的读入
[ 13.836543 INFO  0 0:0 virtio_drivers::pci::blk] after notifying
[ 13.836928 INFO  0 0:0 virtio_drivers::pci::blk] poped!
[ 13.837229 INFO  0 0:0 virtio_drivers::pci::blk] reading block 0xffd
[ 13.837611 INFO  0 0:0 virtio_drivers::pci::blk] before adding
[ 13.838021 INFO  0 0:0 virtio_drivers::queue] set_buf va=0xffffff01001fe890, pa=0x37dc3890 len=0x10
[ 13.838520 INFO  0 0:0 virtio_drivers::queue] set_buf va=0xffffff000135b400, pa=0x37fc4400 len=0x200
[ 13.839002 INFO  0 0:0 virtio_drivers::queue] set_buf va=0xffffff01001fe7d8, pa=0x37dc37d8 len=0x1
[ 13.839448 INFO  0 0:0 virtio_drivers::pci::blk] before notifying
[ 13.839773 INFO  0 0:0 virtio_drivers::pci::header] queue_notify_off=0x0,notify_off_multiplier=0x4
#出现错误的读入
[ 13.862102 INFO  1 0:0 linux_syscall::file::fd] openat: dir_fd=FileDesc(-100), path="/bin/busybox", flags=RDONLY, mode=0o0
[ 13.864045 INFO  1 0:0 virtio_drivers::pci::blk] reading block 0x50
[ 13.864308 INFO  1 0:0 virtio_drivers::pci::blk] before adding
[ 13.864676 INFO  1 0:0 virtio_drivers::queue] set_buf va=0xffffff01001fa490, pa=0x37dc7490 len=0x10
[ 13.864958 INFO  1 0:0 virtio_drivers::queue] set_buf va=0xffffff000135b600, pa=0x37fc4600 len=0x200
[ 13.865274 INFO  1 0:0 virtio_drivers::queue] set_buf va=0xffffff01001fa3d8, pa=0x37dc73d8 len=0x1
[ 13.865601 INFO  1 0:0 virtio_drivers::pci::blk] before notifying
panic cpu=1
panicked at 'called `Option::unwrap()` on a `None` value', zCore/src/handler.rs:22:60
```

感觉应该是死在该函数的第一行：

```rust
fn queue_notify_address(&self) -> usize {
    let queue_notify_off = self.common_cfg.queue_notify_off.read() as usize;
    // self.notify_cap_addr includes bar.base_addr + cap.offset in 4.1.4.4
    info!("queue_notify_off={:#x},notify_off_multiplier={:#x}", queue_notify_off, self.notify_off_multiplier);
    self.notify_cap_addr + queue_notify_off * self.notify_off_multiplier as usize
}
```

且等我先把之前过多log删掉一些。

然后，这个common_cfg又是在什么地方？根据之前的记录应该是在0x8_0000_0000也就是32G高一点的地方。理论上来讲，应用空间应该映射它的。那让我们来找找应用空间目前是如何映射的。

创建一个进程的过程：

```rust
# 首先是在loader里面
let job = Job::root();
let proc = Process::create_linux(&job, rootfs.clone()).unwrap();
# 见linux-object，Process来自zircon-object，这里提供了一个拓展trait以及实现
/// Process extension for linux
pub trait ProcessExt {
    /// create Linux process
    fn create_linux(job: &Arc<Job>, rootfs: Arc<dyn FileSystem>) -> ZxResult<Arc<Self>>;
    /// get linux process
    fn linux(&self) -> &LinuxProcess;
    /// fork from current linux process
    fn fork_from(parent: &Arc<Self>, vfork: bool) -> ZxResult<Arc<Self>>;
}
impl ProcessExt for Process {
    fn create_linux(job: &Arc<Job>, rootfs: Arc<dyn FileSystem>) -> ZxResult<Arc<Self>> {
        let linux_proc = LinuxProcess::new(rootfs);
        Process::create_with_ext(job, "root", linux_proc)
    }
}
```

这里可以看到我们先创建一个LinuxProcess（来自linux-object，和Process不同），再将这个参数传给zircon-object中的Process的`create_with_ext`方法。

```rust
/// Create a new process with extension info.
pub fn create_with_ext(
    job: &Arc<Job>,
    name: &str,
    ext: impl Any + Send + Sync,
) -> ZxResult<Arc<Self>> {
    let proc = Arc::new(Process {
        base: KObjectBase::with_name(name),
        _counter: CountHelper::new(),
        job: job.clone(),
        policy: job.policy(),
        vmar: VmAddressRegion::new_root(),
        ext: Box::new(ext),
        exceptionate: Exceptionate::new(ExceptionChannelType::Process),
        debug_exceptionate: Exceptionate::new(ExceptionChannelType::Debugger),
        inner: Mutex::new(ProcessInner::default()),
    });
    job.add_process(proc.clone())?;
    Ok(proc)
}
```

这里可以看到虚拟内存这里，每个进程初始化了一个`VmAddressRegion::new_root`。那么`VmAddressRegion`是什么呢？

```rust
/// Virtual Memory Address Regions
pub struct VmAddressRegion {
    flags: VmarFlags,
    base: KObjectBase,
    _counter: CountHelper,
    addr: VirtAddr,
    size: usize,
    parent: Option<Arc<VmAddressRegion>>,
    page_table: Arc<Mutex<dyn GenericPageTable>>,
    /// If inner is None, this region is destroyed, all operations are invalid.
    inner: Mutex<Option<VmarInner>>,
}
/// The mutable part of `VmAddressRegion`.
#[derive(Default)]
struct VmarInner {
    children: Vec<Arc<VmAddressRegion>>,
    mappings: Vec<Arc<VmMapping>>,
}
/// Virtual Memory Mapping
pub struct VmMapping {
    /// The permission limitation of the vmar
    permissions: MMUFlags,
    vmo: Arc<VmObject>,
    page_table: Arc<Mutex<dyn GenericPageTable>>,
    inner: Mutex<VmMappingInner>,
}
#[derive(Debug, Clone)]
struct VmMappingInner {
    /// The actual flags used in the mapping of each page
    flags: Vec<MMUFlags>,
    addr: VirtAddr,
    size: usize,
    vmo_offset: usize,
}
/// Virtual memory containers
///
/// ## SYNOPSIS
///
/// A Virtual Memory Object (VMO) represents a contiguous region of virtual memory
/// that may be mapped into multiple address spaces.
pub struct VmObject {
    base: KObjectBase,
    _counter: CountHelper,
    resizable: bool,
    trait_: Arc<dyn VMObjectTrait>,
    inner: Mutex<VmObjectInner>,
}
#[derive(Default)]
struct VmObjectInner {
    parent: Weak<VmObject>,
    children: Vec<Weak<VmObject>>,
    mapping_count: usize,
    content_size: usize,
}
```

看起来是x86_64上clone页表的时候只clone了一部分：

```rust
fn pt_clone_kernel_space(dst_pt_root: PhysAddr, src_pt_root: PhysAddr) {
    let entry_range = 0x100..0x200; // 0xFFFF_8000_0000_0000 .. 0xFFFF_FFFF_FFFF_FFFF
    let dst_table = unsafe { slice::from_raw_parts_mut(phys_to_virt(dst_pt_root) as *mut X86PTE, 512) };
    let src_table = unsafe { slice::from_raw_parts(phys_to_virt(src_pt_root) as *const X86PTE, 512) };
    for i in entry_range {
        dst_table[i] = src_table[i];
        if !dst_table[i].is_unused() {
            dst_table[i].0 |= PTF::GLOBAL.bits();
        }
    }
}
```

感觉页表应该是4级48位虚拟地址，那么一级页表项是39位也就是0x80_0000_0000

我觉得这个函数的用意在于不要复制用户态部分的页表（低的那一半），只复制内核部分（高的那一半），这样看起来可以更加快速的创建应用地址空间，然而PCI MMIO区域恰好在低的那一半（两个BAR分别是0x8_0000_0000和0xc108_5000，还有一个0x6000的不明觉厉）。所以我们可能需要手动映射低的那一半了。然而非常奇怪的一点是这些MMIO的恒等映射究竟是什么时候完成的？

## 29/07/22

简单打印了一下感觉是在rboot进行物理内存映射之前就映射好了...因此猜测可能是固件或者uefi运行时。

但现在这并非我们关心的重点，先来回顾下当前的内存布局：

物理内存采取线性映射，偏移量0xFFFFFF8000000000，以1G RAM为例，占据虚拟地址0xFFFFFF8000000000-0xFFFFFF8040000000

内核栈需要分配物理页帧进行映射，占据虚拟地址0xFFFFFF0100000000-?

内核ELF起始虚拟地址0xFFFFFF0000000000，首先是在rboot的load_file函数中调用uefi接口分配了一些连续的物理页面保存内核ELF，于是就可以把内核ELF线性映射过去。打印可以看到连续物理页面的起始地址是0x3ce0_4000，目前ELF大小应该是0x135_aa70，下一个可用页面应该是0x3e15_f000。于是姑且说是占据虚拟地址0xFFFFFF0000000000-0xFFFFFF000135AA70。

来打印一下传到zCore里面的memory_map是什么样子，结果是和rboot里面的mmap_iter看不出有什么不同，但实际肯定是有变化的吧...?

然后zCore获取可用内存区域的方法是：

```rust
pub fn free_pmem_regions() -> Vec<Range<PhysAddr>> {
    KCONFIG
        .memory_map
        .iter()
        .filter_map(|r| {
            if r.ty == MemoryType::CONVENTIONAL {
                let start = r.phys_start as usize;
                let end = start + r.page_count as usize * PAGE_SIZE;
                Some(start..end)
            } else {
                None
            }
        })
        .collect()
}
```

所以也许可以在pci初始化的时候给bar4做一个偏移量为0xFFFFFF8000000000的线性映射，这样就也能够落在kernel space里面了。

那么具体应该在什么地方做呢？

在pci初始化的那个地方，先循环一遍收集所有的mmio区域，完成映射；再循环一遍所有设备完成初始化。曲线救国了属于是。

这样做完之后，设备倒是初始化好了。文件系统也载入成功了。但是似乎是在重定位的时候，读某个块的时候忽然失败了，具体应该是common_cfg.queue_notify_off失败了。虽然这个是RO的也许可以提前拿出来，但是这个操作不应该失败啊...?目前看来稳定死在0xb58这个块。所以重定位搞了什么事情？是产生了虚拟地址冲突吗？？？

Intrepreter的行为参考linux-object/loader里面的LinuxElfLoader，然后我们多打印一点信息看看。感觉是在将动态链接库加载到内存的时候炸了。但是这中间应该不存在任何页表映射行为吧。

```rust
if let Ok(interp) = elf.get_interpreter() {
    info!("interp: {:?}, path: {:?}", interp, path);
    let inode = self.root_inode.lookup(interp)?;
    info!("p0");
    let data = inode.read_as_vec()?;
    info!("p1");
    let mut new_args = vec![interp.into(), path.clone()];
    new_args.extend_from_slice(&args[1..]);
    return self.load(vmar, &data, new_args, envs, path);
}
```

目前是p0成功完成。在读取`read_as_vec`的第一个块的时候爆炸。

## 30/07/22

所以说在读取`read_as_vec`的时候发生了什么？我感觉应该和文件系统的实现关系不大。看了一下中间并没有切换页表。

把之前需要查BAR的一个值硬编码了，这样多读了几个块之后GG。报错是unwrap，所以感觉上应该是触发pagefault了？

```
[  4.677516 INFO  0 0:0 zcore::handler] into handle_page_fault, fault_vaddr=0xffff800800003000, access_flags=WRITE
```

看起来确实是触发了page fault...在handler里面试图拿当前的线程，就炸掉了。整个过程中没有出现页表切换，所以页表是什么时候被修改了？我忽然有点怀疑可能是保存页表的物理页帧被分配出去存数据了，也就是说当时页表映射调用的接口可能不对。

在kernel_hal/x86_64/drivers.rs中，我们调用`PageTable::map_cont`接口，该接口来自kernel_hal/common/vm.rs。

感觉创建一个临时页表来映射不太对，因为：

```rust
pub struct PageTableImpl<L: PageTableLevel, PTE: GenericPTE> {
    /// Root table frame.
    root: PhysFrame,
    /// Intermediate level table frames.
    intrm_tables: Vec<PhysFrame>,
    /// Phantom data.
    _phantom: PhantomData<(L, PTE)>,
}
```

用来存储页表的物理页面放在`inirm_tables`里面，当这个页表生命周期结束之后这些物理页面会回到物理页帧分配器中。一种方法可能是`Box::leak`或者`mem::forget`，但是感觉不太优雅。之前的映射是怎么弄的呢？

感觉`mem::forget`算是个不错的解决方案了。搞完之后好像跑起来惹！但是从membuf换成virtio之后好像巨慢无比...目前的话，不开kvm耗时大概在10s左右。比对了一下之前的记录，不使用disk的话，时钟中断频率仍是4000Hz，关掉kvm耗时1.8s左右，打开kvm耗时3.5s左右。按照Lluis的说法应该是QEMU的参数问题。

先整理一下代码然后提交吧......然后先休息一下嘿嘿嘿。

---

下面看Intel了解一些基础知识。

64位虚拟地址，有效的只有48位，那么第63-48位需要和第48位相同，不然会触发page fault。这个叫做canonical addressing。不知道为什么用这个词...比方说，可能触发#GP或者#SS（分别表示general-protection exception和stack fault）。可能触发#SS的操作有PUSH/POP相关指令或者把RSP/RBP作为基地址的指令。后面还提到了canonical fault的一些其他情形，不过感觉用不上。

