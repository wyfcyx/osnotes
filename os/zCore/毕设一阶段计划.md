## 21/06/22

先开个头，过会完善。

思路就是先分出一个核S负责执行所有的系统调用。其他核一旦通过系统调用陷入到内核需要将系统调用请求提交给核S，等到核S处理完毕之后再唤醒来源核上对应的task。这就是所谓第一阶段的计划。

将目前的实现过程记录在这里。

首先，要完整运行的话，只需在zCore根目录下`make roofs && make libc-test && make other-test && make image`，随后在zCore/zCore目录下`make run LINUX=1 SMP=2 LOG=warn MODE=release`即可。

根据此前的分析，我们希望将`loader/linux.rs`中`handle_user_trap`函数中的`syscall.syscall.await`执行过程移动到hart 1上面。然后其余的地方保持不变。应当如何实现呢？首先应该新创建一种Future（姑且称为`RemoteSyscallFuture`），这种Future会将`syscall.syscall`包裹起来，当poll该Future的时候，将以某种方式将syscall请求提交到远程syscall模块，随后返回`Poll::Pending`，这样才能在hart 0的运行时上面暂停该任务。那么如何将其唤醒呢？远程syscall模块自己的运行时将以poll的方式执行syscall，在执行完毕之后应该以某种方式唤醒hart 0上的任务。

那么，我们可能需要研究一下现有的Waker机制。看了一下已有的叶子Future的实现，其中一个比较典型的例子是`YieldFuture`，是在我们调用`yield_now`的时候用到的，而`yield_now`用于在裸机模式下实现基于时间片的抢占式调度。`YieldFuture`的实现是内部保存一个flag，首次poll的时候flag为false，此时将flag设置为true，调用`cx.waker().wake_by_ref()`，然后返回`Poll::Pending`；第二次poll的时候发现内部flag为true则直接返回`Poll::Ready`。如果想知道这里面的Waker做了什么事情，就需要找到顶层Future管理器以及对应生成的Waker本身。我能想到的一个比较高层的Future是`run_user`生成的，不过这个大概还不是顶层Future。这个Future还会被`ThreadSwitchFuture`包裹起来（详见zircon_object里面的thread.rs）并提交给kernel_hal。于是这个`ThreadSwitchFuture`又是什么呢？它只是会poll里层Future，但是在poll之前先切换到对应进程的地址空间。那么我们发现在这里是找不到对应的Waker的。结论：`ThreadSwitchFuture`应该就是顶层Future。

那么看起来Waker的实现还是要在executor库如PreemptiveScheduler中寻找了。看了一下实现比较复杂，但是总体上来说，在每个task被spawned到运行时中的时候我们会为其生成一个对应的Waker用来唤醒它，内部应该是使用到了某种索引。同时，事件循环的逻辑是，每次会取出（也即take）一个task，然后尝试poll这个task。如果结果是Ready的话则销毁对应的Waker，结果是Pending的话则什么都不做，因为task已经被移除了。应该是当我们调用`cx.waker().wake_by_ref()`的时候task才会被重新加入到运行时中。重新思考一下`YieldFuture`，第一次poll的时候，注意在poll之前task已经被移除了，所以这个时候要重新将其加入到运行时中，相当于从队头移动到队尾，是经典的Round Robin操作。这样就有了第二次被poll的机会，这次的话直接返回Ready即可让task继续向下运行。

总结一下，如果使用的是rcore-os/executor或者PreemptiveScheduler这些运行时的话，需要满足的要求是在顶层Future被poll之前它们会被移除。同时需要在适当的时机调用Waker将顶层Future重新放置到就绪队列中。不过，目前我们希望自己重新实现一套运行时，就不必遵循这里的规则了。但是这可以作为参考。这里可以提炼出一条重要的规则：Waker应该是由运行时负责生成和管理的，这也跟之前async_modules中的经验一致了。

还有一些问题。

第一个问题：我们是否要为其实现自己的调度器，还是沿用已有的PreemptiveScheduler？

> 目前来看，为这种模块实现一个模块管理器并在里面自己实现一个event loop，也就是可以参考之前的async_modules看上去是比较好的。这样就不用去计较PreemptiveScheduler的种种细节了，同时也更加灵活。

第二个问题：SubmissionQueue和CompletionQueue是否还需要？

> 如果照搬之前的async_modules的话，自然还是需要的。

然后就是考虑系统调用本身的实现是同步还是异步的。同步的自然比较简单。对于异步的，假设其与I/O有关，且我们不打开hart 1的中断使能，那么也就有这么一种复杂的情况：在hart 0接收到外设中断之后，需要以某种方式通知hart 1该外设上的数据已经准备好了，从而唤醒hart 1运行时上的syscall任务。如果实现不好的话，这一步可能涉及到大量的数据拷贝。另外，实际情况是否真的有这么复杂，还需要一些代码阅读（比如，可能是一些软件上的异步事件）。

现在总体上考虑一下第一阶段的设计吧。

`RemoteSyscallFuture`是对`syscall.syscall`的封装，确切来说，是`syscall.syscall(num as u32, args)`这个Future的一个wrapper。注意，这个Future的返回值是一个isize。这个Future会比较奇怪，因为它是hart 0和hart 1上两套运行时之间沟通的桥梁。首先，它是hart 0运行时体系中的一个叶子Future。当我们在poll这个Future的时候，`cx.waker().wake_by_ref()`可以用来唤醒hart 0中的顶层任务。唤醒的时候自然是hart 1上的一个系统调用已经执行完毕了，hart1应该用怎样的方式通知hart 0？所以一种不错的思路可能是将`cx.waker().wake_by_ref()`包在一个闭包里面由hart 1直接去调用。当然`RemoteSyscallFuture`内部的记录也需要在某个时候更新，因为它还需要正确的返回一个isize。

因此，当第一次poll `RemoteSyscallFuture`的时候，我们需要将`syscall.syscall`这个Future、唤醒当前task的`cx.waker().wake_by_ref()`的闭包（以直接拷贝或引用的形式，比如说不是Future本体而是所有用来生成该Future所需的信息）、还有一个指向`RemoteSyscallFuture`内部的`*mut isize`（这个是用来保存syscall返回值）提交到hart 1，然后就可以返回`Poll::Pending`。这将使当前task在远程syscall返回之前不会被放回到hart 0运行时中。当hart 1完成了远程syscall之后，将返回值写回到`RemoteSyscallFuture`，然后调用传进来的闭包唤醒hart 0上的task。过了一段时间之后，hart 0上的`RemoteSyscallFuture`将会第二次被poll到，此时我们直接返回`Poll::Ready`，带上`RemoteSyscallFuture`内部的syscall返回值即可。

这样来看的话，我们的AsyncModule固然需要Submission Queue提供的filter/reordering等能力，Completion Queue可能是真的不需要了。因为如果一个模块要调用其他模块的话，它可以向其他模块在提交任务的时候带上一个闭包来允许对方唤醒自己的某个任务，这样的实现可能会比较方便？另一方面，从局部性上来说也不一定会劣于Completion Queue的实现。

如果只限定于`RemoteSyscallFuture`，AsyncModule的实现？由于没有调用到其他的模块，至少是不需要考虑Waker的。然后显然需要有一个无锁的Submission Queue方便hart 0来提交任务。自身的事件循环的话，就是从Submission Queue取出hart 0的请求来填充自身的任务队列，然后poll已有（在hart 0上生成的）或是在hart 1上原地生成的Future。这里最大的问题是Reactor的实际应用，也即哪些事件能让中间暂停的syscall继续执行，这就需要调研已有的异步syscall之所以为异步的原因。某些事件若想完成跨核通信的话，也许并不容易。

如何使用宏来降低实现复杂性？

同时将一些可能的困难记录一下。

第一点：`Syscall`里面有一个`CurrentThread`的引用，回顾一下知道`CurrentThread`只不过是`Arc<Thread>`的一层封装。也就是说这可能会为我们将其移动到另外一个核上去执行带来一些本不必要的困难。我们可以考虑直接将`Arc<Thread>`移动到另一个核上，然后再顺势生成`Syscall`实例来有效利用已有的API。那么zCore里面又为何是这样设计的呢？`CurrentThread`自身是被传入`run_user`闭包中的，然后`run_user`里面就一直以引用的形式访问它。不对它进行clone，是出于性能考虑，还是必须保持引用计数不变？目前还不清楚。不过，我觉得如果将`CurrentThread`复制一份传到另一个核上应当没有什么问题。

第二点：zCore并没有采用KPTI的设计。因此，在hart 1上，每次在执行一个syscall之前都需要先切换到线程所在进程的地址空间。在hart 0上这是由顶层Future `ThreadSwitchFuture`保证的，在hart 1上就没有了。所以我们后续需要将zCore改成KPTI的设计吗？从性能和实现复杂性两方面考虑一下？

第三点：网卡驱动和相关的测例（还有其他测例）需要调研一下。目前如果只考虑Linux裸机上的测例的话，注意`zCore/tests`目录，大概可以分成一大堆libc测试（进而分成functional/math/musl/regression几类，其中regression类别中包括pthread/printf相关测例，这些测例的源码可以在`zCore/libc-test`目录下找到）、与busybox有关的测试，还有就是`tests/src`目录下的socket（包括tcp/udp）和串口相关的测试。但是`tests/src`这里的测试还没有跑起来，具体的环境配置方式还需要再了解一下。最终生成的rootfs可以在`zCore/rootfs/x86_64`目录下找到，`x86_64/bin`里面除了busybox还有一些test开头的测例，比如`testpipe1`等等。这些测例的源码可以在`linux-syscall/test`目录下找到。于是测例的位置我们大致上找到了。

接下来是研究一下网卡驱动是否真的是异步的。zCore这里对于设备的抽象相当复杂，可以说是跟rCore一脉相承了。想当初最早接触rCore的时候也是这里最让我头大。我们从系统调用入手吧，可以看到sys_socket是同步的，而sys_connect和sys_accept都是异步的。例如sys_connect，最终是调用了`Socket` Trait的`connect().await`。这个`Socket` Trait可以在linux-object中找到，看起来目前应该是为`TcpSocketState`和`UdpSocketState`实现了这个Trait。比如`TcpSocketState`...等等，这里看起来好像是个悲伤的故事，`TcpSocketState`的connect/accept被声明为async fn，但实际上都是基于轮询的同步实现。那么有非轮询式访问的设备吗？据说只有串口和键盘了？

那么，无论网卡驱动是否支持中断处理，至少zCore里面是完全轮询的，这个答案已经找到了。

第四点：为何某些系统调用是异步的需要调研一下。在进行这一步之前首先需要当心的是：某些async fn的实现却可能是同步的！首先找出所有可能异步的syscall，在linux下有这些：read/connect/accept/recvfrom/recvmsg/wait4/futex/nanosleep/clock_nanosleep/semop/poll/select/vfork。看起来颇为怪异的一点是与read对应的write为什么不是异步的。

再稍微分一下类吧：

第一类：read。抛去socket不看，另一类是`FileLike` trait的`.read().await`。这部分有点复杂，先不想看了。

第二类：与网络相关的connect/accept/recvfrom/recvmsg，从已知的信息来看实际上是同步实现

第三类：与任务相关：wait4/vfork。比如wait4，可以追溯到linux-object的Process的wait_child和wait_child_any，最终调用`child.wait_signal(Signal::PROCESS_TERMINATED).await`，这个`wait_signal`则是来自于zircon-object，是一个比较典型的软件事件机制：

```rust
/// Asynchronous wait for one of `signal`.
pub fn wait_signal(self: &Arc<Self>, signal: Signal) -> impl Future<Output = Signal> {
    #[must_use = "wait_signal does nothing unless polled/`await`-ed"]
    struct SignalFuture {
        object: Arc<dyn KernelObject>,
        signal: Signal,
        first: bool,
    }

    impl Future for SignalFuture {
        type Output = Signal;

        fn poll(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {
            let current_signal = self.object.signal();
            if !(current_signal & self.signal).is_empty() {
                return Poll::Ready(current_signal);
            }
            if self.first {
                self.object.add_signal_callback(Box::new({
                    let signal = self.signal;
                    let waker = cx.waker().clone();
                    move |s| {
                        if (s & signal).is_empty() {
                            return false;
                        }
                        waker.wake_by_ref();
                        true
                    }
                }));
                self.first = false;
            }
            Poll::Pending
        }
    }

    SignalFuture {
        object: self.clone(),
        signal,
        first: true,
    }
}
```

vfork的await同样来源于wait_signal。

第四类：与时间相关：nanosleep，这个最终可以追溯到kernel-hal中的SleepFuture。

第五类：与同步相关：futex/semop，实现比较复杂，但同样是一些软件事件。

第六类：与I/O复用相关：poll/select，这两个原理差不多，应该都是调用FileLike的`async_poll`接口来检查该文件描述符的状态是否变更。这里又会有一些叶子Future，硬件/软件事件均有可能。

看完这一轮代码的话可以说对zCore的了解又上了一个层次。（雾

