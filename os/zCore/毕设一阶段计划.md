## 21/06/22

先开个头，过会完善。

思路就是先分出一个核S负责执行所有的系统调用。其他核一旦通过系统调用陷入到内核需要将系统调用请求提交给核S，等到核S处理完毕之后再唤醒来源核上对应的task。这就是所谓第一阶段的计划。

将目前的实现过程记录在这里。

首先，要完整运行的话，只需在zCore根目录下`make rootfs && make libc-test && make other-test && make image`，随后在zCore/zCore目录下`make run LINUX=1 SMP=2 LOG=warn MODE=release ACCEL=1`即可。

根据此前的分析，我们希望将`loader/linux.rs`中`handle_user_trap`函数中的`syscall.syscall.await`执行过程移动到hart 1上面。然后其余的地方保持不变。应当如何实现呢？首先应该新创建一种Future（姑且称为`RemoteSyscallFuture`），这种Future会将`syscall.syscall`包裹起来，当poll该Future的时候，将以某种方式将syscall请求提交到远程syscall模块，随后返回`Poll::Pending`，这样才能在hart 0的运行时上面暂停该任务。那么如何将其唤醒呢？远程syscall模块自己的运行时将以poll的方式执行syscall，在执行完毕之后应该以某种方式唤醒hart 0上的任务。

那么，我们可能需要研究一下现有的Waker机制。看了一下已有的叶子Future的实现，其中一个比较典型的例子是`YieldFuture`，是在我们调用`yield_now`的时候用到的，而`yield_now`用于在裸机模式下实现基于时间片的抢占式调度。`YieldFuture`的实现是内部保存一个flag，首次poll的时候flag为false，此时将flag设置为true，调用`cx.waker().wake_by_ref()`，然后返回`Poll::Pending`；第二次poll的时候发现内部flag为true则直接返回`Poll::Ready`。如果想知道这里面的Waker做了什么事情，就需要找到顶层Future管理器以及对应生成的Waker本身。我能想到的一个比较高层的Future是`run_user`生成的，不过这个大概还不是顶层Future。这个Future还会被`ThreadSwitchFuture`包裹起来（详见zircon_object里面的thread.rs）并提交给kernel_hal。于是这个`ThreadSwitchFuture`又是什么呢？它只是会poll里层Future，但是在poll之前先切换到对应进程的地址空间。那么我们发现在这里是找不到对应的Waker的。结论：`ThreadSwitchFuture`应该就是顶层Future。

那么看起来Waker的实现还是要在executor库如PreemptiveScheduler中寻找了。看了一下实现比较复杂，但是总体上来说，在每个task被spawned到运行时中的时候我们会为其生成一个对应的Waker用来唤醒它，内部应该是使用到了某种索引。同时，事件循环的逻辑是，每次会取出（也即take）一个task，然后尝试poll这个task。如果结果是Ready的话则销毁对应的Waker，结果是Pending的话则什么都不做，因为task已经被移除了。应该是当我们调用`cx.waker().wake_by_ref()`的时候task才会被重新加入到运行时中。重新思考一下`YieldFuture`，第一次poll的时候，注意在poll之前task已经被移除了，所以这个时候要重新将其加入到运行时中，相当于从队头移动到队尾，是经典的Round Robin操作。这样就有了第二次被poll的机会，这次的话直接返回Ready即可让task继续向下运行。

总结一下，如果使用的是rcore-os/executor或者PreemptiveScheduler这些运行时的话，需要满足的要求是在顶层Future被poll之前它们会被移除。同时需要在适当的时机调用Waker将顶层Future重新放置到就绪队列中。不过，目前我们希望自己重新实现一套运行时，就不必遵循这里的规则了。但是这可以作为参考。这里可以提炼出一条重要的规则：Waker应该是由运行时负责生成和管理的，这也跟之前async_modules中的经验一致了。

还有一些问题。

第一个问题：我们是否要为其实现自己的调度器，还是沿用已有的PreemptiveScheduler？

> 目前来看，为这种模块实现一个模块管理器并在里面自己实现一个event loop，也就是可以参考之前的async_modules看上去是比较好的。这样就不用去计较PreemptiveScheduler的种种细节了，同时也更加灵活。

第二个问题：SubmissionQueue和CompletionQueue是否还需要？

> 如果照搬之前的async_modules的话，自然还是需要的。

然后就是考虑系统调用本身的实现是同步还是异步的。同步的自然比较简单。对于异步的，假设其与I/O有关，且我们不打开hart 1的中断使能，那么也就有这么一种复杂的情况：在hart 0接收到外设中断之后，需要以某种方式通知hart 1该外设上的数据已经准备好了，从而唤醒hart 1运行时上的syscall任务。如果实现不好的话，这一步可能涉及到大量的数据拷贝。另外，实际情况是否真的有这么复杂，还需要一些代码阅读（比如，可能是一些软件上的异步事件）。

现在总体上考虑一下第一阶段的设计吧。

`RemoteSyscallFuture`是对`syscall.syscall`的封装，确切来说，是`syscall.syscall(num as u32, args)`这个Future的一个wrapper。注意，这个Future的返回值是一个isize。这个Future会比较奇怪，因为它是hart 0和hart 1上两套运行时之间沟通的桥梁。首先，它是hart 0运行时体系中的一个叶子Future。当我们在poll这个Future的时候，`cx.waker().wake_by_ref()`可以用来唤醒hart 0中的顶层任务。唤醒的时候自然是hart 1上的一个系统调用已经执行完毕了，hart1应该用怎样的方式通知hart 0？所以一种不错的思路可能是将`cx.waker().wake_by_ref()`包在一个闭包里面由hart 1直接去调用。当然`RemoteSyscallFuture`内部的记录也需要在某个时候更新，因为它还需要正确的返回一个isize。

因此，当第一次poll `RemoteSyscallFuture`的时候，我们需要将`syscall.syscall`这个Future、唤醒当前task的`cx.waker().wake_by_ref()`的闭包（以直接拷贝或引用的形式，比如说不是Future本体而是所有用来生成该Future所需的信息）、还有一个指向`RemoteSyscallFuture`内部的`*mut isize`（这个是用来保存syscall返回值）提交到hart 1，然后就可以返回`Poll::Pending`。这将使当前task在远程syscall返回之前不会被放回到hart 0运行时中。当hart 1完成了远程syscall之后，将返回值写回到`RemoteSyscallFuture`，然后调用传进来的闭包唤醒hart 0上的task。过了一段时间之后，hart 0上的`RemoteSyscallFuture`将会第二次被poll到，此时我们直接返回`Poll::Ready`，带上`RemoteSyscallFuture`内部的syscall返回值即可。

这样来看的话，我们的AsyncModule固然需要Submission Queue提供的filter/reordering等能力，Completion Queue可能是真的不需要了。因为如果一个模块要调用其他模块的话，它可以向其他模块在提交任务的时候带上一个闭包来允许对方唤醒自己的某个任务，这样的实现可能会比较方便？另一方面，从局部性上来说也不一定会劣于Completion Queue的实现。

如果只限定于`RemoteSyscallFuture`，AsyncModule的实现？由于没有调用到其他的模块，至少是不需要考虑Waker的。然后显然需要有一个无锁的Submission Queue方便hart 0来提交任务。自身的事件循环的话，就是从Submission Queue取出hart 0的请求来填充自身的任务队列，然后poll已有（在hart 0上生成的）或是在hart 1上原地生成的Future。这里最大的问题是Reactor的实际应用，也即哪些事件能让中间暂停的syscall继续执行，这就需要调研已有的异步syscall之所以为异步的原因。某些事件若想完成跨核通信的话，也许并不容易。

如何使用宏来降低实现复杂性？

同时将一些可能的困难记录一下。

第一点：`Syscall`里面有一个`CurrentThread`的引用，回顾一下知道`CurrentThread`只不过是`Arc<Thread>`的一层封装。也就是说这可能会为我们将其移动到另外一个核上去执行带来一些本不必要的困难。我们可以考虑直接将`Arc<Thread>`移动到另一个核上，然后再顺势生成`Syscall`实例来有效利用已有的API。那么zCore里面又为何是这样设计的呢？`CurrentThread`自身是被传入`run_user`闭包中的，然后`run_user`里面就一直以引用的形式访问它。不对它进行clone，是出于性能考虑，还是必须保持引用计数不变？目前还不清楚。不过，我觉得如果将`CurrentThread`复制一份传到另一个核上应当没有什么问题。

第二点：zCore并没有采用KPTI的设计。因此，在hart 1上，每次在执行一个syscall之前都需要先切换到线程所在进程的地址空间。在hart 0上这是由顶层Future `ThreadSwitchFuture`保证的，在hart 1上就没有了。所以我们后续需要将zCore改成KPTI的设计吗？从性能和实现复杂性两方面考虑一下？

第三点：网卡驱动和相关的测例（还有其他测例）需要调研一下。目前如果只考虑Linux裸机上的测例的话，注意`zCore/tests`目录，大概可以分成一大堆libc测试（进而分成functional/math/musl/regression几类，其中regression类别中包括pthread/printf相关测例，这些测例的源码可以在`zCore/libc-test`目录下找到）、与busybox有关的测试，还有就是`tests/src`目录下的socket（包括tcp/udp）和串口相关的测试。但是`tests/src`这里的测试还没有跑起来，具体的环境配置方式还需要再了解一下。最终生成的rootfs可以在`zCore/rootfs/x86_64`目录下找到，`x86_64/bin`里面除了busybox还有一些test开头的测例，比如`testpipe1`等等。这些测例的源码可以在`linux-syscall/test`目录下找到。于是测例的位置我们大致上找到了。

接下来是研究一下网卡驱动是否真的是异步的。zCore这里对于设备的抽象相当复杂，可以说是跟rCore一脉相承了。想当初最早接触rCore的时候也是这里最让我头大。我们从系统调用入手吧，可以看到sys_socket是同步的，而sys_connect和sys_accept都是异步的。例如sys_connect，最终是调用了`Socket` Trait的`connect().await`。这个`Socket` Trait可以在linux-object中找到，看起来目前应该是为`TcpSocketState`和`UdpSocketState`实现了这个Trait。比如`TcpSocketState`...等等，这里看起来好像是个悲伤的故事，`TcpSocketState`的connect/accept被声明为async fn，但实际上都是基于轮询的同步实现。那么有非轮询式访问的设备吗？据说只有串口和键盘了？

那么，无论网卡驱动是否支持中断处理，至少zCore里面是完全轮询的，这个答案已经找到了。

第四点：为何某些系统调用是异步的需要调研一下。在进行这一步之前首先需要当心的是：某些async fn的实现却可能是同步的！首先找出所有可能异步的syscall，在linux下有这些：read/connect/accept/recvfrom/recvmsg/wait4/futex/nanosleep/clock_nanosleep/semop/poll/select/vfork。看起来颇为怪异的一点是与read对应的write为什么不是异步的。

再稍微分一下类吧：

第一类：read。抛去socket不看，另一类是`FileLike` trait的`.read().await`。这部分有点复杂，先不想看了。

第二类：与网络相关的connect/accept/recvfrom/recvmsg，从已知的信息来看实际上是同步实现

第三类：与任务相关：wait4/vfork。比如wait4，可以追溯到linux-object的Process的wait_child和wait_child_any，最终调用`child.wait_signal(Signal::PROCESS_TERMINATED).await`，这个`wait_signal`则是来自于zircon-object，是一个比较典型的软件事件机制：

```rust
/// Asynchronous wait for one of `signal`.
pub fn wait_signal(self: &Arc<Self>, signal: Signal) -> impl Future<Output = Signal> {
    #[must_use = "wait_signal does nothing unless polled/`await`-ed"]
    struct SignalFuture {
        object: Arc<dyn KernelObject>,
        signal: Signal,
        first: bool,
    }

    impl Future for SignalFuture {
        type Output = Signal;

        fn poll(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Self::Output> {
            let current_signal = self.object.signal();
            if !(current_signal & self.signal).is_empty() {
                return Poll::Ready(current_signal);
            }
            if self.first {
                self.object.add_signal_callback(Box::new({
                    let signal = self.signal;
                    let waker = cx.waker().clone();
                    move |s| {
                        if (s & signal).is_empty() {
                            return false;
                        }
                        waker.wake_by_ref();
                        true
                    }
                }));
                self.first = false;
            }
            Poll::Pending
        }
    }

    SignalFuture {
        object: self.clone(),
        signal,
        first: true,
    }
}
```

vfork的await同样来源于wait_signal。

第四类：与时间相关：nanosleep，这个最终可以追溯到kernel-hal中的SleepFuture。

第五类：与同步相关：futex/semop，实现比较复杂，但同样是一些软件事件。

第六类：与I/O复用相关：poll/select，这两个原理差不多，应该都是调用FileLike的`async_poll`接口来检查该文件描述符的状态是否变更。这里又会有一些叶子Future，硬件/软件事件均有可能。

看完这一轮代码的话可以说对zCore的了解又上了一个层次。（雾

## 28/06/22

初步实现了一下，觉得可能最大的困难是：现在core 0和core 1是两套runtime了，那么各种异步信号机制还能正常工作吗？当core 1上的一个syscall被挂起之后，应该在何时、以怎样的方式来唤醒它。把syscall的执行移动到core 1上面会在这种意义上带来影响吗？

想了一下，应该不会有什么问题。但前提是core 1的runtime需要有一套自己的waker，对应到那些顶层syscall Future，在poll这些syscall Future的时候将这些waker传递下去，并在叶子Future返回pending的时候将这些waker注册到对应的reactor的位置上。这样，当一个事件触发之后，原来是唤醒core 0上的一个task，现在则是唤醒core 1上的一个syscall的执行。当core 1上的一个syscall执行完毕之后，进而才会去唤醒core 0上对应的task。

如果一个runtime需要支持从其他地方唤醒它里面的Future，就需要自己实现一套waker了。这个实现还是挺复杂的，可以参考preemptive scheduler。如果想先跑起来的话，我们可以尝试先支持实现为同步的那些syscall，也就是暂时填充一个空的waker，然后断言单次poll的结果为ready。这一套waker系统可以说每个async模块都要用到，也许可以将其独立出来。

目前简单实现了一个不支持Waker的版本，也就是能够在另一个核上跑立即返回的syscall了。

## 01/07/22

现在我们开始研究一下PreemptiveScheduler里面waker的相关实现。或者可以试试通读一下这个库的代码。

发现zCore主仓库已经多出了60多次commit，那么先更新一下吧。

关于kvm：原版zCore的话是否启用kvm看起来都能正常运行，启用kvm的话运行速度确实会比较快一点。我的修改版的话启用kvm之后连hart1上的输出都看不到就直接卡死了。大概情况是hart0进入到`wait_for_exit`函数，打印完`executor run!`之后就卡死了。也看不到hart1上的任何输出。这个行为十分令人费解。不启用kvm的话能看到正常的panic信息，因为目前还不支持poll实际上异步的系统调用。那我们姑且先在关闭kvm的情况下跑通试试。顺带，尝试了一下更新之前的代码（用的是`spin::Mutex`而非更新之后的`kernel-sync`库），启用kvm之后也是会卡死的，证明不是这次更新的问题。然后应该也跟hart1的切换页表行为无关，在那之前就已经卡死了。log一下，发现hart0已经将请求加入到队列中了，主要问题是hart1自始至终没动静。目前搞不清楚什么情况，明天继续调试吧。

## 04/07/22

现在来通读一下PreemptiveScheduler这个库的代码。在主分支上的版本是e8cd353，好的。

### ExecutorRuntime

暴露到最外层的应该是ExecutorRuntime，注意目前它默认只会在单核上访问（也就是每个核有一个，但不考虑同步问题）。这样的话似乎不满足我们的需求？因为我们需要通过在hart1上调用`waker.wake_by_ref`来唤醒hart0上的task。我们且看一看后面会不会有什么影响。每个ExecutorRuntime包含一个当前的cpu_id，一个TaskCollection（没有使用互斥锁保护），一个strong_executor和一组weak_executors（类型型如`Arc<Pin<Box<Executor>>>`，其中Executor为我们比较熟知的执行器），一个current_executor，还有一个Context。这里的Context包括了ra，sp，被调用者保存寄存器还有页表基址。

`ExecutorRuntime::new`，初始化了任务管理器TaskCollection，同时一开始有一个strong_executor而没有任何weak_executors，将任务管理器的指针拷贝一份交给strong。`downgrade_strong_executor`就是将当前的strong_executor内部状态ExecutorState标记为WEAK，将其移动到weak_executors中，最后新建一个strong_executor。`add/remove_task`均是调用TaskCollection的方法。默认为ExecutorRuntime实现了Send和Sync两个trait。然而，GLOBAL_RUNTIME里面还是给每个ExecutorRuntime加上了`spin::Mutex`作为同步保护。

然后是对外暴露出的几个函数：`handle_timeout`，`run_until_idle`，`sched_yield`以及`spawn`，我们依次来看一下。目前`handle_timeout`实现并不完整，只是在关中断的前提下调用`sched_yield`。而`sched_yield`的话，首先根据cpu_id获取当前核上的ExecutorRuntime（也就是`get_current_runtime`），然后如果它的current_executor不是None的话，就从这个executor的Context切换到当前runtime的Context。这个应该和传统意义上的内核线程切换是一个意思。也就是说，当某个Executor正在运行的时候，通过调用`sched_yield`可以将CPU控制权交还给它所在的Runtime。然而我们知道每个Executor应该负责一组任务（一个TaskCollection），这意味着组内的其他任务应该暂时也没有机会被执行了。然后是Runtime的主循环`run_until_idle`，每次迭代我们先运行strong_executor。控制权交还给Runtime之后说明strong_executor主动yield或者其中的一个task超时了。对于主动yield的情况，调用`downgrade_strong_executor`将其移动到weak_executors并新建一个strong_executor。注意，目前看起来所有的Executor共享Runtime里面的TaskCollection，这样合理吗？接着去除掉一些被killed掉的weak_executors之后，按照顺序运行这些weak_executors，最后进入下一次迭代。`spawn`的话实际调用的是`spawn_task`函数，`spawn_task`可能支持不同的优先级和不同的核，不过目前`spawn`不考虑优先级，也只将任务创建在当前的核上。这里我觉得倒是没什么好说的。

### TaskCollection && WakerPage

接下来我们看一下我们比较关心的TaskCollection和WakerPage的实现吧。重点关注Waker是如何唤醒一个特定的任务的。

TaskCollection的单位自然是Task，其中包含一个id，一个`Mutex<Pin<Box<>>>`的Future本体，一个`Mutex<TaskInner>`，还有一个标记任务是否结束的AtomicBool。TaskInner则包括优先级、状态`TaskState`（可能为RUNNABLE和RUNNING两种，BLOCKED被注释掉了）、还有一个intr_enable。Task的id看起来是通过`alloc_id`函数进行全局分配的，任意两个Task的id应该都不同。通过`Task::new`进行初始化，可以看到状态为RUNNABLE，finish和intr_enable均为false。这个Task也实现了poll方法，也就是poll该Task里面的Future。不过有一些细节：在poll之前需要获取Future的锁并在poll期间持有；根据intr_enable还有开关中断等操作。其实intr_enable有些意义不明，这里我们先放着不管。

接着是一个叫做FutureCollection的结构，包括一个`Arc<Task>`的slab allocator，也就是`PinSlab<Arc<Task>>`；一个`pages: Vec<Arc<WakerPage>>`，也附带一个优先级。然后大概是使用Key（一种usize的封装）来进行某种索引，每个Key对应到一个Future，由insert方法返回给调用者（实际上这个Key是PinSlab库提供的，这里我们只是对其进行一些索引操作）。每个Key是由subpage_idx（6位），page_idx和priority组装而成。其中page_idx应该是一级索引，可以索引到一个`Arc<WakerPage>`；然后在这个WakerPage里面再使用二级索引subpage_idx找到一个Future。也就是说每个WakerPage应该保存了64个Future的Waker信息。这样我们可以insert或者remove一个Future了。

然后就是TaskCollection了。它的结构：

```rust
pub struct TaskCollection {
    cpu_id: u8, // Just for debug, not used
    future_collections: Vec<Mutex<FutureCollection>>,
    pub task_num: AtomicUsize,
    generator: Option<Mutex<Pin<Box<dyn Generator<Yield = Option<Key>, Return = ()>>>>>,
}
```

注意future_collections是一组FutureCollection，对于每个不同的priority都有一个，目前有MAX_PRIORITY=32种不同的优先级。

使用`new`进行初始化，主要是对于每种priority插入一个FutureCollection，值得注意的是generator的初始化。可以看到generator是每个yield一个Key出来。这个generator整体上锁，实现可以在`TaskCollection::generator`找到。这个函数比较长，稍微读一下：变量`priority`表示当前的优先级，根据它可以得到对应的FutureCollection，然后遍历其中所有的WakerPage...

这里插入一下WakerPage的内部实现，可以看到有三个64位的位图：notified，dropped还有borrowed，一开始它们均被初始化为0。它们都是用内存顺序为顺序一致性的原子变量实现的，也就是说多核上应该也没什么问题。这里notified这个位图应该说比较关键，它某个位为1的话表示对应的Future准备好再次被polled了。可以`initialize`某个idx，其效果是notified对应位置1，dropped和borrowed对应位置0。`mark_dropped`和`notify`方法分别将dropped和notified对应位置1，而`mark_borrowed`可以将borrowed对应位置为指定值。`take_notified`的作用是返回notified的值并将notified清零，这里返回值需要经过dropped和borrowed的筛选，如果dropped或者borrowed对应位为1的话，返回值的对应位为0。`take_dropped`是返回dropped的当前值并将dropped清零。`clear`则是将三个位图的某一位均清零。最后是`make_waker`，这是对于给定的一个Future的idx和`dropped: &Arc<AtomicBool>`生成一个WakerRef。

WakerRef的结构如下：

```rust
pub struct WakerRef {
    page: Arc<WakerPage>,
    idx: usize,
    dropped: Arc<AtomicBool>,
}
```

至于方法，`mark_borrowed`是调用`WakerPage::mark_borrowed`对于自身的idx对应的位进行修改。`wake_by_ref`和`drop_by_ref`则是分别调用`WakerPage::notify/mark_dropped`也是对WakerPage进行更新。不过事先需要检查WakerRef里面的dropped，均需要dropped为false，`drop_by_ref`之后会将这个dropped改为true。这样的话，一个WakerPage维护的64个Future应该每个都有自己的WakerRef，可以通过它对WakerPage进行更新。后面我们为WakerRef实现了Woke trait，Woke trait来自于一个同名的crate，感觉还是比较好用的：

```rust
pub trait Woke: Send + Sync {
    fn wake(self: Arc<Self>) {
        Self::wake_by_ref(&self)
    }

    fn wake_by_ref(arc_self: &Arc<Self>);
}
```

我们只需要实现`wake_by_ref`这个方法。然后，所有实现了Woke trait的类型会得到额外的好处：该crate会帮助我们生成对应的RawWakerVTable，并进而帮助我们生成Waker和WakerRef，其中WakerRef也是这个crate定义的：

```rust
pub fn waker<W>(wake: Arc<W>) -> Waker
where
    W: Woke;

pub fn waker_ref<W>(wake: &Arc<W>) -> WakerRef<'_>
where
    W: Woke;
```

这样就知道唤醒的大概流程了：WakerRef生成的Waker可以更新WakerPage，而后WakerPage会在`TaskCollection::generator`中轮询（现在我们跳回到原来轮询WakerPage的地方），每一轮最多只会返回一个待唤醒的Future的Key。generator中如果没有要唤醒的Future，则会开启drop流程，不过这并非我们关注的重点。现在的问题是：这个generator是被谁调用的？我们看到`TaskCollection::take_task`调用generator来取出一个任务（这里回想一下，notified其实就是一个就绪队列），而这个`take_task`则应该是被Executor调用的。

这样的话，TaskCollection和WakerPage的实现就看完了。一开始我认为WakerPage是对应到一个物理页面，但实际上并非如此。

### Executor

首先是Executor的结构：

```rust
pub struct Executor {
    id: usize,
    task_collection: Arc<TaskCollection>,
    stack_base: usize,
    pub context: ExecuterContext,
    #[cfg(any(target_arch = "riscv64", target_arch = "aarch64"))]
    context_data: ContextData,
    task_id: usize,
    state: ExecutorState,
}
```

其中id是和Task的id以差不多的方式分配的。在`new`的时候会给每个Executor分配一个栈，并调用`init_stack_and_context`进行栈的初始化。Executor的主循环是`run`方法。每次迭代从TaskCollection中调用`take_task`方法取出一个任务，目前的实现是如果当前Runtime没有的话则从其他核的Runtime上通过`steal_task_from_other_cpu`方法偷任务（这样的话岂不是Runtime已经跨核了，从目前的实现来看似乎并没有问题）。拿到任务之后也能拿到对应的WakerRef（通过Arc放到堆上），进而调用`woke::waker_ref`生成woke定义的WakerRef，然后拿到Rust标准的Context。在poll取出的Future的时候用的就是这个Context，具体来说，到了叶子Future被阻塞的时候注册到Reactor的也就是这些Context。注意一下borrowed的相关细节，在poll之前需要将borrowed标记为true，之后则将borrowed标记为false，当我们想自己实现一个靠谱的Runtime的时候这可能是不可忽略的细节。如果poll的结果是Ready的话则调用`WakerRef::drop_by_ref`在TaskCollection中回收掉对应的Future；如果是Pending的话则什么都不做，因为该Future在被唤醒之前不会再被polled到了。

然后ExecutorState有这么几种：STRONG/WEAK/KILLED/UNUSED。注释提到WEAK状态只允许poll一个Future，然后就需要转化为KILLED并被dropped掉。在`run`中也能找到对应的逻辑。如果当前Executor没有任务的话，则大多数情况调用`sched_yield`将控制权交还给Runtime。

### 我们自己简化版的Runtime设计

感觉有点复杂...先去抽空了解下zCore目前的网络/存储栈的情况，过一会就要用到了。

我觉得比较简单的设计的话，就类似于一个之前的Completion Queue的实现就好。这样确实没有那么高效但胜在简单方便QAQ 

我们的异步模块应该包括Ready/Pending两个Future的队列。然后针对一个取出来的task，在poll之前需要生成一个包含它独有信息的waker，即task的id以及指向该模块调度数据结构的指针。

实现出来简单跑了一下，不开kvm的话看起来是能正常运行的，但是比正常版本要慢很多，感觉是过多的数据拷贝或者是同步开销？开kvm的话貌似是卡在更早的地方。

## 05/07/22

导师的意见是在kvm上跑起来比较关键，于是就先弄这个bug。在qemu中打开[multithread tcg](https://wiki.qemu.org/Features/tcg-multithread)可能是kvm之前一个不错的过渡，可以先尝试一下。具体的做法是将QEMU的加速配置改成`-accel tcg,thread=multi`。改了之后似乎我们的版本能正常运行了，但是依然很慢。特别是尝试`ls libc-test/src/math`这种大目录的时候。大概可以打开`LOG=info`看一下什么操作最为耗时。感觉上非常耗时的大概就是`fstatat`这种syscall。

> 这里稍微提一下，能否把page fault的处理在核间迁移？

在x86_64平台上，QEMU模拟的CPU可以配置的feature很多（无论是否打开kvm），而且我们找不到这些配置的相关说明，所以说局面有点陷入僵局。

后面尝试用GDB调试了一下，tcg的行为看起来很正常，kvm则非常奇怪。这里有一个关闭ASLR的[相关回答](https://askubuntu.com/questions/964540/gdb-qemu-cant-put-break-point-on-kernel-function-kernel-4-10-0-35)，先mark一下。

从目前的情况来看，使用GDB调试的帮助似乎不大。

顺带一提，加上配置`DISK=on`应该就可以把块设备从membuf换成virtio blk，不过可能还需要改代码。

## 06/07/22

今天和向老师交流了一下，总结一下纪要：

1. 使用远程的SiFive u740而非x86_64 QEMU+kvm，优势是我对于RV还是更加熟悉，而且u740上有天然的大小核
2. 达成了项目总体设计的共识：即将内核划分成多个互相之前使用RPC进行通信的异步模块
3. 参考io_uring的接口设计，特别是参数传递方式考虑模块之间的通用接口
4. 借鉴io_uring，采用completion queue的设计而非提供一个callback函数供被调用模块使用
5. 原先的设计是每个模块是一个线程，希望将其改成每个模块是一个协程

目前已经提交了u740的使用申请，但暂未得到回复。

## 12/07/22

今天确认了两个事实：首先是zCore上的x86_64平台的存储设备还并不支持virtio-blk，目前都是使用ramdisk作为替代。使用参数`DISK=on`会报错，原因是7.0.0版本的qemu似乎已经移除了ahci总线。其次是主线zCore的多核启动在启用kvm的情况下也无法顺利运行，这就与我们的代码改动无关了。

根据导师的建议，目前是尝试调整qemu的配置参数（包括cpu model还有各种flags），直到多核可以顺利跑起来且性能不出现明显瓶颈为止。一个可能比较大的性能瓶颈是vm exit指令的频率。我们可以尝试使用perf来监控qemu的运行来测量这些硬件计数。

根据导师的建议，目前简单的将参数换成了`-cpu qemu64`，就把多核跑起来了，目测速度和tcg一样慢的可以（`qemu64`应该等价与tcg）。同样的参数，原版zCore跑的就非常快，我们的实现要至少慢上一个数量级。我们需要明确：这样的速度差异，到底是我们的实现问题，还是qemu模拟过程中过多的抽象层导致的？更详细一点，我们的最终目标是要比较我们的实现和原版zCore在bare metal上的性能。那么目前的问题就有两种情况：

1. 即使在bare metal上我们的实现也比原版zCore慢一个数量级，这就说明是我们的设计或实现有问题；
2. 仅在qemu目前的参数配置下我们很慢，在bare metal上我们的实现性能不会这么差，则说明是qemu参数配置的问题，目前的抽象栈与我们期望的bare metal有较多差异。

那么如何知道目前的问题是哪一种呢？

我们先考虑再尝试一下各种参数。

> 顺便：其他参数的尝试以及详细数据，以`ls libc-test/src/math`操作为例
>
> * 不使用任何加速，采用Makefile中默认的cpu model（不启用tcg multithread），耗时6.99s->47.17s，即40.18s
>
> * `-cpu kvm64`能正常跑，耗时6.87s->46.97s，即40.1s
>
> * tcg multithread能正常跑，耗时6.55s->46.56s，即40.01s
>
> 相比而言，原版zCore，默认cpu model，耗时7.88s->8.92s，即1.04s

为什么我们这么慢？

大概总结一下我们在跑自己实现的时候看到的一些一致的现象，比如输出是：

```
/ # ls libc-test/src/math
[  6.553935 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1159b8(WRITE | USER), pid=1028
[  6.557232 WARN  1 0:0 linux_syscall] setpgid: unimplemented
[  6.577655 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x95d9c(WRITE | USER), pid=1028
[  6.578147 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e4318(WRITE | USER), pid=1028
[  6.578557 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x92a80(WRITE | USER), pid=1028
[  6.601704 WARN  1 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  6.601924 WARN  1 0:0 linux_syscall] setpgid: unimplemented
[  6.626130 WARN  1 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  6.626168 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x94d84(WRITE | USER), pid=1028
[  6.649589 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f5288(WRITE | USER), pid=1028
[  6.649873 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f7ff0(WRITE | USER), pid=1028
[  6.650147 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e5024(WRITE | USER), pid=1028
[  6.650480 WARN  1 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[  6.673560 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f6390(WRITE | USER), pid=1028
[  6.673985 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[  6.745771 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[  6.747110 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x93948(WRITE | USER), pid=1028
/ # [  6.914618 WARN  1 0:0 linux_syscall::vm] mprotect: unimplemented
[  6.938176 WARN  1 0:0 linux_syscall::vm] mprotect: unimplemented
[  6.962777 WARN  1 0:0 linux_syscall] getuid: unimplemented
[  7.513620 WARN  1 0:0 linux_syscall] brk: unimplemented
[  8.065632 WARN  1 0:0 linux_syscall] brk: unimplemented
[  8.641566 WARN  1 0:0 linux_syscall] brk: unimplemented
[  9.721508 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 10.825578 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 12.937472 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 15.097418 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 19.297288 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 23.497174 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 31.872965 WARN  1 0:0 linux_syscall] brk: unimplemented
{实际输出}
[ 46.560628 WARN  1 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[ 46.608559 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[ 46.704576 WARN  1 0:0 linux_syscall] geteuid: unimplemented
/ # QEMU: Terminated
```

而下面是原版zCore的输出：

```
/ # ls libc-test/src/math
[  7.881749 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1159b8(WRITE | USER), pid=1028
[  7.886516 WARN  0 0:0 linux_syscall] setpgid: unimplemented
[  7.886814 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x95d9c(WRITE | USER), pid=1028
[  7.887260 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e4318(WRITE | USER), pid=1028
[  7.887798 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x92a80(WRITE | USER), pid=1028
[  7.888933 WARN  0 0:0 linux_syscall] setpgid: unimplemented
[  7.889212 WARN  0 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  7.905922 WARN  0 0:0 linux_syscall::vm] mprotect: unimplemented
[  7.906600 WARN  0 0:0 linux_syscall::vm] mprotect: unimplemented
[  7.908023 WARN  0 0:0 linux_syscall] getuid: unimplemented
[  7.913904 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.916394 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.918769 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.925064 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.934318 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.955441 WARN  0 0:0 linux_syscall] brk: unimplemented
[  7.987248 WARN  0 0:0 linux_syscall] brk: unimplemented
[  8.075086 WARN  0 0:0 linux_syscall] brk: unimplemented
[  8.205886 WARN  0 0:0 linux_syscall] brk: unimplemented
[  8.581359 WARN  0 0:0 linux_syscall] brk: unimplemented
{实际输出}
[  8.927216 WARN  0 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  8.927666 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x94d84(WRITE | USER), pid=1028
[  8.928118 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f5288(WRITE | USER), pid=1028
[  8.929099 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f7ff0(WRITE | USER), pid=1028
[  8.929390 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e5024(WRITE | USER), pid=1028
[  8.929689 WARN  0 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[  8.929917 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f6390(WRITE | USER), pid=1028
[  8.930245 WARN  0 0:0 linux_syscall] geteuid: unimplemented
[  8.930544 WARN  0 0:0 linux_syscall] geteuid: unimplemented
[  8.930786 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x93948(WRITE | USER), pid=1028
/ # QEMU: Terminated
```

可以看到，拉开差距的关键在两个部分：

第一是中间一长串未实现的brk调用。而且在我们的实现中，每两次调用的间隔越来越久。我们后续可以通过`LOG=info`来看中间究竟发生了什么事情。总体而言，原版zCore总共用了0.7秒左右，而我们用了24秒左右。

第二是最终输出的打印部分。可以看到原版zCore只用了不到半秒钟，而我们的实现用了15秒左右。

其余部分则可以说性能相近，至少没有成为瓶颈。

## 13/07/22

一个非常诡异的现象是是否启用tcg multithread跑的时间差不多。

从输出部分可以看到一个有趣的现象：

```
[ 37.456578 INFO  1 0:0 linux_syscall::file::file] writev: fd=FileDesc(1), iov=0x115c80, count=2
Makefile                fmaf.c                  modfl.c
[ 37.457103 INFO  1 0:0 linux_syscall] <= Ok(86)
[ 37.457317 INFO  1 0:0 linux_syscall::async_syscall] task 1433 dropped
[ 37.480386 INFO  1 0:0 linux_syscall::async_syscall] select task id: 1434
[ 37.480588 INFO  1 0:0 linux_syscall::file::file] writev: fd=FileDesc(1), iov=0x115ca0, count=2
README                  fmaf.err                modfl.err
[ 37.481106 INFO  1 0:0 linux_syscall] <= Ok(88)
[ 37.481332 INFO  1 0:0 linux_syscall::async_syscall] task 1434 dropped
[ 37.504323 INFO  1 0:0 linux_syscall::async_syscall] select task id: 1435
[ 37.504508 INFO  1 0:0 linux_syscall::file::file] writev: fd=FileDesc(1), iov=0x115ca0, count=2
REPORT                  fmaf.exe                modfl.exe
[ 37.504989 INFO  1 0:0 linux_syscall] <= Ok(88)
[ 37.505184 INFO  1 0:0 linux_syscall::async_syscall] task 1435 dropped
[ 37.529004 INFO  1 0:0 linux_syscall::async_syscall] select task id: 1436
[ 37.529398 INFO  1 0:0 linux_syscall::file::file] writev: fd=FileDesc(1), iov=0x115ca0, count=2
acos.c                  fmaf.exe.ld.err         modfl.exe.ld.err
[ 37.529943 INFO  1 0:0 linux_syscall] <= Ok(95)
```

可以看到每隔5行有一个实际的输出。值得注意的是重大的延迟差异在于task dropped和select task之间有0.025s左右的延迟，一共大约有400行的实际输出，于是总延迟在10s左右，这构成了输出部分的绝大部分延迟。那么为什么会有这么高的延迟呢？通过进一步打log，可以看到大部分延迟集中在事件循环开头从共享队列中取出client提交的任务并加到自身队列的过程。进一步定位，发现原因在于大多数时候共享队列中没有任务，但是仍然需要尝试获取共享队列的锁（目前是用的`spin::Mutex`来同步互斥而非`crossbeam`，因为`crossbeam`居然不支持`no_std`），而该锁的同步开销或者自身的基础操作开销居然成为了瓶颈。

所以这意味着我们异步模块的基本架构可能需要做出变更：在接受外部请求方面，需要从轮询转化为一种另类的wakeup，至少尽量减少轮询的频率。另一种思路则是从互斥锁换成无锁队列。找了一下，发现crossbeam的`SegQueue`是支持alloc的，所以先换上试试。crossbeam声称它支持alloc，但实际上并不...看起来是GG了。

于是，另一种思路是使用一个`AtomicBool`来维护目前共享队列中是否有任务。仅当它为`true`的时候，再拿锁并加任务。轮询原子变量应该比加锁要快的多。

但是这样真的有用吗？会不会根本原因在于client响应时间过长？

注意到可能有一个关键性的差异。在原版zCore中，对于一个实际上立即返回的syscall，在`syscall.syscall` poll返回之后立刻推进原来Future的进度；而对于我们的修改版，首先在发出syscall请求之后，会返回一个Pending，然后原本的Future会被丢回到client core上的runtime中。在我们server core调用wakeup_fn之后，我们还需要等待：原先的Future被调度回来，重新走一遍自顶向下的poll路径，接着才能继续推进原先Future的进度。那么这0.025s左右的延迟可能来自于这两部分？

## 19/07/22

继续几天之前的工作...如何知道这0.025s的延迟来自什么地方？

根据经验来看对Future重新poll一遍应该不会产生什么显著开销，那么就应该是调度器某些实现的问题？让我们重新去看一下client调度器的代码吧。

通过`LOG=debug`可以看到重新poll一次Future的延迟非常低。主要问题是我们在server端wakeup一个client之后，经过了大概0.02s之后才看到client的输出，而且是一个比较奇怪的`handle kernel timeout`，看起来与时钟中断有关。

具体来说，中间发生的事情如下：

```
[ 11.446432 INFO  1 0:0 linux_syscall::async_syscall] wakeup client!
[ 11.466677 DEBUG 0 0:0 executor::runtime] handle kernel timeout
[ 11.466895 DEBUG 0 0:0 executor::runtime] switch 1 -> idle
[ 11.467044 DEBUG 0 0:0 executor::runtime] switch idle -> 1
[ 11.467200 DEBUG 0 0:0 executor::runtime] run strong executor
[ 11.467400 DEBUG 0 0:0 executor::executor] running future 1:2
[ 11.467576 DEBUG 0 0:0 zcore_loader::linux] go to user: tid = 1059 pc = 45567
[ 11.467871 DEBUG 0 0:0 zcore_loader::linux] back from user: tid = 1059 pc = 45567 trap reason = Syscall
[ 11.468182 DEBUG 0 0:0 executor::executor] back from future 1:2
[ 11.468184 INFO  1 0:0 linux_syscall::async_syscall] queue op completed, before adding new req
```

首先明确一下时钟中断的调用链，也就是这个`handle kernel timeout`是如何被打印出来的。我们看到kernel-hal中的`trap_handler`函数在时钟中断的时候尝试调用`executor::handle_timeout`，而从这个函数的实现来看，它是通过调用`sched_yield`函数从当前的executor切换回管理这个executor的runtime。

所以问题的关键可能是，为什么一定要等到时钟中断的时候client的调度才会产生变化？

这时候，我忽然看到了这样一行输出：

```
[ 11.468389 DEBUG 0 0:0 executor::executor] no other tasks, wait for interrupt
```

这大概可以看出，zCore目前真的是没有任何一个后台任务。在当前应用thread Future向server提交请求进入阻塞状态之后，当前executor上就再也没有任何活跃任务了，于是直接调用`wait_for_interrupt`等待下一个时钟中断，在此期间CPU直接摆烂...即使server已经完成了系统调用请求，还是要等到client上触发了时钟中断才能接着往下跑...

那么解决方案有两种，要么去掉这个万恶的`wait_for_interrupt`，要么加上一个固定的后台任务。看起来第一种方法比较简单...但是实际上是不行的，因为目前串口也是通过中断驱动的。如果这样做就相当于内核态全程关中断了，无法进行输入。于是我们考虑另一种简单的方案，就是将时钟中断频率从100Hz提高为500Hz。这个只需要改kernel-hal就行了。效果如下：

```
/ # ls libc-test/src/math
[  4.976590 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1159b8(WRITE | USER), pid=1028
[  4.979743 WARN  1 0:0 linux_syscall] setpgid: unimplemented
[  4.980984 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x95d9c(WRITE | USER), pid=1028
[  4.981336 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e4318(WRITE | USER), pid=1028
[  4.981720 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x92a80(WRITE | USER), pid=1028
[  4.986152 WARN  1 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  4.986823 WARN  1 0:0 linux_syscall] setpgid: unimplemented
[  4.990865 WARN  1 0:0 linux_object::fs::stdio] stdout TCGETS | TIOCSPGRP, pretend to be tty.
[  4.990872 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x94d84(WRITE | USER), pid=1028
[  4.995567 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f5288(WRITE | USER), pid=1028
[  4.996432 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f7ff0(WRITE | USER), pid=1028
[  4.997047 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1e5024(WRITE | USER), pid=1028
[  4.997470 WARN  1 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[  5.000148 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x1f6390(WRITE | USER), pid=1028
[  5.000587 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[  5.019448 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[  5.020370 WARN  0 0:0 zcore_loader::linux] page fault from user mode @ 0x93948(WRITE | USER), pid=1028
/ # [  5.049136 WARN  1 0:0 linux_syscall::vm] mprotect: unimplemented
[  5.053262 WARN  1 0:0 linux_syscall::vm] mprotect: unimplemented
[  5.059380 WARN  1 0:0 linux_syscall] getuid: unimplemented
[  5.168183 WARN  1 0:0 linux_syscall] brk: unimplemented
[  5.278573 WARN  1 0:0 linux_syscall] brk: unimplemented
[  5.393731 WARN  1 0:0 linux_syscall] brk: unimplemented
[  5.609772 WARN  1 0:0 linux_syscall] brk: unimplemented
[  5.830575 WARN  1 0:0 linux_syscall] brk: unimplemented
[  6.252955 WARN  1 0:0 linux_syscall] brk: unimplemented
[  6.684915 WARN  1 0:0 linux_syscall] brk: unimplemented
[  7.524890 WARN  1 0:0 linux_syscall] brk: unimplemented
[  8.364865 WARN  1 0:0 linux_syscall] brk: unimplemented
[ 10.040016 WARN  1 0:0 linux_syscall] brk: unimplemented
{output}
[ 13.116722 WARN  1 0:0 linux_object::fs::stdio] stdin TCGETS | TIOCSPGRP, pretend to be tty.
[ 13.126304 WARN  1 0:0 linux_syscall] geteuid: unimplemented
[ 13.145473 WARN  1 0:0 linux_syscall] geteuid: unimplemented
/ #
```

确实比原来快了很多，如果我们将其调整为1000Hz的话，用时总计4秒左右。而原版zCore的总用时大概在1秒多，所以就是说要调整为4000Hz?可以试试，这回总延迟降低到1.81秒（原版zCore 1.05秒）。当然我们并不想让时钟中断的频率这么高，所以还是加上一个固定的后台任务会更好。

---

另一件事是搞清楚qemu的模拟相比裸机性能是否有很大差异（特别是虚拟化相关开销）。

首先通过perf看看tcg multithread是否在vmexit这里会有些开销。

---

另外一件事情是在x86_64平台上启用virtio-blk并使用异步操作。
